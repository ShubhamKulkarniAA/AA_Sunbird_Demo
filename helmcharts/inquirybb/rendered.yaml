---
# Source: inquirybb/charts/assessment/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: assessment
---
# Source: inquirybb/charts/cassandra/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cassandra-ibb
  namespace: "default"
  labels:
    app.kubernetes.io/name: cassandra
    helm.sh/chart: cassandra-10.1.0
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "4.1.0"
  annotations:
    helm.sh/hook-weight: "-5"
automountServiceAccountToken: true
---
# Source: inquirybb/charts/flink/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flink
  annotations:
---
# Source: inquirybb/charts/kafka/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kafka-ibb
  namespace: "default"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-20.0.2
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.3.1"
    app.kubernetes.io/component: kafka
  annotations:
    helm.sh/hook-weight: "-5"
automountServiceAccountToken: true
---
# Source: inquirybb/charts/neo4j/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: neo4j-ibb
---
# Source: inquirybb/charts/redis/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  name: redis-ibb
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.8.3
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.9"
  annotations:
    helm.sh/hook-weight: "-5"
---
# Source: inquirybb/charts/cassandra/templates/cassandra-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: cassandra-ibb
  namespace: "default"
  labels:
    app.kubernetes.io/name: cassandra
    helm.sh/chart: cassandra-10.1.0
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "4.1.0"
  annotations:
    helm.sh/hook-weight: "-5"
type: Opaque
data:
  cassandra-password: "NEp0WFdIY3V2Rg=="
---
# Source: inquirybb/charts/assessment/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: assessment
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: assessment
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: assessment-0.1.0
  annotations:

    reloader.stakater.com/auto: "true"
data: # Take only root level files (configs/*) for configmaps # Skip env.yaml as configmap, as it's env file
    
  application.conf: |-
          # This is the main configuration file for the application.
          # https://www.playframework.com/documentation/latest/ConfigFile
          # ~~~~~
          # Play uses HOCON as its configuration file format.  HOCON has a number
          # of advantages over other config formats, but there are two things that
          # can be used when modifying settings.
          #
          # You can include other configuration files in this main application.conf file:
          #include "extra-config.conf"
          #
          # You can declare variables and substitute for them:
          #mykey = ${some.value}
          #
          # And if an environment variable exists when there is no other substitution, then
          # HOCON will fall back to substituting environment variable:
          #mykey = ${JAVA_HOME}
      
          ## Akka
          # https://www.playframework.com/documentation/latest/ScalaAkka#Configuration
          # https://www.playframework.com/documentation/latest/JavaAkka#Configuration
          # ~~~~~
          # Play uses Akka internally and exposes Akka Streams and actors in Websockets and
          # other streaming HTTP responses.
          akka {
            # "akka.log-config-on-start" is extraordinarly useful because it log the complete
            # configuration at INFO level, including defaults and overrides, so it s worth
            # putting at the very top.
            #
            # Put the following in your conf/logback.xml file:
            #
            # <logger name="akka.actor" level="INFO" />
            #
            # And then uncomment this line to debug the configuration.
            #
            #log-config-on-start = true
            default-dispatcher {
              # This will be used if you have set "executor = "fork-join-executor""
              fork-join-executor {
                # Min number of threads to cap factor-based parallelism number to
                parallelism-min = 8
      
                # The parallelism factor is used to determine thread pool size using the
                # following formula: ceil(available processors * factor). Resulting size
                # is then bounded by the parallelism-min and parallelism-max values.
                parallelism-factor = 32.0
      
                # Max number of threads to cap factor-based parallelism number to
                parallelism-max = 64
      
                # Setting to "FIFO" to use queue like peeking mode which "poll" or "LIFO" to use stack
                # like peeking mode which "pop".
                task-peeking-mode = "FIFO"
              }
            }
            actors-dispatcher {
              type = "Dispatcher"
              executor = "fork-join-executor"
              fork-join-executor {
                parallelism-min = 8
                parallelism-factor = 32.0
                parallelism-max = 64
              }
              # Throughput for default Dispatcher, set to 1 for as fair as possible
              throughput = 1
            }
            actor {
              deployment {
                /healthActor
                  {
                    router = smallest-mailbox-pool
                    nr-of-instances = 5
                    dispatcher = actors-dispatcher
                  }
                /itemSetActor
                  {
                    router = smallest-mailbox-pool
                    nr-of-instances = 2
                    dispatcher = actors-dispatcher
                  }
                /questionActor
                  {
                    router = smallest-mailbox-pool
                    nr-of-instances = 5
                    dispatcher = actors-dispatcher
                  }
                /questionSetActor
                  {
                    router = smallest-mailbox-pool
                    nr-of-instances = 5
                    dispatcher = actors-dispatcher
                  }
                /questionV5Actor
                  {
                    router = smallest-mailbox-pool
                    nr-of-instances = 5
                    dispatcher = actors-dispatcher
                  }
                /questionSetV5Actor
                  {
                    router = smallest-mailbox-pool
                    nr-of-instances = 5
                    dispatcher = actors-dispatcher
                  }
              }
            }
          }
      
          ## Secret key
          # http://www.playframework.com/documentation/latest/ApplicationSecret
          # ~~~~~
          # The secret key is used to sign Play's session cookie.
          # This must be changed for production, but we don't recommend you change it in this file.
          play.http.secret.key="a-dummy-play-http-secret-key"
      
          ## Modules
          # https://www.playframework.com/documentation/latest/Modules
          # ~~~~~
          # Control which modules are loaded when Play starts.  Note that modules are
          # the replacement for "GlobalSettings", which are deprecated in 2.5.x.
          # Please see https://www.playframework.com/documentation/latest/GlobalSettings
          # for more information.
          #
          # You can also extend Play functionality by using one of the publically available
          # Play modules: https://playframework.com/documentation/latest/ModuleDirectory
          play.modules {
            # By default, Play will load any class called Module that is defined
            # in the root package (the "app" directory), or you can define them
            # explicitly below.
            # If there are any built-in modules that you want to enable, you can list them here.
            #enabled += my.application.Module
      
            # If there are any built-in modules that you want to disable, you can list them here.
            #disabled += ""
            enabled += modules.AssessmentModule
          }
      
          ## IDE
          # https://www.playframework.com/documentation/latest/IDE
          # ~~~~~
          # Depending on your IDE, you can add a hyperlink for errors that will jump you
          # directly to the code location in the IDE in dev mode. The following line makes
          # use of the IntelliJ IDEA REST interface:
          #play.editor="http://localhost:63342/api/file/?file=%s&line=%s"
      
          ## Internationalisation
          # https://www.playframework.com/documentation/latest/JavaI18N
          # https://www.playframework.com/documentation/latest/ScalaI18N
          # ~~~~~
          # Play comes with its own i18n settings, which allow the user's preferred language
          # to map through to internal messages, or allow the language to be stored in a cookie.
          play.i18n {
            # The application languages
            langs = [ "en" ]
      
            # Whether the language cookie should be secure or not
            #langCookieSecure = true
      
            # Whether the HTTP only attribute of the cookie should be set to true
            #langCookieHttpOnly = true
          }
      
          ## Play HTTP settings
          # ~~~~~
          play.http {
            ## Router
            # https://www.playframework.com/documentation/latest/JavaRouting
            # https://www.playframework.com/documentation/latest/ScalaRouting
            # ~~~~~
            # Define the Router object to use for this application.
            # This router will be looked up first when the application is starting up,
            # so make sure this is the entry point.
            # Furthermore, it's assumed your route file is named properly.
            # So for an application router like `my.application.Router`,
            # you may need to define a router file `conf/my.application.routes`.
            # Default to Routes in the root package (aka "apps" folder) (and conf/routes)
            #router = my.application.Router
      
            ## Action Creator
            # https://www.playframework.com/documentation/latest/JavaActionCreator
            # ~~~~~
            #actionCreator = null
      
            ## ErrorHandler
            # https://www.playframework.com/documentation/latest/JavaRouting
            # https://www.playframework.com/documentation/latest/ScalaRouting
            # ~~~~~
            # If null, will attempt to load a class called ErrorHandler in the root package,
            #errorHandler = null
      
            ## Session & Flash
            # https://www.playframework.com/documentation/latest/JavaSessionFlash
            # https://www.playframework.com/documentation/latest/ScalaSessionFlash
            # ~~~~~
            session {
              # Sets the cookie to be sent only over HTTPS.
              #secure = true
      
              # Sets the cookie to be accessed only by the server.
              #httpOnly = true
      
              # Sets the max-age field of the cookie to 5 minutes.
              # NOTE: this only sets when the browser will discard the cookie. Play will consider any
              # cookie value with a valid signature to be a valid session forever. To implement a server side session timeout,
              # you need to put a timestamp in the session and check it at regular intervals to possibly expire it.
              #maxAge = 300
      
              # Sets the domain on the session cookie.
              #domain = "example.com"
            }
      
            flash {
              # Sets the cookie to be sent only over HTTPS.
              #secure = true
      
              # Sets the cookie to be accessed only by the server.
              #httpOnly = true
            }
          }
      
          play.server.http.idleTimeout = 60s
          play.http.parser.maxDiskBuffer = 10MB
          parsers.anyContent.maxLength = 10MB
      
          ## Netty Provider
          # https://www.playframework.com/documentation/latest/SettingsNetty
          # ~~~~~
          play.server.netty {
            # Whether the Netty wire should be logged
            log.wire = true
      
            # If you run Play on Linux, you can use Netty's native socket transport
            # for higher performance with less garbage.
            transport = "native"
          }
      
          ## WS (HTTP Client)
          # https://www.playframework.com/documentation/latest/ScalaWS#Configuring-WS
          # ~~~~~
          # The HTTP client primarily used for REST APIs.  The default client can be
          # configured directly, but you can also create different client instances
          # with customized settings. You must enable this by adding to build.sbt:
          #
          # libraryDependencies += ws // or javaWs if using java
          #
          play.ws {
            # Sets HTTP requests not to follow 302 requests
            #followRedirects = false
      
            # Sets the maximum number of open HTTP connections for the client.
            #ahc.maxConnectionsTotal = 50
      
            ## WS SSL
            # https://www.playframework.com/documentation/latest/WsSSL
            # ~~~~~
            ssl {
              # Configuring HTTPS with Play WS does not require programming.  You can
              # set up both trustManager and keyManager for mutual authentication, and
              # turn on JSSE debugging in development with a reload.
              #debug.handshake = true
              #trustManager = {
              #  stores = [
              #    { type = "JKS", path = "exampletrust.jks" }
              #  ]
              #}
            }
          }
      
          ## Cache
          # https://www.playframework.com/documentation/latest/JavaCache
          # https://www.playframework.com/documentation/latest/ScalaCache
          # ~~~~~
          # Play comes with an integrated cache API that can reduce the operational
          # overhead of repeated requests. You must enable this by adding to build.sbt:
          #
          # libraryDependencies += cache
          #
          play.cache {
            # If you want to bind several caches, you can bind the individually
            #bindCaches = ["db-cache", "user-cache", "session-cache"]
          }
      
          ## Filter Configuration
          # https://www.playframework.com/documentation/latest/Filters
          # ~~~~~
          # There are a number of built-in filters that can be enabled and configured
          # to give Play greater security.
          #
          play.filters {
      
            # Enabled filters are run automatically against Play.
            # CSRFFilter, AllowedHostFilters, and SecurityHeadersFilters are enabled by default.
            enabled = [filters.AccessLogFilter]
      
            # Disabled filters remove elements from the enabled list.
            # disabled += filters.CSRFFilter
      
      
            ## CORS filter configuration
            # https://www.playframework.com/documentation/latest/CorsFilter
            # ~~~~~
            # CORS is a protocol that allows web applications to make requests from the browser
            # across different domains.
            # NOTE: You MUST apply the CORS configuration before the CSRF filter, as CSRF has
            # dependencies on CORS settings.
            cors {
              # Filter paths by a whitelist of path prefixes
              #pathPrefixes = ["/some/path", ...]
      
              # The allowed origins. If null, all origins are allowed.
              #allowedOrigins = ["http://www.example.com"]
      
              # The allowed HTTP methods. If null, all methods are allowed
              #allowedHttpMethods = ["GET", "POST"]
            }
      
            ## Security headers filter configuration
            # https://www.playframework.com/documentation/latest/SecurityHeaders
            # ~~~~~
            # Defines security headers that prevent XSS attacks.
            # If enabled, then all options are set to the below configuration by default:
            headers {
              # The X-Frame-Options header. If null, the header is not set.
              #frameOptions = "DENY"
      
              # The X-XSS-Protection header. If null, the header is not set.
              #xssProtection = "1; mode=block"
      
              # The X-Content-Type-Options header. If null, the header is not set.
              #contentTypeOptions = "nosniff"
      
              # The X-Permitted-Cross-Domain-Policies header. If null, the header is not set.
              #permittedCrossDomainPolicies = "master-only"
      
              # The Content-Security-Policy header. If null, the header is not set.
              #contentSecurityPolicy = "default-src 'self'"
            }
      
            ## Allowed hosts filter configuration
            # https://www.playframework.com/documentation/latest/AllowedHostsFilter
            # ~~~~~
            # Play provides a filter that lets you configure which hosts can access your application.
            # This is useful to prevent cache poisoning attacks.
            hosts {
              # Allow requests to example.com, its subdomains, and localhost:9000.
              #allowed = [".example.com", "localhost:9000"]
            }
          }
      
          play.http.parser.maxMemoryBuffer = 50MB
          akka.http.parsing.max-content-length = 50MB
           
          schema.base_path= "https:////schemas/local" 
          
      
          # Cassandra Configuration
          cassandra.lp.connection="cassandra:9042"
      
          # Redis Configuration
          redis.host="redis-master"
          redis.port="6379"
          redis.maxConnections=128
      
          # Configuration
          graph.dir=/data/graphDB
          akka.request_timeout=30
          environment.id=20000000
          graph.ids=["domain"]
          graph.passport.key.base="a-dummy-graph_passport-secret-key"
          route.domain="bolt://neo4j:7687"
          route.bolt.write.domain="bolt://neo4j:7687"
          route.bolt.read.domain="bolt://neo4j:7687"
          route.all="bolt://neo4j:8687"
          route.bolt.write.all="bolt://neo4j:8687"
          route.bolt.read.all="bolt://neo4j:8687"
      
          shard.id= 1
          platform.auth.check.enabled=false
          platform.cache.ttl=3600000
      
          #Top N Config for Search Telemetry
          telemetry_env=dev
      
          installation.id=ekstep
      
      
          languageCode {
            assamese : "as"
            bengali : "bn"
            english : "en"
            gujarati : "gu"
            hindi : "hi"
            kannada : "ka"
            marathi : "mr"
            odia : "or"
            tamil : "ta"
            telugu : "te"
          }
      
          kafka {
            urls : "kafka:9092"
            topic.send.enable : true
            topics.instruction : ".assessment.publish.request"
          }
          objectcategorydefinition.keyspace="_category_store"
          question.keyspace="_question_store"
          questionset.keyspace="_hierarchy_store"
      
          composite {
            search {
              url : "http://search-service:9000/v3/search"
            }
          }
      
          import {
            request_size_limit : 300
            output_topic_name : ".object.import.request"
            required_props {
              question : ["name", "code", "mimeType", "framework", "channel"]
              questionset : ["name", "code", "mimeType", "framework", "channel"]
            }
          }
      
          master.category.validation.enabled= true
      
          question.cache.enable=true
          questionset.cache.enable=true
          assessment.copy.origin_data= ["name", "author", "license", "organisation"]
          assessment.copy.props_to_remove= ["downloadUrl", "artifactUrl", "variants", "createdOn", "collections", "children", "lastUpdatedOn", "SYS_INTERNAL_LAST_UPDATED_ON", "versionKey", "s3Key", "status", "pkgVersion", "toc_url", "mimeTypesCount", "contentTypesCount", "leafNodesCount", "childNodes", "prevState", "lastPublishedOn", "flagReasons", "compatibilityLevel", "size", "publishChecklist", "publishComment", "LastPublishedBy", "rejectReasons", "rejectComment", "gradeLevel", "subject", "medium", "board", "topic", "purpose", "subtopic", "contentCredits", "owner", "collaborators", "creators", "contributors", "badgeAssertions", "dialcodes", "concepts", "keywords", "reservedDialcodes", "dialcodeRequired", "leafNodes", "sYS_INTERNAL_LAST_UPDATED_ON", "prevStatus", "lastPublishedBy", "streamingUrl"]
      
          cloud_storage_container: ""
      
          cloudstorage {
            metadata.replace_absolute_path=true
            relative_path_prefix="CONTENT_STORAGE_BASE_PATH"
            metadata.list= ["appIcon", "artifactUrl", "posterImage", "previewUrl", "thumbnail", "assetsMap", "certTemplate", "itemSetPreviewUrl", "grayScaleAppIcon", "sourceURL", "variants", "downloadUrl", "streamingUrl", "toc_url", "data", "question", "solutions", "editorState", "media", "pdfUrl", "transcripts"]
            read_base_path="https://"
            write_base_path= ["https://","https://obj.dev.sunbird.org"]
          }
      
          question.list.limit="20"
          
          v5_supported_qumlVersions=[1.1]
          v5_default_qumlVersion="1.1"
       # Take only root level files (configs/*) for configmaps # Take only root level files (configs/*) for configmaps # Skip env.yaml as configmap, as it's env file
    
  logback.xml: |-
      <configuration>
      
          <conversionRule conversionWord="coloredLevel" converterClass="play.api.libs.logback.ColoredLevel" />
      
          <!-- transaction-event-trigger START -->
          <timestamp key="timestamp" datePattern="yyyy-MM-dd"/>
          <!-- common transactions logs -->
          <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
          <encoder>
              <pattern>%d %msg%n</pattern>
          </encoder>
          </appender>
      
          <appender name="ASYNCSTDOUT" class="ch.qos.logback.classic.AsyncAppender">
          <appender-ref ref="STDOUT" />
          </appender>
      
      
          <logger name="play" level="INFO" />
          <logger name="DefaultPlatformLogger" level="INFO" />
          <!-- Telemetry Loggers-->
      
          <root level="INFO">
          <appender-ref ref="ASYNCSTDOUT" />
          </root>
      
      
          <appender name="kafka-appender" class="com.github.danielwegener.logback.kafka.KafkaAppender">
          <encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
              <pattern>%msg</pattern>
          </encoder>
      
          <topic>dev.telemetry.raw</topic>
          <!-- ensure that every message sent by the executing host is partitioned to the same partition strategy -->
              <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.NoKeyKeyingStrategy" />
              <!-- block the logging application thread if the kafka appender cannot keep up with sending the log messages -->
              <deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy" />
              
          <!-- each <producerConfig> translates to regular kafka-client config (format: key=value) -->
              <!-- producer configs are documented here: https://kafka.apache.org/documentation.html#newproducerconfigs -->
              <!-- bootstrap.servers is the only mandatory producerConfig -->
              <producerConfig>bootstrap.servers=kafka:9092</producerConfig>
              <!-- don't wait for a broker to ack the reception of a batch.  -->
              <producerConfig>acks=0</producerConfig>
              <!-- wait up to 1000ms and collect log messages before sending them as a batch -->
              <producerConfig>linger.ms=15000</producerConfig>
              <!-- even if the producer buffer runs full, do not block the application but start to drop messages -->
              <producerConfig>max.block.ms=0</producerConfig>
              <!-- define a client-id that you use to identify yourself against the kafka broker -->
              <producerConfig>client.id=${HOSTNAME}-${CONTEXT_NAME}-logback-relaxed</producerConfig>
      
              <!-- there is no fallback <appender-ref>. If this appender cannot deliver, it will drop its messages. -->
      
          </appender>
      
          <logger name="TelemetryEventLogger" level="INFO">
          <appender-ref ref="kafka-appender" />
          </logger>
      
      </configuration>
---
# Source: inquirybb/charts/assessment/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: assessment-env
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: assessment
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: assessment-0.1.0
  annotations:

    reloader.stakater.com/auto: "true"
data:
  # You can add key value pair here, to create env values.
  # for example,
  
  # ENV: dev
  JAVA_OPTIONS: -Xms500m -Xmx500m
  _JAVA_OPTIONS: -Dlog4j2.formatMsgNoLookups=true
---
# Source: inquirybb/charts/cassandra/templates/metrics-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cassandra-ibb-metrics-conf
  namespace: "default"
  labels:
    app.kubernetes.io/name: cassandra
    helm.sh/chart: cassandra-10.1.0
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "4.1.0"
    app.kubernetes.io/part-of: cassandra
    app.kubernetes.io/component: cassandra-exporter
  annotations:
    helm.sh/hook-weight: "-5"
data:
  config.yml: |-
    host: localhost:7199
    ssl: False
    user:
    password:
    listenPort: 8080
    blacklist:
      # To profile the duration of jmx call you can start the program with the following options
      # > java -Dorg.slf4j.simpleLogger.defaultLogLevel=trace -jar cassandra_exporter.jar config.yml --oneshot
      #
      # To get intuition of what is done by cassandra when something is called you can look in cassandra
      # https://github.com/apache/cassandra/tree/trunk/src/java/org/apache/cassandra/metrics
      # Please avoid to scrape frequently those calls that are iterating over all sstables
    
      # Unaccessible metrics (not enough privilege)
      - java:lang:memorypool:.*usagethreshold.*
    
      # Leaf attributes not interesting for us but that are presents in many path
      - .*:999thpercentile
      - .*:95thpercentile
      - .*:fifteenminuterate
      - .*:fiveminuterate
      - .*:durationunit
      - .*:rateunit
      - .*:stddev
      - .*:meanrate
      - .*:mean
      - .*:min
    
      # Path present in many metrics but uninterresting
      - .*:viewlockacquiretime:.*
      - .*:viewreadtime:.*
      - .*:cas[a-z]+latency:.*
      - .*:colupdatetimedeltahistogram:.*
    
      # Mostly for RPC, do not scrap them
      - org:apache:cassandra:db:.*
    
      # columnfamily is an alias for Table metrics
      # https://github.com/apache/cassandra/blob/8b3a60b9a7dbefeecc06bace617279612ec7092d/src/java/org/apache/cassandra/metrics/TableMetrics.java#L162
      - org:apache:cassandra:metrics:columnfamily:.*
    
      # Should we export metrics for system keyspaces/tables ?
      - org:apache:cassandra:metrics:[^:]+:system[^:]*:.*
    
      # Don't scrap us
      - com:criteo:nosql:cassandra:exporter:.*
    
    maxScrapFrequencyInSec:
      50:
        - .*
    
      # Refresh those metrics only every hour as it is costly for cassandra to retrieve them
      3600:
        - .*:snapshotssize:.*
        - .*:estimated.*
        - .*:totaldiskspaceused:.*
---
# Source: inquirybb/charts/flink/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: async-questionset-publish-config
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
data:
  flink-conf: |-
    jobmanager.memory.flink.size: 1024m
    taskmanager.memory.flink.size: 1024m
    taskmanager.numberOfTaskSlots: 1
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    

  config: |-
    include file("file:///data/flink/conf/base-config.conf")
    
    job {
      env = ""
      enable.distributed.checkpointing = true
      statebackend {
        base.url = "file:///tmp/flink-checkpoints"
      }
    
      checkpointing.interval = 60000
      checkpointing.pause.between.seconds = 5000
      checkpointing.compression.enabled = true
      restart-strategy.attempts = 3
      restart-strategy.delay = 30000
    }
    
    kafka {
      broker-servers = "kafka:9092"
      producer.broker-servers = "kafka:9092"
      consumer.broker-servers = "kafka:9092"
      zookeeper = "zookeeper:2181"
      
      input.topic = {{ .Values.global.env }}.assessment.publish.request
      post_publish.topic = {{ .Values.global.env }}.assessment.postpublish.request
      groupId = {{ .Values.global.env }}-questionset-publish-group
      
      producer {
        max-request-size = 1572864
        batch.size = 98304
        linger.ms = 10
      }
    }
    
    task {
      parallelism = 1
      consumer.parallelism = 1
      downstream.operators.parallelism = 1
      router.parallelism = 1
      checkpointing.compressed = true
      checkpointing.interval = 60000
      checkpointing.pause.between.seconds = 5000
      restart-strategy.attempts = 3
      restart-strategy.delay = 30000
      raw {
        consumer.parallelism = 1
        downstream.operators.parallelism = 1
      }
    }
    
    redisdb.connection.timeout = 30000
    redis {
      host = "redis-master"
      port = 6379
      database {
        default.id = 0
      }
    }
    
    redis-meta {
      host = "redis-master"
      port = 6379
      database {
        default.id = 0
      }
    }
    
    lms-cassandra {
      host = "cassandra"
      port = "9042"
      keyspace = ""
    }
    
    question {
      keyspace = {{ .Values.global.env }}_question_store
      table = "question_data"
    }
    
    questionset {
      keyspace = {{ .Values.global.env }}_hierarchy_store
      table = "questionset_hierarchy"
    }
    
    print_service.base_url = "http://print:5000"
    
    cloud_storage_type: ""
    cloud_storage_key: ""  
    cloud_storage_secret: "" 
    cloud_storage_endpoint: ""
    cloud_storage_container: ""
    
    master.category.validation.enabled = "Yes"
    
    cloudstorage {
      metadata.replace_absolute_path = false
      metadata.list = ["appIcon","posterImage","artifactUrl","downloadUrl","variants","previewUrl","pdfUrl"]
      relative_path_prefix = "CONTENT_STORAGE_BASE_PATH"
      read_base_path="https://"
      write_base_path=["https://"]
      #read_base_path = "https://{{ .Values.global.object_storage_endpoint }}"
      #write_base_path = ["https://{{ .Values.global.object_storage_endpoint }}"]
    }
    
    schema {
      basePath = "https:////schemas/local"
      supportedVersion {
        itemset = "2.0"
      }
    }
    

  base-config: |-
    kafka {
      broker-servers = "kafka:9092"
      producer.broker-servers = "kafka:9092"
      consumer.broker-servers = "kafka:9092"
      zookeeper = "zookeeper:2181"
      producer {
        max-request-size = 1572864
        batch.size = 98304
        linger.ms = 10
      }
      input {
        user_pii_topic = ".delete.user.job.request"
        ownership_transfer_topic = ".user.ownership.transfer"
      }
      groupId = "-user-pii-data-updater-group"
    }
    
    job {
          env = ""
          enable.distributed.checkpointing = true
          statebackend {
            base.url = "file:///tmp/flink-checkpoints"
          }
    
          checkpointing.interval = 60000
          checkpointing.pause.between.seconds = 5000
          checkpointing.compression.enabled = true
          restart-strategy.attempts = 3
          restart-strategy.delay = 30000
    }
    
    task {
      parallelism = 1
      consumer.parallelism = 1
      checkpointing.compressed = true
      checkpointing.interval = 60000
      checkpointing.pause.between.seconds = 5000
      restart-strategy.attempts = 3
      restart-strategy.delay = 30000
    }
    
    redis {
      host = redis-master
      port = 6379
    }
    
    lms-cassandra {
      host = "cassandra"
      port = 9042
    }
    
    neo4j {
      routePath = "bolt://neo4j:7687"
      graph = "domain"
    }
    
    es {
      basePath = "elasticsearch:9200"
    }
    
    schema {
      basePath = "https:////schemas/local"
      supportedVersion = {
        itemset = "2.0"
      }
    }
    

  log4j_console_properties: |-
    # This affects logging for both user code and Flink
    rootLogger.level = INFO
    rootLogger.appenderRef.console.ref = ConsoleAppender
    
    # Uncomment this if you want to _only_ change Flink's logging
    #logger.flink.name = org.apache.flink
    #logger.flink.level = INFO
    
    # The following lines keep the log level of common libraries/connectors on
    # log level INFO. The root logger does not override this. You have to manually
    # change the log levels here.
    logger.akka.name = akka
    logger.akka.level = ERROR
    logger.kafka.name= org.apache.kafka
    logger.kafka.level = ERROR
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = ERROR
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = ERROR
    
    # Log all infos to the console
    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    
    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF
---
# Source: inquirybb/charts/flink/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: questionset-republish-config
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
data:
  flink-conf: |-
    jobmanager.memory.flink.size: 1024m
    taskmanager.memory.flink.size: 1024m
    taskmanager.numberOfTaskSlots: 1
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    

  config: |-
    include file("file:///data/flink/conf/base-config.conf")
    
    job {
      env = ""
      enable.distributed.checkpointing = true
      statebackend {
        base.url = "file:///tmp/flink-checkpoints"
      }
    
      checkpointing.interval = 60000
      checkpointing.pause.between.seconds = 5000
      checkpointing.compression.enabled = true
      restart-strategy.attempts = 3
      restart-strategy.delay = 30000
    }
    
    kafka {
      broker-servers = "kafka:9092"
      producer.broker-servers = "kafka:9092"
      consumer.broker-servers = "kafka:9092"
      zookeeper = "zookeeper:2181"
      
      input.topic = "demo.assessment.republish.request"
      post_publish.topic = "{{ .Values.global.env }}.assessment.postpublish.request"
      groupId = "{{ .Values.global.env }}-questionset-republish-group"
      
      producer {
        max-request-size = 1572864
        batch.size = 98304
        linger.ms = 10
      }
    }
    
    task {
      parallelism = 1
      consumer.parallelism = 1
      downstream.operators.parallelism = 1
      router.parallelism = 1
      checkpointing.compressed = true
      checkpointing.interval = 60000
      checkpointing.pause.between.seconds = 5000
      restart-strategy.attempts = 3
      restart-strategy.delay = 30000
      raw {
        consumer.parallelism = 1
        downstream.operators.parallelism = 1
      }
    }
    
    redisdb.connection.timeout = 30000
    redis {
      host = "redis-master"
      port = 6379
      database {
        default.id = 0
      }
    }
    
    redis-meta {
      host = "redis-master"
      port = 6379
      database {
        default.id = 0
      }
    }
    
    lms-cassandra {
      host = "cassandra"
      port = "9042"
      keyspace = ""
    }
    
    neo4j {
      routePath = "bolt://neo4j:7687"
      graph = "domain"
    }
    
    
    question {
      keyspace = "{{ .Values.global.env }}_question_store"
      table = "question_data"
    }
    
    questionset {
      keyspace = "{{ .Values.global.env }}_hierarchy_store"
      table = "questionset_hierarchy"
    }
    
    print_service.base_url = "http://print:5000"
    
    cloud_storage_type: ""
    cloud_storage_key: ""
    cloud_storage_secret: ""
    cloud_storage_endpoint: ""
    cloud_storage_container: ""
    
    master.category.validation.enabled = "Yes"
    
    cloudstorage {
      metadata.replace_absolute_path = false
      metadata.list = ["appIcon","posterImage","artifactUrl","downloadUrl","variants","previewUrl","pdfUrl"]
      relative_path_prefix = "CONTENT_STORAGE_BASE_PATH"
      read_base_path="https://"
      write_base_path=["https://"]
      #read_base_path = "https://{{ .Values.global.object_storage_endpoint }}"
      #write_base_path = ["https://{{ .Values.global.object_storage_endpoint }}"]
    }
    
    schema {
      basePath = "https:////schemas/local"
      supportedVersion {
        itemset = "2.0"
      }
    }
    

  base-config: |-
    kafka {
      broker-servers = "kafka:9092"
      producer.broker-servers = "kafka:9092"
      consumer.broker-servers = "kafka:9092"
      zookeeper = "zookeeper:2181"
      producer {
        max-request-size = 1572864
        batch.size = 98304
        linger.ms = 10
      }
      input {
        user_pii_topic = ".delete.user.job.request"
        ownership_transfer_topic = ".user.ownership.transfer"
      }
      groupId = "-user-pii-data-updater-group"
    }
    
    job {
          env = ""
          enable.distributed.checkpointing = true
          statebackend {
            base.url = "file:///tmp/flink-checkpoints"
          }
    
          checkpointing.interval = 60000
          checkpointing.pause.between.seconds = 5000
          checkpointing.compression.enabled = true
          restart-strategy.attempts = 3
          restart-strategy.delay = 30000
    }
    
    task {
      parallelism = 1
      consumer.parallelism = 1
      checkpointing.compressed = true
      checkpointing.interval = 60000
      checkpointing.pause.between.seconds = 5000
      restart-strategy.attempts = 3
      restart-strategy.delay = 30000
    }
    
    redis {
      host = redis-master
      port = 6379
    }
    
    lms-cassandra {
      host = "cassandra"
      port = 9042
    }
    
    neo4j {
      routePath = "bolt://neo4j:7687"
      graph = "domain"
    }
    
    es {
      basePath = "elasticsearch:9200"
    }
    
    schema {
      basePath = "https:////schemas/local"
      supportedVersion = {
        itemset = "2.0"
      }
    }
    

  log4j_console_properties: |-
    # This affects logging for both user code and Flink
    rootLogger.level = INFO
    rootLogger.appenderRef.console.ref = ConsoleAppender
    
    # Uncomment this if you want to _only_ change Flink's logging
    #logger.flink.name = org.apache.flink
    #logger.flink.level = INFO
    
    # The following lines keep the log level of common libraries/connectors on
    # log level INFO. The root logger does not override this. You have to manually
    # change the log levels here.
    logger.akka.name = akka
    logger.akka.level = ERROR
    logger.kafka.name= org.apache.kafka
    logger.kafka.level = ERROR
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = ERROR
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = ERROR
    
    # Log all infos to the console
    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    
    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF
---
# Source: inquirybb/charts/flink/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-pii-data-updater-config
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
data:
  flink-conf: |-
    jobmanager.memory.flink.size: 1024m
    taskmanager.memory.flink.size: 1024m
    taskmanager.numberOfTaskSlots: 1
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    

  config: |-
    include file("file:///data/flink/conf/base-config.conf")
    job {
      env = ""
      enable.distributed.checkpointing = true
      statebackend {
        base.url = "file:///tmp/flink-checkpoints"
      }
    
      checkpointing.interval = 60000
      checkpointing.pause.between.seconds = 5000
      checkpointing.compression.enabled = true
      restart-strategy.attempts = 3
      restart-strategy.delay = 30000
    }
    
    kafka {
      broker-servers = "kafka:9092"
      producer.broker-servers = "kafka:9092"
      consumer.broker-servers = "kafka:9092"
      zookeeper = "zookeeper:2181"
      
      input.topic = "{{ .Values.global.env }}.delete.user"
      groupId = "{{ .Values.global.env }}-user-pii-updater-group"
      
      producer {
        max-request-size = 1572864
        batch.size = 98304
        linger.ms = 10
      }
      input {
        user_pii_topic = ".delete.user.job.request"
        ownership_transfer_topic = ".user.ownership.transfer"
      }
    }
    
    task {
      parallelism = 1
      consumer.parallelism = 1
      downstream.operators.parallelism = 1
      router.parallelism = 1
      checkpointing.compressed = true
      checkpointing.interval = 60000
      checkpointing.pause.between.seconds = 5000
      restart-strategy.attempts = 3
      restart-strategy.delay = 30000
      raw {
        consumer.parallelism = 1
        downstream.operators.parallelism = 1
      }
    }
    
    redisdb.connection.timeout = 30000
    redis {
      host = "redis-master"
      port = 6379
      database {
        default.id = 0
      }
    }
    
    redis-meta {
      host = "redis-master"
      port = 6379
      database {
        default.id = 0
      }
    }
    
    neo4j {
      routePath = "bolt://neo4j:7687"
      graph = "domain"
    }
    
    lms-cassandra {
      host = "cassandra"
      port = "9042"
      keyspace = ""
    }
    
    target_object_types = { "Question": ["1.0", "1.1"], "QuestionSet": ["1.0", "1.1"], "Asset": ["1.0"], "Content": ["1.0"], "Collection": ["1.0"] }
    user_pii_replacement_value = "Deleted User"
    admin_email_notification_enable = true
    userorg_service_base_url = "http://userorg-service:9000"
    
    notification {
      email {
        subject = "User Account Deletion Notification"
        regards = "Team"
      }
    }
    
    schema {
      basePath = "https:////schemas/local"
      supportedVersion {
        itemset = "2.0"
      }
    }
    

  base-config: |-
    kafka {
      broker-servers = "kafka:9092"
      producer.broker-servers = "kafka:9092"
      consumer.broker-servers = "kafka:9092"
      zookeeper = "zookeeper:2181"
      producer {
        max-request-size = 1572864
        batch.size = 98304
        linger.ms = 10
      }
      input {
        user_pii_topic = ".delete.user.job.request"
        ownership_transfer_topic = ".user.ownership.transfer"
      }
      groupId = "-user-pii-data-updater-group"
    }
    
    job {
          env = ""
          enable.distributed.checkpointing = true
          statebackend {
            base.url = "file:///tmp/flink-checkpoints"
          }
    
          checkpointing.interval = 60000
          checkpointing.pause.between.seconds = 5000
          checkpointing.compression.enabled = true
          restart-strategy.attempts = 3
          restart-strategy.delay = 30000
    }
    
    task {
      parallelism = 1
      consumer.parallelism = 1
      checkpointing.compressed = true
      checkpointing.interval = 60000
      checkpointing.pause.between.seconds = 5000
      restart-strategy.attempts = 3
      restart-strategy.delay = 30000
    }
    
    redis {
      host = redis-master
      port = 6379
    }
    
    lms-cassandra {
      host = "cassandra"
      port = 9042
    }
    
    neo4j {
      routePath = "bolt://neo4j:7687"
      graph = "domain"
    }
    
    es {
      basePath = "elasticsearch:9200"
    }
    
    schema {
      basePath = "https:////schemas/local"
      supportedVersion = {
        itemset = "2.0"
      }
    }
    

  log4j_console_properties: |-
    # This affects logging for both user code and Flink
    rootLogger.level = INFO
    rootLogger.appenderRef.console.ref = ConsoleAppender
    
    # Uncomment this if you want to _only_ change Flink's logging
    #logger.flink.name = org.apache.flink
    #logger.flink.level = INFO
    
    # The following lines keep the log level of common libraries/connectors on
    # log level INFO. The root logger does not override this. You have to manually
    # change the log levels here.
    logger.akka.name = akka
    logger.akka.level = ERROR
    logger.kafka.name= org.apache.kafka
    logger.kafka.level = ERROR
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = ERROR
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = ERROR
    
    # Log all infos to the console
    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    
    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF
---
# Source: inquirybb/charts/kafka/charts/zookeeper/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: zookeeper-ibb-scripts
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-11.0.2
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.8.0"
    app.kubernetes.io/component: zookeeper
data:
  init-certs.sh: |-
    #!/bin/bash
  setup.sh: |-
    #!/bin/bash

    # Execute entrypoint as usual after obtaining ZOO_SERVER_ID
    # check ZOO_SERVER_ID in persistent volume via myid
    # if not present, set based on POD hostname
    if [[ -f "/bitnami/zookeeper/data/myid" ]]; then
        export ZOO_SERVER_ID="$(cat /bitnami/zookeeper/data/myid)"
    else
        HOSTNAME="$(hostname -s)"
        if [[ $HOSTNAME =~ (.*)-([0-9]+)$ ]]; then
            ORD=${BASH_REMATCH[2]}
            export ZOO_SERVER_ID="$((ORD + 1 ))"
        else
            echo "Failed to get index from hostname $HOST"
            exit 1
        fi
    fi
    exec /entrypoint.sh /run.sh
---
# Source: inquirybb/charts/kafka/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-ibb-scripts
  namespace: "default"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-20.0.2
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.3.1"
  annotations:
    helm.sh/hook-weight: "-5"
data:
  setup.sh: |-
    #!/bin/bash

    ID="${MY_POD_NAME#"kafka-ibb-"}"
    if [[ -f "/bitnami/kafka/data/meta.properties" ]]; then
        export KAFKA_CFG_BROKER_ID="$(grep "broker.id" "/bitnami/kafka/data/meta.properties" | awk -F '=' '{print $2}')"
    else
        export KAFKA_CFG_BROKER_ID="$((ID + 0))"
    fi

    # Configure zookeeper client

    exec /entrypoint.sh /run.sh
---
# Source: inquirybb/charts/neo4j/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: logstash-config-ibb
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: neo4j
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: neo4j-0.1.0
  annotations:
    helm.sh/hook-weight: "-5"
    reloader.stakater.com/auto: "true"
data:
  logstash.conf: |-
    input {
      file {
        start_position =>"beginning"
        type => "graph_event"
        path => ["/txn-handler/learning_graph_event_neo4j.log"]
        sincedb_path => "/usr/share/logstash/.sincedb_learning_graph_event_mw"
      }
    }
    filter {
      grok {
        match => [ "message",
                  "%{TIMESTAMP_ISO8601:timestamp} %{GREEDYDATA:msg}"]
      }
      mutate {
          gsub => [ "message","%{timestamp}","" ]
          strip => [ "message" ]
      }
      json {
          source => "message"
      }
    }
    output {
      kafka {
        bootstrap_servers => "kafka:9092"
        codec => plain {
            format => "%{message}"
        }
        message_key => "%{nodeUniqueId}"
        topic_id => ".knowlg.learning.graph.events"
        retries => 20
        retry_backoff_ms => 180000
      }
    }
---
# Source: inquirybb/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-ibb-configuration
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.8.3
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.9"
  annotations:
    helm.sh/hook-weight: "-5"
data:
  redis.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
  master.conf: |-
    dir /data
    # User-supplied master configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of master configuration
  replica.conf: |-
    dir /data
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
---
# Source: inquirybb/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-ibb-health
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.8.3
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.9"
  annotations:
    helm.sh/hook-weight: "-5"
data:
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: inquirybb/charts/redis/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-ibb-scripts
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.8.3
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.9"
  annotations:
    helm.sh/hook-weight: "-5"
data:
  start-master.sh: |
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--protected-mode" "no")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
    exec redis-server "${ARGS[@]}"
  start-replica.sh: |
    #!/bin/bash

    get_port() {
        hostname="$1"
        type="$2"

        port_var=$(echo "${hostname^^}_SERVICE_PORT_$type" | sed "s/-/_/g")
        port=${!port_var}

        if [ -z "$port" ]; then
            case $type in
                "SENTINEL")
                    echo 26379
                    ;;
                "REDIS")
                    echo 6379
                    ;;
            esac
        else
            echo $port
        fi
    }

    get_full_hostname() {
        hostname="$1"
        full_hostname="${hostname}.${HEADLESS_SERVICE}"
        echo "${full_hostname}"
    }

    REDISPORT=$(get_port "$HOSTNAME" "REDIS")

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/replica.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi

    echo "" >> /opt/bitnami/redis/etc/replica.conf
    echo "replica-announce-port $REDISPORT" >> /opt/bitnami/redis/etc/replica.conf
    echo "replica-announce-ip $(get_full_hostname "$HOSTNAME")" >> /opt/bitnami/redis/etc/replica.conf
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--replicaof" "${REDIS_MASTER_HOST}" "${REDIS_MASTER_PORT_NUMBER}")
    ARGS+=("--protected-mode" "no")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/replica.conf")
    exec redis-server "${ARGS[@]}"
---
# Source: inquirybb/charts/neo4j/templates/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: neo4j-ibb-claim
  labels:
    app: neo4j-ibb
  annotations: 
    helm.sh/resource-policy: "keep"
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
---
# Source: inquirybb/charts/assessment/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: assessment-service
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: assessment
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: assessment-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
  ports:
  - name: http-assessment
    port: 9000
    targetPort: 9000
  selector:
    app.kubernetes.io/name: assessment
---
# Source: inquirybb/charts/cassandra/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: cassandra-ibb-headless
  namespace: "default"
  labels:
    app.kubernetes.io/name: cassandra
    helm.sh/chart: cassandra-10.1.0
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "4.1.0"
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: intra
      port: 7000
      targetPort: intra
    - name: tls
      port: 7001
      targetPort: tls
    - name: jmx
      port: 7199
      targetPort: jmx
    - name: cql
      port: 9042
      targetPort: cql
  selector:
    app.kubernetes.io/name: cassandra
    app.kubernetes.io/instance: async-questionset-publish
---
# Source: inquirybb/charts/cassandra/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: cassandra-ibb
  namespace: "default"
  labels:
    app.kubernetes.io/name: cassandra
    helm.sh/chart: cassandra-10.1.0
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "4.1.0"
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: cql
      port: 9042
      targetPort: cql
      nodePort: null
    - name: metrics
      port: 8080
      nodePort: null
  selector:
    app.kubernetes.io/name: cassandra
    app.kubernetes.io/instance: async-questionset-publish
---
# Source: inquirybb/charts/elasticsearch/templates/master/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch-ibb-master-hl
  namespace: "default"
  labels: 
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-19.5.4
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "8.5.2"
    app.kubernetes.io/component: master
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  type: ClusterIP
  clusterIP: ""
  publishNotReadyAddresses: true
  ports:
    - name: tcp-rest-api
      port: 9200
      targetPort: rest-api
    - name: tcp-transport
      port: 9300
      targetPort: transport
  selector:
    app.kubernetes.io/name: elasticsearch
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/component: master
---
# Source: inquirybb/charts/elasticsearch/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch-ibb
  namespace: "default"
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-19.5.4
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "8.5.2"
    app.kubernetes.io/component: master
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-rest-api
      port: 9200
      targetPort: rest-api
      nodePort: null
    - name: tcp-transport
      port: 9300
      nodePort: null
  selector:
    app.kubernetes.io/name: elasticsearch
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/component: master
---
# Source: inquirybb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: async-questionset-publish-jobmanager
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: http-flink
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: async-questionset-publish-jobmanager
---
# Source: inquirybb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: async-questionset-publish-taskmanager
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: http-flink
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: async-questionset-publish-taskmanager
---
# Source: inquirybb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: questionset-republish-jobmanager
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: http-flink
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: questionset-republish-jobmanager
---
# Source: inquirybb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: questionset-republish-taskmanager
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: http-flink
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: questionset-republish-taskmanager
---
# Source: inquirybb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: user-pii-data-updater-jobmanager
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: http-flink
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: user-pii-data-updater-jobmanager
---
# Source: inquirybb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: user-pii-data-updater-taskmanager
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: http-flink
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: user-pii-data-updater-taskmanager
---
# Source: inquirybb/charts/kafka/charts/zookeeper/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: zookeeper-ibb-headless
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-11.0.2
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.8.0"
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/component: zookeeper
---
# Source: inquirybb/charts/kafka/charts/zookeeper/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: zookeeper-ibb
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-11.0.2
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.8.0"
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
      nodePort: null
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/component: zookeeper
---
# Source: inquirybb/charts/kafka/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: kafka-ibb-headless
  namespace: "default"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-20.0.2
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.3.1"
    app.kubernetes.io/component: kafka
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: false
  ports:
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: kafka-client
    - name: tcp-internal
      port: 9093
      protocol: TCP
      targetPort: kafka-internal
  selector:
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/component: kafka
---
# Source: inquirybb/charts/kafka/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: kafka-ibb
  namespace: "default"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-20.0.2
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.3.1"
    app.kubernetes.io/component: kafka
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: kafka-client
      nodePort: null
  selector:
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/component: kafka
---
# Source: inquirybb/charts/neo4j/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: neo4j-ibb
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: neo4j
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: neo4j-0.1.0
  annotations:
    helm.sh/hook-weight: "-5"
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
  ports:
  - name: cypher-port
    port: 7474
    targetPort: 7474
  - name: bolt-port-1
    port: 7687
    targetPort: 7687
  - name: bolt-port-2
    port: 8687
    targetPort: 8687
  selector:
    app.kubernetes.io/name: neo4j
---
# Source: inquirybb/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-ibb-headless
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.8.3
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.9"
  annotations:
    helm.sh/hook-weight: "-5"
    
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: async-questionset-publish
---
# Source: inquirybb/charts/redis/templates/master/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-ibb-master
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.8.3
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.9"
    app.kubernetes.io/component: master
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  type: ClusterIP
  internalTrafficPolicy: Cluster
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/component: master
---
# Source: inquirybb/charts/redis/templates/replicas/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-ibb-replicas
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.8.3
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.9"
    app.kubernetes.io/component: replica
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  type: ClusterIP
  internalTrafficPolicy: Cluster
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/component: replica
---
# Source: inquirybb/charts/assessment/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: assessment
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: assessment
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: assessment-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: assessment
  template:
    metadata:
      labels:
        app.kubernetes.io/name: assessment
      annotations:
        checksum/config: 3333e58d102784ef0aae5c8db2086293cc5c75d7b22572ae9262d7f47a984c68
    spec:
      serviceAccountName: assessment
      securityContext:
        fsGroup: 1001
        runAsNonRoot: true
        runAsUser: 1001
      containers:
        - name: assessment
          image: "sunbirded.azurecr.io/assessment-service:release-7.0.0_RC2_aeccd6e_16"
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 9000
            initialDelaySeconds: 15
            periodSeconds: 15
            timeoutSeconds: 5
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 9000
            initialDelaySeconds: 15
            periodSeconds: 15
            timeoutSeconds: 5
          ports:
            - name: http-assessment
              containerPort: 9000
          resources:
            limits:
              cpu: 1
              memory: 1G
            requests:
              cpu: 100m
              memory: 100M
          securityContext:
            {}
          envFrom:
          - configMapRef:
              name: assessment-env
          volumeMounts:
          - name: config
            mountPath: /home/sunbird/assessment-service-1.0-SNAPSHOT/config
      volumes:
      - name: env
        configMap:
          name: assessment-env
      - name: config
        configMap:
          name: assessment
---
# Source: inquirybb/charts/flink/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: async-questionset-publish-taskmanager
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    checksum/job-config: b49ad56b25d1cd8add600eb8c458b979c768dbddaa06c855552c900f28ccd64b
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: flink
      app.kubernetes.io/component: async-questionset-publish-taskmanager
  template:
    metadata:
      labels:
        app.kubernetes.io/name: flink
        app.kubernetes.io/component: async-questionset-publish-taskmanager
      annotations:
        checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/job-config: b49ad56b25d1cd8add600eb8c458b979c768dbddaa06c855552c900f28ccd64b
    spec:
      serviceAccountName: flink
      securityContext:
        fsGroup: 0
        runAsUser: 0
      containers:
        - name: flink
          image: "sunbirded.azurecr.io/data-pipeline:release-8.0.0_RC1_b4f909c"
          imagePullPolicy: IfNotPresent
          workingDir: 
          command: ["/opt/flink/bin/taskmanager.sh"]
          args: ["start-foreground",
            "-Dweb.submit.enable=false",
            "-Dmetrics.reporter.prom.class=org.apache.flink.metrics.prometheus.PrometheusReporter",
            "-Dmetrics.reporter.prom.host=async-questionset-publish-taskmanager",
            "-Dmetrics.reporter.prom.port=9251-9260",
            "-Djobmanager.rpc.address=async-questionset-publish-jobmanager",
            "-Dtaskmanager.rpc.port=6122"]
          ports:
            - name: http-flink
              containerPort: 8081
          resources:
            limits:
              cpu: 1
              memory: 2048Mi
            requests:
              cpu: 100m
              memory: 500M
          securityContext:
            {}
          volumeMounts:
            - name: flink-config-volume
              mountPath: /opt/flink/conf/flink-conf.yaml
              subPath: flink-conf.yaml
            - name: flink-config-volume
              mountPath: /opt/flink/conf/log4j-console.properties
              subPath: log4j-console.properties
      volumes:
        - name: flink-config-volume
          configMap:
            name: async-questionset-publish-config
            items:
              - key: flink-conf
                path: flink-conf.yaml
              - key: log4j_console_properties
                path: log4j-console.properties
---
# Source: inquirybb/charts/flink/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: async-questionset-publish-jobmanager
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    checksum/job-config: b49ad56b25d1cd8add600eb8c458b979c768dbddaa06c855552c900f28ccd64b
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: flink
      app.kubernetes.io/component: async-questionset-publish-jobmanager
  template:
    metadata:
      labels:
        app.kubernetes.io/name: flink
        app.kubernetes.io/component: async-questionset-publish-jobmanager
      annotations:
        checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/job-config: b49ad56b25d1cd8add600eb8c458b979c768dbddaa06c855552c900f28ccd64b
    spec:
      serviceAccountName: flink
      securityContext:
        fsGroup: 0
        runAsUser: 0
      containers:
        - name: flink
          image: "sunbirded.azurecr.io/data-pipeline:release-8.0.0_RC1_b4f909c"
          imagePullPolicy: IfNotPresent
          workingDir: 
          command: ["/opt/flink/bin/standalone-job.sh"]
          args: ["start-foreground",
            "--job-classname=org.sunbird.job.questionset.task.QuestionSetPublishStreamTask",
            "-Dweb.submit.enable=false",
            "-Dmetrics.reporter.prom.class=org.apache.flink.metrics.prometheus.PrometheusReporter",
            "-Dmetrics.reporter.prom.port=9250",
            "-Djobmanager.rpc.address=async-questionset-publish-jobmanager",
            "-Djobmanager.rpc.port=6123",
            "-Dparallelism.default=1",
            "-Dblob.server.port=6124",
            "-Dqueryable-state.server.ports=6125",
            "--config.file.path",
            "/data/flink/conf/async-questionset-publish.conf"]
          ports:
            - name: http-flink
              containerPort: 8081
          resources:
            limits:
              cpu: 1
              memory: 2048Mi
            requests:
              cpu: 100m
              memory: 500M
          securityContext:
            {}
          volumeMounts:
            - name: flink-config-volume
              mountPath: /opt/flink/conf/flink-conf.yaml
              subPath: flink-conf.yaml
            - name: flink-config-volume
              mountPath: /data/flink/conf/base-config.conf
              subPath: base-config.conf
            - name: flink-config-volume
              mountPath: /data/flink/conf/async-questionset-publish.conf
              subPath: async-questionset-publish.conf
            - name: flink-config-volume
              mountPath: /opt/flink/conf/log4j-console.properties
              subPath: log4j-console.properties
      volumes:
        - name: flink-config-volume
          configMap:
            name: async-questionset-publish-config
            items:
              - key: flink-conf
                path: flink-conf.yaml
              - key: base-config
                path: base-config.conf
              - key: config
                path: async-questionset-publish.conf
              - key: log4j_console_properties
                path: log4j-console.properties
---
# Source: inquirybb/charts/flink/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: questionset-republish-taskmanager
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    checksum/job-config: c17d3919b96c77004d1700c5d55279912a298c1bbb21cb10c8bb05361671f248
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: flink
      app.kubernetes.io/component: questionset-republish-taskmanager
  template:
    metadata:
      labels:
        app.kubernetes.io/name: flink
        app.kubernetes.io/component: questionset-republish-taskmanager
      annotations:
        checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/job-config: c17d3919b96c77004d1700c5d55279912a298c1bbb21cb10c8bb05361671f248
    spec:
      serviceAccountName: flink
      securityContext:
        fsGroup: 0
        runAsUser: 0
      containers:
        - name: flink
          image: "sunbirded.azurecr.io/data-pipeline:release-8.0.0_RC1_b4f909c"
          imagePullPolicy: IfNotPresent
          workingDir: 
          command: ["/opt/flink/bin/taskmanager.sh"]
          args: ["start-foreground",
            "-Dweb.submit.enable=false",
            "-Dmetrics.reporter.prom.class=org.apache.flink.metrics.prometheus.PrometheusReporter",
            "-Dmetrics.reporter.prom.host=questionset-republish-taskmanager",
            "-Dmetrics.reporter.prom.port=9251-9260",
            "-Djobmanager.rpc.address=questionset-republish-jobmanager",
            "-Dtaskmanager.rpc.port=6122"]
          ports:
            - name: http-flink
              containerPort: 8081
          resources:
            limits:
              cpu: 1
              memory: 2048Mi
            requests:
              cpu: 100m
              memory: 500M
          securityContext:
            {}
          volumeMounts:
            - name: flink-config-volume
              mountPath: /opt/flink/conf/flink-conf.yaml
              subPath: flink-conf.yaml
            - name: flink-config-volume
              mountPath: /opt/flink/conf/log4j-console.properties
              subPath: log4j-console.properties
      volumes:
        - name: flink-config-volume
          configMap:
            name: questionset-republish-config
            items:
              - key: flink-conf
                path: flink-conf.yaml
              - key: log4j_console_properties
                path: log4j-console.properties
---
# Source: inquirybb/charts/flink/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: questionset-republish-jobmanager
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    checksum/job-config: c17d3919b96c77004d1700c5d55279912a298c1bbb21cb10c8bb05361671f248
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: flink
      app.kubernetes.io/component: questionset-republish-jobmanager
  template:
    metadata:
      labels:
        app.kubernetes.io/name: flink
        app.kubernetes.io/component: questionset-republish-jobmanager
      annotations:
        checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/job-config: c17d3919b96c77004d1700c5d55279912a298c1bbb21cb10c8bb05361671f248
    spec:
      serviceAccountName: flink
      securityContext:
        fsGroup: 0
        runAsUser: 0
      containers:
        - name: flink
          image: "sunbirded.azurecr.io/data-pipeline:release-8.0.0_RC1_b4f909c"
          imagePullPolicy: IfNotPresent
          workingDir: 
          command: ["/opt/flink/bin/standalone-job.sh"]
          args: ["start-foreground",
            "--job-classname=org.sunbird.job.questionset.republish.task.QuestionSetRePublishStreamTask",
            "-Dweb.submit.enable=false",
            "-Dmetrics.reporter.prom.class=org.apache.flink.metrics.prometheus.PrometheusReporter",
            "-Dmetrics.reporter.prom.port=9250",
            "-Djobmanager.rpc.address=questionset-republish-jobmanager",
            "-Djobmanager.rpc.port=6123",
            "-Dparallelism.default=1",
            "-Dblob.server.port=6124",
            "-Dqueryable-state.server.ports=6125",
            "--config.file.path",
            "/data/flink/conf/questionset-republish.conf"]
          ports:
            - name: http-flink
              containerPort: 8081
          resources:
            limits:
              cpu: 1
              memory: 2048Mi
            requests:
              cpu: 100m
              memory: 500M
          securityContext:
            {}
          volumeMounts:
            - name: flink-config-volume
              mountPath: /opt/flink/conf/flink-conf.yaml
              subPath: flink-conf.yaml
            - name: flink-config-volume
              mountPath: /data/flink/conf/base-config.conf
              subPath: base-config.conf
            - name: flink-config-volume
              mountPath: /data/flink/conf/questionset-republish.conf
              subPath: questionset-republish.conf
            - name: flink-config-volume
              mountPath: /opt/flink/conf/log4j-console.properties
              subPath: log4j-console.properties
      volumes:
        - name: flink-config-volume
          configMap:
            name: questionset-republish-config
            items:
              - key: flink-conf
                path: flink-conf.yaml
              - key: base-config
                path: base-config.conf
              - key: config
                path: questionset-republish.conf
              - key: log4j_console_properties
                path: log4j-console.properties
---
# Source: inquirybb/charts/flink/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-pii-data-updater-taskmanager
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    checksum/job-config: cefbb2e0da3fa71319cef185c7dcfd28e0d2d76f86670f4d723826a5b55087ac
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: flink
      app.kubernetes.io/component: user-pii-data-updater-taskmanager
  template:
    metadata:
      labels:
        app.kubernetes.io/name: flink
        app.kubernetes.io/component: user-pii-data-updater-taskmanager
      annotations:
        checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/job-config: cefbb2e0da3fa71319cef185c7dcfd28e0d2d76f86670f4d723826a5b55087ac
    spec:
      serviceAccountName: flink
      securityContext:
        fsGroup: 0
        runAsUser: 0
      containers:
        - name: flink
          image: "sunbirded.azurecr.io/data-pipeline:release-8.0.0_RC1_b4f909c"
          imagePullPolicy: IfNotPresent
          workingDir: 
          command: ["/opt/flink/bin/taskmanager.sh"]
          args: ["start-foreground",
            "-Dweb.submit.enable=false",
            "-Dmetrics.reporter.prom.class=org.apache.flink.metrics.prometheus.PrometheusReporter",
            "-Dmetrics.reporter.prom.host=user-pii-data-updater-taskmanager",
            "-Dmetrics.reporter.prom.port=9251-9260",
            "-Djobmanager.rpc.address=user-pii-data-updater-jobmanager",
            "-Dtaskmanager.rpc.port=6122"]
          ports:
            - name: http-flink
              containerPort: 8081
          resources:
            limits:
              cpu: 1
              memory: 2048Mi
            requests:
              cpu: 100m
              memory: 500M
          securityContext:
            {}
          volumeMounts:
            - name: flink-config-volume
              mountPath: /opt/flink/conf/flink-conf.yaml
              subPath: flink-conf.yaml
            - name: flink-config-volume
              mountPath: /opt/flink/conf/log4j-console.properties
              subPath: log4j-console.properties
      volumes:
        - name: flink-config-volume
          configMap:
            name: user-pii-data-updater-config
            items:
              - key: flink-conf
                path: flink-conf.yaml
              - key: log4j_console_properties
                path: log4j-console.properties
---
# Source: inquirybb/charts/flink/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-pii-data-updater-jobmanager
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    checksum/job-config: cefbb2e0da3fa71319cef185c7dcfd28e0d2d76f86670f4d723826a5b55087ac
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: flink
      app.kubernetes.io/component: user-pii-data-updater-jobmanager
  template:
    metadata:
      labels:
        app.kubernetes.io/name: flink
        app.kubernetes.io/component: user-pii-data-updater-jobmanager
      annotations:
        checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/job-config: cefbb2e0da3fa71319cef185c7dcfd28e0d2d76f86670f4d723826a5b55087ac
    spec:
      serviceAccountName: flink
      securityContext:
        fsGroup: 0
        runAsUser: 0
      containers:
        - name: flink
          image: "sunbirded.azurecr.io/data-pipeline:release-8.0.0_RC1_b4f909c"
          imagePullPolicy: IfNotPresent
          workingDir: 
          command: ["/opt/flink/bin/standalone-job.sh"]
          args: ["start-foreground",
            "--job-classname=org.sunbird.job.user.pii.updater.task.UserPiiUpdaterStreamTask",
            "-Dweb.submit.enable=false",
            "-Dmetrics.reporter.prom.class=org.apache.flink.metrics.prometheus.PrometheusReporter",
            "-Dmetrics.reporter.prom.port=9250",
            "-Djobmanager.rpc.address=user-pii-data-updater-jobmanager",
            "-Djobmanager.rpc.port=6123",
            "-Dparallelism.default=1",
            "-Dblob.server.port=6124",
            "-Dqueryable-state.server.ports=6125",
            "--config.file.path",
            "/data/flink/conf/user-pii-data-updater.conf"]
          ports:
            - name: http-flink
              containerPort: 8081
          resources:
            limits:
              cpu: 1
              memory: 2048Mi
            requests:
              cpu: 100m
              memory: 500M
          securityContext:
            {}
          volumeMounts:
            - name: flink-config-volume
              mountPath: /opt/flink/conf/flink-conf.yaml
              subPath: flink-conf.yaml
            - name: flink-config-volume
              mountPath: /data/flink/conf/base-config.conf
              subPath: base-config.conf
            - name: flink-config-volume
              mountPath: /data/flink/conf/user-pii-data-updater.conf
              subPath: user-pii-data-updater.conf
            - name: flink-config-volume
              mountPath: /opt/flink/conf/log4j-console.properties
              subPath: log4j-console.properties
      volumes:
        - name: flink-config-volume
          configMap:
            name: user-pii-data-updater-config
            items:
              - key: flink-conf
                path: flink-conf.yaml
              - key: base-config
                path: base-config.conf
              - key: config
                path: user-pii-data-updater.conf
              - key: log4j_console_properties
                path: log4j-console.properties
---
# Source: inquirybb/charts/neo4j/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: neo4j-ibb
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: neo4j
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: neo4j-0.1.0
  annotations:
    helm.sh/hook-weight: "-5"
    reloader.stakater.com/auto: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: neo4j
  template:
    metadata:
      labels:
        app.kubernetes.io/name: neo4j
      annotations:
        checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    spec:
      serviceAccountName: neo4j-ibb
      securityContext:
        null
      containers:
        - name: neo4j
          image: "sunbirded.azurecr.io/neo4j:3.3.0"
          imagePullPolicy: IfNotPresent
          env:
            - name: NEO4J_dbms_security_auth__enabled
              value: "false"
          ports:
            - name: cypher-port
              containerPort: 7474
            - name: bolt-port-1
              containerPort: 7687
            - name: bolt-port-2
              containerPort: 8687
          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi
          securityContext:
            null
          volumeMounts:
          - name: shared-data
            mountPath: /var/lib/neo4j/logs/plugins/txn-handler
          - name: neo4j-data
            mountPath: "/data"
        - name: logstash
          image: logstash:6.8.21
          imagePullPolicy: Always
          volumeMounts:
            - name: config
              mountPath: /usr/share/logstash/pipeline/logstash.conf
              subPath: logstash.conf
            - name: shared-data
              mountPath: /txn-handler
      volumes:
      - name: config
        configMap:
          name: logstash-config-ibb
          items:
          - key: logstash.conf
            path: logstash.conf
      - name: shared-data
        emptyDir: {}
      - name: neo4j-data
        persistentVolumeClaim:
          claimName: neo4j-ibb-claim
---
# Source: inquirybb/charts/cassandra/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cassandra-ibb
  namespace: "default"
  labels:
    app.kubernetes.io/name: cassandra
    helm.sh/chart: cassandra-10.1.0
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "4.1.0"
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: cassandra
      app.kubernetes.io/instance: async-questionset-publish
  serviceName: cassandra-ibb-headless
  podManagementPolicy: OrderedReady
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: cassandra
        helm.sh/chart: cassandra-10.1.0
        app.kubernetes.io/instance: async-questionset-publish
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: "4.1.0"
    spec:
      
      serviceAccountName: cassandra-ibb
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: async-questionset-publish
                    app.kubernetes.io/name: cassandra
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      containers:
        - name: cassandra
          command:
            - bash
            - -ec
            - |
              # Node 0 is the password seeder
              if [[ $POD_NAME =~ (.*)-0$ ]]; then
                  echo "Setting node as password seeder"
                  export CASSANDRA_PASSWORD_SEEDER=yes
              else
                  # Only node 0 will execute the startup initdb scripts
                  export CASSANDRA_IGNORE_INITDB_SCRIPTS=1
              fi
              /opt/bitnami/scripts/cassandra/entrypoint.sh /opt/bitnami/scripts/cassandra/run.sh
          image: docker.io/bitnami/cassandra:3.11.13-debian-11-r3
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: CASSANDRA_CLUSTER_NAME
              value: cassandra
            - name: CASSANDRA_SEEDS
              value: "cassandra-ibb-0.cassandra-ibb-headless.default.svc.cluster.local"
            - name: CASSANDRA_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: cassandra-ibb
                  key: cassandra-password
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: CASSANDRA_USER
              value: "cassandra"
            - name: CASSANDRA_NUM_TOKENS
              value: "256"
            - name: CASSANDRA_DATACENTER
              value: dc1
            - name: CASSANDRA_ENDPOINT_SNITCH
              value: SimpleSnitch
            - name: CASSANDRA_KEYSTORE_LOCATION
              value: "/opt/bitnami/cassandra/certs/keystore"
            - name: CASSANDRA_TRUSTSTORE_LOCATION
              value: "/opt/bitnami/cassandra/certs/truststore"
            - name: CASSANDRA_RACK
              value: rack1
            - name: CASSANDRA_TRANSPORT_PORT_NUMBER
              value: "7000"
            - name: CASSANDRA_JMX_PORT_NUMBER
              value: "7199"
            - name: CASSANDRA_CQL_PORT_NUMBER
              value: "9042"
            - name: CASSANDRA_AUTHENTICATOR
              value: AllowAllAuthenticator
            - name: CASSANDRA_AUTHORIZER
              value: AllowAllAuthorizer
          envFrom:
          livenessProbe:
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  nodetool info | grep "Native Transport active: true"
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 30
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  nodetool status | grep -E "^UN\\s+${POD_IP}"
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 30
            successThreshold: 1
            failureThreshold: 5
          lifecycle:
            preStop:
              exec:
                command:
                  - bash
                  - -ec
                  - nodetool drain
          ports:
            - name: intra
              containerPort: 7000
            - name: tls
              containerPort: 7001
            - name: jmx
              containerPort: 7199
            - name: cql
              containerPort: 9042
          resources: 
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /bitnami/cassandra
            
      volumes:
        - name: metrics-conf
          configMap:
            name: cassandra-ibb-metrics-conf
  volumeClaimTemplates:
    - metadata:
        name: data
        labels:
          app.kubernetes.io/name: cassandra
          app.kubernetes.io/instance: async-questionset-publish
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "25Gi"
---
# Source: inquirybb/charts/elasticsearch/templates/master/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch-ibb-master
  namespace: "default"
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-19.5.4
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "8.5.2"
    app.kubernetes.io/component: master
    ## Istio Labels: https://istio.io/docs/ops/deployment/requirements/
    app: master
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  replicas: 1
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app.kubernetes.io/name: elasticsearch
      app.kubernetes.io/instance: async-questionset-publish
      app.kubernetes.io/component: master
  serviceName: elasticsearch-ibb-master-hl
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: elasticsearch
        helm.sh/chart: elasticsearch-19.5.4
        app.kubernetes.io/instance: async-questionset-publish
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: "8.5.2"
        app.kubernetes.io/component: master
        ## Istio Labels: https://istio.io/docs/ops/deployment/requirements/
        app: master
      annotations:
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
        ## Image that performs the sysctl operation to modify Kernel settings (needed sometimes to avoid boot errors)
        - name: sysctl
          image: docker.io/bitnami/bitnami-shell-archived:11-debian-11-r54
          imagePullPolicy: "IfNotPresent"
          command:
            - /bin/bash
            - -ec
            - |
              CURRENT=`sysctl -n vm.max_map_count`;
              DESIRED="262144";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w vm.max_map_count=262144;
              fi;
              CURRENT=`sysctl -n fs.file-max`;
              DESIRED="65536";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w fs.file-max=65536;
              fi;
          securityContext:
            privileged: true
            runAsUser: 0
          resources:
            limits: {}
            requests: {}
      containers:
        - name: elasticsearch
          image: docker.io/bitnami/elasticsearch:6.8.23
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: ELASTICSEARCH_IS_DEDICATED_NODE
              value: "no"
            - name: ELASTICSEARCH_NODE_ROLES
              value: "master"
            - name: ELASTICSEARCH_TRANSPORT_PORT_NUMBER
              value: "9300"
            - name: ELASTICSEARCH_HTTP_PORT_NUMBER
              value: "9200"
            - name: ELASTICSEARCH_CLUSTER_NAME
              value: "elastic"
            - name: ELASTICSEARCH_CLUSTER_HOSTS
              value: "elasticsearch-ibb-master-hl.default.svc.cluster.local,"
            - name: ELASTICSEARCH_TOTAL_NODES
              value: "1"
            - name: ELASTICSEARCH_CLUSTER_MASTER_HOSTS
              value: elasticsearch-ibb-master-0 
            - name: ELASTICSEARCH_MINIMUM_MASTER_NODES
              value: "1"
            - name: ELASTICSEARCH_ADVERTISED_HOSTNAME
              value: "$(MY_POD_NAME).elasticsearch-ibb-master-hl.default.svc.cluster.local"
            - name: ELASTICSEARCH_HEAP_SIZE
              value: "2G"
          ports:
            - name: rest-api
              containerPort: 9200
            - name: transport
              containerPort: 9300
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 90
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 90
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          volumeMounts:
            - name: data
              mountPath: /bitnami/elasticsearch/data
      volumes:
  volumeClaimTemplates:
    - metadata:
        name: "data"
        annotations:
          helm.sh/hook-weight: "-5"
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: inquirybb/charts/kafka/charts/zookeeper/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: zookeeper-ibb
  namespace: default
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-11.0.2
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.8.0"
    app.kubernetes.io/component: zookeeper
    role: zookeeper
spec:
  replicas: 1
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app.kubernetes.io/name: zookeeper
      app.kubernetes.io/instance: async-questionset-publish
      app.kubernetes.io/component: zookeeper
  serviceName: zookeeper-ibb-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      annotations:
      labels:
        app.kubernetes.io/name: zookeeper
        helm.sh/chart: zookeeper-11.0.2
        app.kubernetes.io/instance: async-questionset-publish
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: "3.8.0"
        app.kubernetes.io/component: zookeeper
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: async-questionset-publish
                    app.kubernetes.io/name: zookeeper
                    app.kubernetes.io/component: zookeeper
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
      containers:
        - name: zookeeper
          image: docker.io/bitnami/zookeeper:3.8.0-debian-11-r65
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /scripts/setup.sh
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: ZOO_DATA_LOG_DIR
              value: ""
            - name: ZOO_PORT_NUMBER
              value: "2181"
            - name: ZOO_TICK_TIME
              value: "2000"
            - name: ZOO_INIT_LIMIT
              value: "10"
            - name: ZOO_SYNC_LIMIT
              value: "5"
            - name: ZOO_PRE_ALLOC_SIZE
              value: "65536"
            - name: ZOO_SNAPCOUNT
              value: "100000"
            - name: ZOO_MAX_CLIENT_CNXNS
              value: "60"
            - name: ZOO_4LW_COMMANDS_WHITELIST
              value: "srvr, mntr, ruok"
            - name: ZOO_LISTEN_ALLIPS_ENABLED
              value: "no"
            - name: ZOO_AUTOPURGE_INTERVAL
              value: "0"
            - name: ZOO_AUTOPURGE_RETAIN_COUNT
              value: "3"
            - name: ZOO_MAX_SESSION_TIMEOUT
              value: "40000"
            - name: ZOO_SERVERS
              value: zookeeper-ibb-0.zookeeper-ibb-headless.default.svc.cluster.local:2888:3888::1 
            - name: ZOO_ENABLE_AUTH
              value: "no"
            - name: ZOO_ENABLE_QUORUM_AUTH
              value: "no"
            - name: ZOO_HEAP_SIZE
              value: "1024"
            - name: ZOO_LOG_LEVEL
              value: "ERROR"
            - name: ALLOW_ANONYMOUS_LOGIN
              value: "yes"
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
          ports:
            - name: client
              containerPort: 2181
            - name: follower
              containerPort: 2888
            - name: election
              containerPort: 3888
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
          volumeMounts:
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
            - name: data
              mountPath: /bitnami/zookeeper
      volumes:
        - name: scripts
          configMap:
            name: zookeeper-ibb-scripts
            defaultMode: 0755
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "2Gi"
---
# Source: inquirybb/charts/kafka/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka-ibb
  namespace: "default"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-20.0.2
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.3.1"
    app.kubernetes.io/component: kafka
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: kafka
      app.kubernetes.io/instance: async-questionset-publish
      app.kubernetes.io/component: kafka
  serviceName: kafka-ibb-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kafka
        helm.sh/chart: kafka-20.0.2
        app.kubernetes.io/instance: async-questionset-publish
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: "3.3.1"
        app.kubernetes.io/component: kafka
      annotations:
    spec:
      
      hostNetwork: false
      hostIPC: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: async-questionset-publish
                    app.kubernetes.io/name: kafka
                    app.kubernetes.io/component: kafka
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      serviceAccountName: kafka-ibb
      containers:
        - name: kafka
          image: docker.io/bitnami/kafka:3.3.1-debian-11-r25
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /scripts/setup.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: KAFKA_CFG_ZOOKEEPER_CONNECT
              value: "zookeeper-ibb"
            - name: KAFKA_INTER_BROKER_LISTENER_NAME
              value: "INTERNAL"
            - name: KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP
              value: "INTERNAL:PLAINTEXT,CLIENT:PLAINTEXT"
            - name: KAFKA_CFG_LISTENERS
              value: "INTERNAL://:9093,CLIENT://:9092"
            - name: KAFKA_CFG_ADVERTISED_LISTENERS
              value: "INTERNAL://$(MY_POD_NAME).kafka-ibb-headless.default.svc.cluster.local:9093,CLIENT://$(MY_POD_NAME).kafka-ibb-headless.default.svc.cluster.local:9092"
            - name: ALLOW_PLAINTEXT_LISTENER
              value: "yes"
            - name: KAFKA_ZOOKEEPER_PROTOCOL
              value: PLAINTEXT
            - name: KAFKA_VOLUME_DIR
              value: "/bitnami/kafka"
            - name: KAFKA_LOG_DIR
              value: "/opt/bitnami/kafka/logs"
            - name: KAFKA_CFG_DELETE_TOPIC_ENABLE
              value: "false"
            - name: KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE
              value: "true"
            - name: KAFKA_HEAP_OPTS
              value: "-Xmx1024m -Xms1024m"
            - name: KAFKA_CFG_LOG_FLUSH_INTERVAL_MESSAGES
              value: "10000"
            - name: KAFKA_CFG_LOG_FLUSH_INTERVAL_MS
              value: "1000"
            - name: KAFKA_CFG_LOG_RETENTION_BYTES
              value: "1073741824"
            - name: KAFKA_CFG_LOG_RETENTION_CHECK_INTERVAL_MS
              value: "300000"
            - name: KAFKA_CFG_LOG_RETENTION_HOURS
              value: "168"
            - name: KAFKA_CFG_MESSAGE_MAX_BYTES
              value: "1000012"
            - name: KAFKA_CFG_LOG_SEGMENT_BYTES
              value: "1073741824"
            - name: KAFKA_CFG_LOG_DIRS
              value: "/bitnami/kafka/data"
            - name: KAFKA_CFG_DEFAULT_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR
              value: "1"
            - name: KAFKA_CFG_NUM_IO_THREADS
              value: "8"
            - name: KAFKA_CFG_NUM_NETWORK_THREADS
              value: "3"
            - name: KAFKA_CFG_NUM_PARTITIONS
              value: "1"
            - name: KAFKA_CFG_NUM_RECOVERY_THREADS_PER_DATA_DIR
              value: "1"
            - name: KAFKA_CFG_SOCKET_RECEIVE_BUFFER_BYTES
              value: "102400"
            - name: KAFKA_CFG_SOCKET_REQUEST_MAX_BYTES
              value: "104857600"
            - name: KAFKA_CFG_SOCKET_SEND_BUFFER_BYTES
              value: "102400"
            - name: KAFKA_CFG_ZOOKEEPER_CONNECTION_TIMEOUT_MS
              value: "6000"
            - name: KAFKA_CFG_AUTHORIZER_CLASS_NAME
              value: ""
            - name: KAFKA_CFG_ALLOW_EVERYONE_IF_NO_ACL_FOUND
              value: "true"
            - name: KAFKA_CFG_SUPER_USERS
              value: "User:admin"
          ports:
            - name: kafka-client
              containerPort: 9092
            - name: kafka-internal
              containerPort: 9093
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: kafka-client
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: kafka-client
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /bitnami/kafka
            - name: logs
              mountPath: /opt/bitnami/kafka/logs
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
      volumes:
        - name: scripts
          configMap:
            name: kafka-ibb-scripts
            defaultMode: 0755
        - name: logs
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "25Gi"
---
# Source: inquirybb/charts/redis/templates/master/application.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-ibb-master
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.8.3
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.9"
    app.kubernetes.io/component: master
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: redis
      app.kubernetes.io/instance: async-questionset-publish
      app.kubernetes.io/component: master
  serviceName: redis-ibb-headless
  updateStrategy:
    type: RollingUpdate
  minReadySeconds: 0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: redis
        helm.sh/chart: redis-17.8.3
        app.kubernetes.io/instance: async-questionset-publish
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: "7.0.9"
        app.kubernetes.io/component: master
      annotations:
        checksum/configmap: 31f2f5ab0d5d1dd24fb2c27f4c1b8dc3c2c2a3b08c897e69d5db585bb19cb3e3
        checksum/health: 72bdbd7ff3796e736e2d1564a0493b0485d706e306f7c5f0f09db71f567d9e92
        checksum/scripts: 563f59fdf32242e96fd1e7a67b18603debeb751642de3f44f14fcfb39ddac969
        checksum/secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
    spec:
      
      securityContext:
        fsGroup: 1001
      serviceAccountName: redis-ibb
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: async-questionset-publish
                    app.kubernetes.io/name: redis
                    app.kubernetes.io/component: master
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:7.0.9-debian-11-r1
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-master.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: master
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            # One second longer than command timeout should prevent generation of zombie processes.
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: redis-tmp-conf
              mountPath: /opt/bitnami/redis/etc/
            - name: tmp
              mountPath: /tmp
      volumes:
        - name: start-scripts
          configMap:
            name: redis-ibb-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: redis-ibb-health
            defaultMode: 0755
        - name: config
          configMap:
            name: redis-ibb-configuration
        - name: redis-tmp-conf
          emptyDir: {}
        - name: tmp
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: redis-data
        labels:
          app.kubernetes.io/name: redis
          app.kubernetes.io/instance: async-questionset-publish
          app.kubernetes.io/component: master
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "25Gi"
---
# Source: inquirybb/charts/redis/templates/replicas/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-ibb-replicas
  namespace: "default"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.8.3
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.9"
    app.kubernetes.io/component: replica
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  replicas: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: redis
      app.kubernetes.io/instance: async-questionset-publish
      app.kubernetes.io/component: replica
  serviceName: redis-ibb-headless
  updateStrategy:
    type: RollingUpdate
  minReadySeconds: 0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: redis
        helm.sh/chart: redis-17.8.3
        app.kubernetes.io/instance: async-questionset-publish
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: "7.0.9"
        app.kubernetes.io/component: replica
      annotations:
        checksum/configmap: 31f2f5ab0d5d1dd24fb2c27f4c1b8dc3c2c2a3b08c897e69d5db585bb19cb3e3
        checksum/health: 72bdbd7ff3796e736e2d1564a0493b0485d706e306f7c5f0f09db71f567d9e92
        checksum/scripts: 563f59fdf32242e96fd1e7a67b18603debeb751642de3f44f14fcfb39ddac969
        checksum/secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
    spec:
      
      securityContext:
        fsGroup: 1001
      serviceAccountName: redis-ibb
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: async-questionset-publish
                    app.kubernetes.io/name: redis
                    app.kubernetes.io/component: replica
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:7.0.9-debian-11-r1
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-replica.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: replica
            - name: REDIS_MASTER_HOST
              value: redis-ibb-master-0.redis-ibb-headless.default.svc.cluster.local
            - name: REDIS_MASTER_PORT_NUMBER
              value: "6379"
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          startupProbe:
            failureThreshold: 22
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: redis
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local_and_master.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local_and_master.sh 1
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: redis-tmp-conf
              mountPath: /opt/bitnami/redis/etc
      volumes:
        - name: start-scripts
          configMap:
            name: redis-ibb-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: redis-ibb-health
            defaultMode: 0755
        - name: config
          configMap:
            name: redis-ibb-configuration
        - name: redis-tmp-conf
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: redis-data
        labels:
          app.kubernetes.io/name: redis
          app.kubernetes.io/instance: async-questionset-publish
          app.kubernetes.io/component: replica
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: inquirybb/templates/provision/cassandra.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: inquirybb-cassandra-migration-job
  labels:
    app: inquirybb-cassandra-migration
    scope: provisioning
  annotations:
    helm.sh/hook-weight: "-4"
spec:
  template:
    metadata:
      labels:
        app: inquirybb-cassandra-migration
    spec:
      restartPolicy: Never
      volumes:
      - name: shared-volume
        emptyDir: {}
      initContainers:
      - name: wait-for-cassandra
        image: alpine/git
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c']
        args:
          - |
            timeout 120 sh -c 'until nc -z cassandra 9042; do echo waiting for cassandra; sleep 10; done'
            set -e
            cd /opt/shared-volume
            export inquiry_automation_version=release-7.0.0

            # Download the migration files
            git clone --filter=blob:none --no-checkout https://github.com/Sunbird-inQuiry/inquiry-api-service --branch=$inquiry_automation_version --depth 1
            cd inquiry-api-service
            git sparse-checkout init --cone
            git sparse-checkout set scripts/cassandra-db
            git checkout
            chmod -R 777 /opt/shared-volume
        volumeMounts:
          - name: shared-volume
            mountPath: /opt/shared-volume
      containers:
      - name: migration
        image: bitnami/cassandra:3.11.13-debian-11-r3
        imagePullPolicy: IfNotPresent
        command: ['bash', '-c']
        args:
        - |
          cd /opt/shared-volume/inquiry-api-service/scripts/cassandra-db
          export ENV=
          
          sed -i "s/{{ env }}/$ENV/g" generalized-cassandra.cql
          echo cqlsh cassandra -f "generalized-cassandra.cql"
          cqlsh cassandra -f "generalized-cassandra.cql"
        volumeMounts:
          - name: shared-volume
            mountPath: /opt/shared-volume
  backoffLimit: 0
---
# Source: inquirybb/templates/provision/kafka.yaml
kind: Job
apiVersion: batch/v1
metadata:
  name: async-questionset-publish-inquirybb-provisioning
  namespace: "default"
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: inquirybb
    app.kubernetes.io/version: 1.16.0
    helm.sh/chart: inquirybb-0.1.0
    app.kubernetes.io/component: kafka-provisioning
    scope: provisioning
  annotations:
    helm.sh/hook-weight: "-4"
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/component: kafka-provisioning
    spec:
      restartPolicy: OnFailure
      terminationGracePeriodSeconds: 0
      containers:
        - name: kafka-provisioning
          image: "bitnami/kafka:3.3.1-debian-11-r25"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
          args:
            - -ec
            - |
              echo "Configuring environment"
              . /opt/bitnami/scripts/libkafka.sh
              export CLIENT_CONF="/tmp/client.properties"
              touch $CLIENT_CONF
              kafka_common_conf_set "$CLIENT_CONF" security.protocol "PLAINTEXT"
              KAFKA_SERVICE="kafka:9092"
              kafka_provisioning_commands=(
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server "$KAFKA_SERVICE" \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config retention.ms=172800000 \
                    --command-config ${CLIENT_CONF} \
                    --topic .object.import.request"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server "$KAFKA_SERVICE" \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config retention.ms=172800000 \
                    --command-config ${CLIENT_CONF} \
                    --topic .assessment.publish.request"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server "$KAFKA_SERVICE" \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config retention.ms=172800000 \
                    --command-config ${CLIENT_CONF} \
                    --topic .delete.user"
              )

              echo "Starting provisioning"
              for ((index=0; index < ${#kafka_provisioning_commands[@]}; index+=1))
              do
                for j in $(seq ${index} $((${index}+1-1)))
                do
                    ${kafka_provisioning_commands[j]} & # Async command
                done
                wait  # Wait the end of the jobs
              done

              echo "Provisioning succeeded"
---
# Source: inquirybb/templates/provision/neo4j.yaml
kind: Job
apiVersion: batch/v1
metadata:
  name: async-questionset-publish-inquirybb-neo4j-provisioning
  namespace: "default"
  labels:
    app.kubernetes.io/instance: async-questionset-publish
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: inquirybb
    app.kubernetes.io/version: 1.16.0
    helm.sh/chart: inquirybb-0.1.0
    app.kubernetes.io/component: neo4j-provisioning
    scope: provisioning
  annotations:
    helm.sh/hook-weight: "-4"
spec:
  template:
    metadata:
      labels: 
        app.kubernetes.io/component: neo4j-provisioning
    spec:
      restartPolicy: OnFailure
      terminationGracePeriodSeconds: 0
      initContainers:
        - name: wait-for-available-neo4j
          image: busybox
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
          args:
            - -ec
            - |
              PORT=7687
              HOST=neo4j
              TIMEOUT=120
              START_TIME=$(date +%s)

              while ! nc -z $HOST $PORT; do
                CURRENT_TIME=$(date +%s)
                ELAPSED_TIME=$(( CURRENT_TIME - START_TIME ))

                if [ $ELAPSED_TIME -ge $TIMEOUT ]; then
                  echo "Timeout reached. Port $PORT is not available."
                  exit 1
                fi

                sleep 1
              done

              echo "Port Neo4j is now available."
      containers:
        - name: neo4j-provisioning
          image: "sunbirded.azurecr.io/neo4j:3.3.0"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
          args:
            - -ec
            - |
              /var/lib/neo4j/bin/cypher-shell -a bolt://neo4j:7687 <<EOF
              CREATE CONSTRAINT ON (domain:domain) ASSERT domain.IL_UNIQUE_ID IS UNIQUE;
              CREATE INDEX ON :domain(IL_FUNC_OBJECT_TYPE);
              CREATE INDEX ON :domain(IL_SYS_NODE_TYPE);
              EOF
---
# Source: inquirybb/templates/provision/job-cleaner.yaml
# This chart is required because helm does not support deleting jobs if not having hooks.
# We can add pre-install, hook as some services may require schemas in db, and it'll cause catch22.
apiVersion: v1
kind: ServiceAccount
metadata:
  name: inquirybb-job-deleter
  annotations:
    helm.sh/hook: "pre-install, pre-upgrade"
    helm.sh/hook-weight: "-5"
---
# Source: inquirybb/templates/provision/job-cleaner.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: inquirybb-job-deleter-role
  annotations:
    helm.sh/hook: "pre-install,pre-upgrade"
    helm.sh/hook-weight: "-5"
rules:
- apiGroups: ["batch"]
  resources: ["jobs"]
  verbs: ["delete", "get", "list"]
---
# Source: inquirybb/templates/provision/job-cleaner.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: inquirybb-job-deleter-rolebinding
  annotations:
    helm.sh/hook: "pre-install,pre-upgrade"
    helm.sh/hook-weight: "-5"
subjects:
- kind: ServiceAccount
  name: inquirybb-job-deleter
roleRef:
  kind: Role
  name: inquirybb-job-deleter-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: inquirybb/templates/provision/job-cleaner.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: inquirybb-job-deleter
  annotations:
    helm.sh/hook: "pre-install,pre-upgrade"
    helm.sh/hook-weight: "-3"
spec:
  template:
    spec:
      serviceAccountName: inquirybb-job-deleter
      containers:
      - name: kubectl-container
        image: bitnami/kubectl
        imagePullPolicy: IfNotPresent
        command: ["bash", "-c"]
        args:
          - |
            chart_name="inquirybb"
            for job in $(kubectl get jobs -l scope=provisioning -o name | grep $chart_name); do
              if ! kubectl delete $job; then
                echo "Couldn't delete job $job"
                continue
              fi
              echo "deleted $job"
            done
      restartPolicy: Never
