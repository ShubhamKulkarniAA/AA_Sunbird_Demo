---
# Source: knowledgebb/charts/cassandra/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cassandra-kbb
  namespace: "sunbird"
  labels:
    app.kubernetes.io/name: cassandra
    helm.sh/chart: cassandra-10.1.0
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "4.1.0"
  annotations:
    helm.sh/hook-weight: "-5"
automountServiceAccountToken: true
---
# Source: knowledgebb/charts/content/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: content
---
# Source: knowledgebb/charts/dial/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: dial
---
# Source: knowledgebb/charts/flink/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: neo4j-kbb-sa
  annotations:
---
# Source: knowledgebb/charts/kafka/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kafka-kbb
  namespace: "sunbird"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-20.0.2
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.3.1"
    app.kubernetes.io/component: kafka
  annotations:
    helm.sh/hook-weight: "-5"
automountServiceAccountToken: true
---
# Source: knowledgebb/charts/neo4j/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: neo4j-kbb
---
# Source: knowledgebb/charts/redis/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  name: redis-kbb
  namespace: "sunbird"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.8.3
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.9"
  annotations:
    helm.sh/hook-weight: "-5"
---
# Source: knowledgebb/charts/search/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: search
---
# Source: knowledgebb/charts/taxonomy/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: taxonomy
---
# Source: knowledgebb/charts/cassandra/templates/cassandra-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: cassandra-kbb
  namespace: "sunbird"
  labels:
    app.kubernetes.io/name: cassandra
    helm.sh/chart: cassandra-10.1.0
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "4.1.0"
  annotations:
    helm.sh/hook-weight: "-5"
type: Opaque
data:
  cassandra-password: "aUdrWW1oUHhJUQ=="
---
# Source: knowledgebb/charts/cassandra/templates/metrics-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cassandra-kbb-metrics-conf
  namespace: "sunbird"
  labels:
    app.kubernetes.io/name: cassandra
    helm.sh/chart: cassandra-10.1.0
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "4.1.0"
    app.kubernetes.io/part-of: cassandra
    app.kubernetes.io/component: cassandra-exporter
  annotations:
    helm.sh/hook-weight: "-5"
data:
  config.yml: |-
    host: localhost:7199
    ssl: False
    user:
    password:
    listenPort: 8080
    blacklist:
      # To profile the duration of jmx call you can start the program with the following options
      # > java -Dorg.slf4j.simpleLogger.defaultLogLevel=trace -jar cassandra_exporter.jar config.yml --oneshot
      #
      # To get intuition of what is done by cassandra when something is called you can look in cassandra
      # https://github.com/apache/cassandra/tree/trunk/src/java/org/apache/cassandra/metrics
      # Please avoid to scrape frequently those calls that are iterating over all sstables
    
      # Unaccessible metrics (not enough privilege)
      - java:lang:memorypool:.*usagethreshold.*
    
      # Leaf attributes not interesting for us but that are presents in many path
      - .*:999thpercentile
      - .*:95thpercentile
      - .*:fifteenminuterate
      - .*:fiveminuterate
      - .*:durationunit
      - .*:rateunit
      - .*:stddev
      - .*:meanrate
      - .*:mean
      - .*:min
    
      # Path present in many metrics but uninterresting
      - .*:viewlockacquiretime:.*
      - .*:viewreadtime:.*
      - .*:cas[a-z]+latency:.*
      - .*:colupdatetimedeltahistogram:.*
    
      # Mostly for RPC, do not scrap them
      - org:apache:cassandra:db:.*
    
      # columnfamily is an alias for Table metrics
      # https://github.com/apache/cassandra/blob/8b3a60b9a7dbefeecc06bace617279612ec7092d/src/java/org/apache/cassandra/metrics/TableMetrics.java#L162
      - org:apache:cassandra:metrics:columnfamily:.*
    
      # Should we export metrics for system keyspaces/tables ?
      - org:apache:cassandra:metrics:[^:]+:system[^:]*:.*
    
      # Don't scrap us
      - com:criteo:nosql:cassandra:exporter:.*
    
    maxScrapFrequencyInSec:
      50:
        - .*
    
      # Refresh those metrics only every hour as it is costly for cassandra to retrieve them
      3600:
        - .*:snapshotssize:.*
        - .*:estimated.*
        - .*:totaldiskspaceused:.*
---
# Source: knowledgebb/charts/content/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: content
  labels:

    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: content
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: content-0.1.0
  annotations:

    reloader.stakater.com/auto: "true"
data: # Take only root level files (configs/*) for configmaps # Skip env.yaml as configmap, as it's env file
    
  application.conf: |-
      # This is the main configuration file for the application.
      # https://www.playframework.com/documentation/latest/ConfigFile
      # ~~~~~
      # Play uses HOCON as its configuration file format. HOCON has a number
      # of advantages over other config formats, but there are two things that
      # can be used when modifying settings.
      #
      # You can include other configuration files in this main application.conf file:
      #include "extra-config.conf"
      #
      # You can declare variables and substitute for them:
      #mykey = ${some.value}
      #
      # And if an environment variable exists when there is no other substitution, then
      # HOCON will fall back to substituting environment variable:
      #mykey = ${JAVA_HOME}
      
      ## Akka
      # https://www.playframework.com/documentation/latest/ScalaAkka#Configuration
      # https://www.playframework.com/documentation/latest/JavaAkka#Configuration
      # ~~~~~
      # Play uses Akka internally and exposes Akka Streams and actors in Websockets and
      # other streaming HTTP responses.
      akka {
          # "akka.log-config-on-start" is extraordinarly useful because it log the complete
          # configuration at INFO level, including defaults and overrides, so it s worth
          # putting at the very top.
          #
          # Put the following in your conf/logback.xml file:
          #
          # <logger name="akka.actor" level="INFO" />
          #
          # And then uncomment this line to debug the configuration.
          #
          #log-config-on-start = true
          default-dispatcher {
              # This will be used if you have set "executor = "fork-join-executor""
              fork-join-executor {
                  # Min number of threads to cap factor-based parallelism number to
                  parallelism-min = 8
      
                  # The parallelism factor is used to determine thread pool size using the
                  # following formula: ceil(available processors * factor). Resulting size
                  # is then bounded by the parallelism-min and parallelism-max values.
                  parallelism-factor = 32.0
      
                  # Max number of threads to cap factor-based parallelism number to
                  parallelism-max = 64
      
                  # Setting to "FIFO" to use queue like peeking mode which "poll" or "LIFO" to use stack
                  # like peeking mode which "pop".
                  task-peeking-mode = "FIFO"
              }
          }
          actors-dispatcher {
              type = "Dispatcher"
              executor = "fork-join-executor"
              fork-join-executor {
                  parallelism-min = 8
                  parallelism-factor = 32.0
                  parallelism-max = 64
              }
              # Throughput for default Dispatcher, set to 1 for as fair as possible
              throughput = 1
          }
          actor {
              deployment {
              /contentActor
              {
                  router = smallest-mailbox-pool
                  nr-of-instances = 10
                  dispatcher = actors-dispatcher
              }
              /channelActor
              {
                  router = smallest-mailbox-pool
                  nr-of-instances = 10
                  dispatcher = actors-dispatcher
              }
              /collectionActor
              {
                  router = smallest-mailbox-pool
                  nr-of-instances = 10
                  dispatcher = actors-dispatcher
              }
              /healthActor
              {
                  router = smallest-mailbox-pool
                  nr-of-instances = 5
                  dispatcher = actors-dispatcher
              }
              /licenseActor
              {
                  router = smallest-mailbox-pool
                  nr-of-instances = 2
                  dispatcher = actors-dispatcher
              }
              }
          }
      }
      
      ## Secret key
      # http://www.playframework.com/documentation/latest/ApplicationSecret
      # ~~~~~
      # The secret key is used to sign Play's session cookie.
      # This must be changed for production, but we don't recommend you change it in this file.
      play.http.secret.key= "secretKey"
      
      ## Modules
      # https://www.playframework.com/documentation/latest/Modules
      # ~~~~~
      # Control which modules are loaded when Play starts. Note that modules are
      # the replacement for "GlobalSettings", which are deprecated in 2.5.x.
      # Please see https://www.playframework.com/documentation/latest/GlobalSettings
      # for more information.
      #
      # You can also extend Play functionality by using one of the publically available
      # Play modules: https://playframework.com/documentation/latest/ModuleDirectory
      play.modules {
          # By default, Play will load any class called Module that is defined
          # in the root package (the "app" directory), or you can define them
          # explicitly below.
          # If there are any built-in modules that you want to enable, you can list them here.
          enabled += modules.ContentModule
      
          # If there are any built-in modules that you want to disable, you can list them here.
          #disabled += ""
      }
      
      ## IDE
      # https://www.playframework.com/documentation/latest/IDE
      # ~~~~~
      # Depending on your IDE, you can add a hyperlink for errors that will jump you
      # directly to the code location in the IDE in dev mode. The following line makes
      # use of the IntelliJ IDEA REST interface:
      #play.editor="http://localhost:63342/api/file/?file=%s&line=%s"
      
      ## Internationalisation
      # https://www.playframework.com/documentation/latest/JavaI18N
      # https://www.playframework.com/documentation/latest/ScalaI18N
      # ~~~~~
      # Play comes with its own i18n settings, which allow the user's preferred language
      # to map through to internal messages, or allow the language to be stored in a cookie.
      play.i18n {
          # The application languages
          langs = [ "en" ]
      
          # Whether the language cookie should be secure or not
          #langCookieSecure = true
      
          # Whether the HTTP only attribute of the cookie should be set to true
          #langCookieHttpOnly = true
      }
      
      ## Play HTTP settings
      # ~~~~~
      play.http {
          ## Router
          # https://www.playframework.com/documentation/latest/JavaRouting
          # https://www.playframework.com/documentation/latest/ScalaRouting
          # ~~~~~
          # Define the Router object to use for this application.
          # This router will be looked up first when the application is starting up,
          # so make sure this is the entry point.
          # Furthermore, it's assumed your route file is named properly.
          # So for an application router like `my.application.Router`,
          # you may need to define a router file `conf/my.application.routes`.
          # Default to Routes in the root package (aka "apps" folder) (and conf/routes)
          #router = my.application.Router
      
          ## Action Creator
          # https://www.playframework.com/documentation/latest/JavaActionCreator
          # ~~~~~
          #actionCreator = null
      
          ## ErrorHandler
          # https://www.playframework.com/documentation/latest/JavaRouting
          # https://www.playframework.com/documentation/latest/ScalaRouting
          # ~~~~~
          # If null, will attempt to load a class called ErrorHandler in the root package,
          #errorHandler = null
      
          ## Session & Flash
          # https://www.playframework.com/documentation/latest/JavaSessionFlash
          # https://www.playframework.com/documentation/latest/ScalaSessionFlash
          # ~~~~~
          session {
              # Sets the cookie to be sent only over HTTPS.
              #secure = true
      
              # Sets the cookie to be accessed only by the server.
              #httpOnly = true
      
              # Sets the max-age field of the cookie to 5 minutes.
              # NOTE: this only sets when the browser will discard the cookie. Play will consider any
              # cookie value with a valid signature to be a valid session forever. To implement a server side session timeout,
              # you need to put a timestamp in the session and check it at regular intervals to possibly expire it.
              #maxAge = 300
      
              # Sets the domain on the session cookie.
              #domain = "example.com"
          }
      
          flash {
              # Sets the cookie to be sent only over HTTPS.
              #secure = true
      
              # If you run Play on Linux, you can use Netty's native socket transport
              # for higher performance with less garbage.
              transport = "jdk"
      
              # Sets the cookie to be accessed only by the server.
              #httpOnly = true
      
          }
      }
      
      play.server.http.idleTimeout = 60s
      play.http.parser.maxDiskBuffer = 100MB
      parsers.anyContent.maxLength = 100MB
      
      ## Netty Provider
      # https://www.playframework.com/documentation/latest/SettingsNetty
      # ~~~~~
      play.server.netty {
          # Whether the Netty wire should be logged
          log.wire = true
      
          # If you run Play on Linux, you can use Netty's native socket transport
          # for higher performance with less garbage.
          transport = "native"
      }
      
      ## WS (HTTP Client)
      # https://www.playframework.com/documentation/latest/ScalaWS#Configuring-WS
      # ~~~~~
      # The HTTP client primarily used for REST APIs. The default client can be
      # configured directly, but you can also create different client instances
      # with customized settings. You must enable this by adding to build.sbt:
      #
      # libraryDependencies += ws // or javaWs if using java
      #
      play.ws {
          # Sets HTTP requests not to follow 302 requests
          #followRedirects = false
      
          # Sets the maximum number of open HTTP connections for the client.
          #ahc.maxConnectionsTotal = 50
      
          ## WS SSL
          # https://www.playframework.com/documentation/latest/WsSSL
          # ~~~~~
          ssl {
              # Configuring HTTPS with Play WS does not require programming. You can
              # set up both trustManager and keyManager for mutual authentication, and
              # turn on JSSE debugging in development with a reload.
              #debug.handshake = true
              #trustManager = {
              # stores = [
              # { type = "JKS", path = "exampletrust.jks" }
              # ]
              #}
          }
      }
      
      ## Cache
      # https://www.playframework.com/documentation/latest/JavaCache
      # https://www.playframework.com/documentation/latest/ScalaCache
      # ~~~~~
      # Play comes with an integrated cache API that can reduce the operational
      # overhead of repeated requests. You must enable this by adding to build.sbt:
      #
      # libraryDependencies += cache
      #
      play.cache {
      # If you want to bind several caches, you can bind the individually
      #bindCaches = ["db-cache", "user-cache", "session-cache"]
      }
      
      ## Filter Configuration
      # https://www.playframework.com/documentation/latest/Filters
      # ~~~~~
      # There are a number of built-in filters that can be enabled and configured
      # to give Play greater security.
      #
      play.filters {
      
          # Enabled filters are run automatically against Play.
          # CSRFFilter, AllowedHostFilters, and SecurityHeadersFilters are enabled by default.
          enabled = [filters.AccessLogFilter]
      
          # Disabled filters remove elements from the enabled list.
          # disabled += filters.CSRFFilter
      
      
          ## CORS filter configuration
          # https://www.playframework.com/documentation/latest/CorsFilter
          # ~~~~~
          # CORS is a protocol that allows web applications to make requests from the browser
          # across different domains.
          # NOTE: You MUST apply the CORS configuration before the CSRF filter, as CSRF has
          # dependencies on CORS settings.
          cors {
          # Filter paths by a whitelist of path prefixes
          #pathPrefixes = ["/some/path", ...]
      
          # The allowed origins. If null, all origins are allowed.
          #allowedOrigins = ["http://www.example.com"]
      
          # The allowed HTTP methods. If null, all methods are allowed
          #allowedHttpMethods = ["GET", "POST"]
          }
      
          ## Security headers filter configuration
          # https://www.playframework.com/documentation/latest/SecurityHeaders
          # ~~~~~
          # Defines security headers that prevent XSS attacks.
          # If enabled, then all options are set to the below configuration by default:
          headers {
          # The X-Frame-Options header. If null, the header is not set.
          #frameOptions = "DENY"
      
          # The X-XSS-Protection header. If null, the header is not set.
          #xssProtection = "1; mode=block"
      
          # The X-Content-Type-Options header. If null, the header is not set.
          #contentTypeOptions = "nosniff"
      
          # The X-Permitted-Cross-Domain-Policies header. If null, the header is not set.
          #permittedCrossDomainPolicies = "master-only"
      
          # The Content-Security-Policy header. If null, the header is not set.
          #contentSecurityPolicy = "default-src 'self'"
          }
      
          ## Allowed hosts filter configuration
          # https://www.playframework.com/documentation/latest/AllowedHostsFilter
          # ~~~~~
          # Play provides a filter that lets you configure which hosts can access your application.
          # This is useful to prevent cache poisoning attacks.
          hosts {
          # Allow requests to example.com, its subdomains, and localhost:9000.
          #allowed = [".example.com", "localhost:9000"]
          }
      }
      
      play.http.parser.maxMemoryBuffer = 50MB
      akka.http.parsing.max-content-length = 50MB
       
      schema.base_path="https:////schemas/local" 
      
      
      # Cassandra Configuration
      cassandra {
          lp {
              connection: "cassandra:9042"
          }
          lpa {
              connection: "cassandra:9042"
          }
      }
      
      # Consistency Level for Multi Node Cassandra cluster
      #cassandra.lp.consistency.level=local_one
      
      collection {
          keyspace: "_hierarchy_store"
          cache.enable: true
          image.migration.enabled: true
      }
      
      content {
          keyspace: "_content_store"
          cache {
              ttl: 86400
              enable: true
          }
          hierarchy {
              removed_props_for_leafNodes: ["collections", "children", "usedByContent", "item_sets", "methods", "libraries", "editorState"]
          }
          # Added for supporting backward compatibility - remove in release-2.7.0
          tagging {
              backward_enable: true
              property: "subject,medium"
          }
          h5p {
              library {
              path: "https://{{ .Values.global.object_storage_endpoint }}/{{ .Values.global.public_container_name }}installation/h5p-standalone-1.3.4.zip"
          }
          }
          copy {
              invalid_statusList: ["Flagged","FlaggedDraft","FraggedReview","Retired", "Processing"]
              origin_data: ["name", "author", "license", "organisation"]
              props_to_remove: ["downloadUrl", "artifactUrl", "variants", "createdOn", "collections", "children", "lastUpdatedOn", "SYS_INTERNAL_LAST_UPDATED_ON", "versionKey", "s3Key", "status", "pkgVersion", "toc_url", "mimeTypesCount", "contentTypesCount", "leafNodesCount", "childNodes", "prevState", "lastPublishedOn", "flagReasons", "compatibilityLevel", "size", "publishChecklist", "publishComment", "LastPublishedBy", "rejectReasons", "rejectComment", "gradeLevel", "subject", "medium", "board", "topic", "purpose", "subtopic", "contentCredits", "owner", "collaborators", "creators", "contributors", "badgeAssertions", "dialcodes", "concepts", "keywords", "reservedDialcodes", "dialcodeRequired", "leafNodes", "sYS_INTERNAL_LAST_UPDATED_ON", "prevStatus", "lastPublishedBy", "streamingUrl", "boardIds", "gradeLevelIds", "subjectIds", "mediumIds", "topicsIds", "targetFWIds", "targetBoardIds", "targetGradeLevelIds", "targetSubjectIds", "targetMediumIds", "targetTopicIds", "se_boards", "se_subjects", "se_mediums", "se_gradeLevels", "se_topics", "se_FWIds", "se_boardIds", "se_subjectIds", "se_mediumIds", "se_gradeLevelIds", "se_topicIds"]
          }
          # Need to depreacte
          media {
              base.url: "https://"
          }
      }
      
      # Redis Configuration
      redis {
          host:"redis-master"
          port:"6379"
          maxConnections:"128"
      }
      
      
      #--Maximum Content Package File Size Limit in Bytes (50 MB)
      MAX_CONTENT_PACKAGE_FILE_SIZE_LIMIT=157286400
      
      #--Maximum Asset File Size Limit in Bytes (50 MB)
      MAX_ASSET_FILE_SIZE_LIMIT=157286400
      
      #--No of Retry While File Download Fails
      RETRY_ASSET_DOWNLOAD_COUNT=1
      
      #Current environment - need this property post cloud-store implementation
      cloud_storage {
          env: 
          content.folder: "content"
          asset.folder: "assets"
          artefact.folder: "artifact"
          bundle.folder: "bundle"
          media.folder: "media"
          ecar.folder: "ecar_files"
          upload.url.ttl: "54000"
      }
      
      # Configuration
      akka.request_timeout: 30
      environment.id: 20000000
      graph.passport.key.base="secretKey"
      route.domain="bolt://neo4j:7687"
      route.bolt.write.domain="bolt://neo4j:7687"
      route.bolt.read.domain="bolt://neo4j:7687"
      route.all="bolt://neo4j:8687"
      route.bolt.write.all="bolt://neo4j:8687"
      route.bolt.read.all="bolt://neo4j:8687"
      
      shard.id: 1
      
      platform {
          auth.check.enabled: false
          cache.ttl: 3600000
          language.codes: ["as","bn","en","gu","hi","hoc","jun","ka","mai","mr","unx","or","san","sat","ta","te","urd", "pj"]
      }
      
      #Top N Config for Search Telemetry
      telemetry_env: staging
      
      installation.id: ekstep
      
      channel {
          default: "in.ekstep"
          fetch.suggested_frameworks: true
      }
      
      languageCode {
          assamese : "as"
          bengali : "bn"
          english : "en"
          gujarati : "gu"
          hindi : "hi"
          kannada : "ka"
          marathi : "mr"
          odia : "or"
          tamil : "ta"
          telugu : "te"
      }
      # Need to depreacte
      composite {
          search {
              url : "http://search-service:9000/v3/search"
          }
      }
      
      cloud_storage_type=""
      cloud_storage_key=""
      cloud_storage_secret=""
      cloud_storage_container=""
      cloud_storage_endpoint=""
      
      # Google Drive APIKEY
      learning_content_drive_apiKey = ""
      
      #DIAL Code Reserve configuration
      kafka.dial.request.topic=".knowlg.qrimage.request"
      dialcode.keyspace= dialcodes                                                                                                                                                                      
      dialcode_image.keyspace=dialcodes                                                                                                                                                               
      cloud_storage_dial_container=
      #Youtube Standard Licence Validation
      learning.content.youtube.application.name="fetch-youtube-license"
      learning_content_youtube_apikey=""
      youtube.license.regex.pattern=["\\?vi?=([^&]*)", "watch\\?.*v=([^&]*)", "(?:embed|vi?)/([^/?]*)","^([A-Za-z0-9\\-\\_]*)"]
      learning.valid_license=["creativeCommon"]
      
      kafka {
          urls : "kafka:9092"
          topic.send.enable : true
          topics.instruction : ".knowlg.learning.job.request"
          publish.request.topic : ".knowlg.publish.job.request"
      }
      
      # Need to depreacte
      # DIAL Link Config
      dial_service {
          api {
              base_url : "http://dial-service:9000"
              auth_key : ""
              search : "/dialcode/v3/search"
              generate : "/dialcode/v3/generate"
          }
      }
      # Need to depreacte
      reserve_dialcode {
          mimeType : ["application/vnd.ekstep.content-collection"]
          max_count : 250
      }
      
      content.link_dialcode.validation=true
      content.link_dialcode.max_limit=10
      
      # Content Import API Config
      import {
          request_size_limit : 1000
          output_topic_name : ".knowlg.auto.creation.job.request"
          required_props : ["name","code","mimeType","primaryCategory","artifactUrl","framework"]
          remove_props : ["downloadUrl","variants","previewUrl","streamingUrl","itemSets","level1Name","level1Concept","level2Name","level2Concept","level3Name","level3Concept","me_totalPlaySessionCount","me_totalTimeSpentInSec","me_totalSessionsCount","me_totalTimespent","me_totalInteractions","me_creationSessions","me_creationTimespent","me_averageInteractionsPerMin","me_averageSessionsPerDevice","me_totalDevices","me_averageTimespentPerSession","me_averageRating","me_totalDownloads","me_totalSideloads","me_totalRatings","me_totalComments","me_totalDialcode","me_totalDialcodeLinkedToContent","me_totalDialcodeAttached","me_hierarchyLevel","origin","originData","contentPolicyCheck","questions"]
      }
      
      # Need to depreacte
      contentTypeToPrimaryCategory {
          ClassroomTeachingVideo: "Explanation Content"
          ConceptMap: "Learning Resource"
          Course: "Course"
          CuriosityQuestionSet: "Practice Question Set"
          eTextBook: "eTextbook"
          ExperientialResource: "Learning Resource"
          ExplanationResource: "Explanation Content"
          ExplanationVideo: "Explanation Content"
          FocusSpot: "Teacher Resource"
          LearningOutcomeDefinition: "Teacher Resource"
          MarkingSchemeRubric: "Teacher Resource"
          PedagogyFlow: "Teacher Resource"
          PracticeQuestionSet: "Practice Question Set"
          PracticeResource: "Practice Question Set"
          SelfAssess: "Course Assessment"
          TeachingMethod: "Teacher Resource"
          TextBook: "Digital Textbook"
          Collection: "Content Playlist"
          ExplanationReadingMaterial: "Learning Resource"
          LearningActivity: "Learning Resource"
          LessonPlan: "Content Playlist"
          LessonPlanResource: "Teacher Resource"
          PreviousBoardExamPapers: "Learning Resource"
          TVLesson: "Explanation Content"
          OnboardingResource: "Learning Resource"
          ReadingMaterial: "Learning Resource"
          Template: "Template"
          Asset: "Asset"
          Plugin: "Plugin"
          LessonPlanUnit: "Lesson Plan Unit"
          CourseUnit: "Course Unit"
          TextBookUnit: "Textbook Unit"
          Asset: "Certificate Template"
      }
      # Need to depreacte
      resourceTypeToPrimaryCategory {
          Learn: "Learning Resource"
          Read: "Learning Resource"
          Practice: "Learning Resource"
          Teach: "Teacher Resource"
          Test: "Learning Resource"
          Experiment: "Learning Resource"
          LessonPlan: "Teacher Resource"
      }
      # Need to depreacte
      mimeTypeToPrimaryCategory {
          "application/vnd.ekstep.h5p-archive": ["Learning Resource"]
          "application/vnd.ekstep.html-archive": ["Learning Resource"]
          "application/vnd.android.package-archive": ["Learning Resource"]
          "video/webm": ["Explanation Content"]
          "video/x-youtube": ["Explanation Content"]
          "video/mp4": ["Explanation Content"]
          "application/pdf": ["Learning Resource", "Teacher Resource"]
          "application/epub": ["Learning Resource", "Teacher Resource"]
          "application/vnd.ekstep.ecml-archive": ["Learning Resource", "Teacher Resource"]
          "text/x-url": ["Learnin Resource", "Teacher Resource"]
      }
      
      objectcategorydefinition.keyspace="_category_store"
      #Default objectCategory mapping for channel
      channel {
          content{
              primarycategories=["Course Assessment", "eTextbook", "Explanation Content", "Learning Resource", "Practice Question Set", "Teacher Resource", "Exam Question"]
              additionalcategories= ["Classroom Teaching Video", "Concept Map", "Curiosity Question Set", "Experiential Resource", "Explanation Video", "Focus Spot", "Learning Outcome Definition", "Lesson Plan", "Marking Scheme Rubric", "Pedagogy Flow", "Previous Board Exam Papers", "TV Lesson", "Textbook"]
          }
          collection {
              primarycategories=["Content Playlist", "Course", "Digital Textbook", "Question paper"]
              additionalcategories=["Textbook", "Lesson Plan"]
          }
          asset {
              primarycategories=["Asset", "CertAsset", "Certificate Template"]
              additionalcategories=[]
          }
      }
      master.category.validation.enabled="Yes"
      
      # Need to depreacte
      #Collection CSV
      sunbird_dialcode_search_api="http://dial-service:9000/dialcode/v3/list"
      framework_read_api_url="http://taxonomy-service:9000/framework/v3/read"
      sunbird_link_dial_code_api="http://content-service:9000/collection/v4/dialcode/link"
      
      # Need to depreacte
      collection {
          csv {
              maxRows = 6500
              allowedContentTypes = ["TextBook","Collection","Course"]
              maxFirstLevelUnits=30
              ttl = 86400
              maxUnitFieldLength=120
              maxDescFieldLength=1500
              contentTypeToUnitType = {"TextBook": "TextBookUnit", "Course": "CourseUnit", "Collection":"Collection"}
              headers {
                  folderIdentifier = ["Folder Identifier"]
                  hierarchy = ["Level 1 Folder","Level 2 Folder","Level 3 Folder","Level 4 Folder"]
                  QR = ["QR Code Required?","QR Code"]
                  topics = ["Mapped Topics"]
                  collectionName = ["Collection Name"]
                  linkedContents = ["Linked Content 1","Linked Content 2","Linked Content 3","Linked Content 4","Linked Content 5","Linked Content 6","Linked Content 7","Linked Content 8","Linked Content 9","Linked Content 10","Linked Content 11","Linked Content 12","Linked Content 13","Linked Content 14","Linked Content 15","Linked Content 16","Linked Content 17","Linked Content 18","Linked Content 19","Linked Content 20","Linked Content 21","Linked Content 22","Linked Content 23","Linked Content 24","Linked Content 25","Linked Content 26","Linked Content 27","Linked Content 28","Linked Content 29","Linked Content 30"]
                  output = ["Collection Name","Folder Identifier","Level 1 Folder","Level 2 Folder","Level 3 Folder","Level 4 Folder","Description","Mapped Topics","Keywords","QR Code Required?","QR Code","Linked Content 1","Linked Content 2","Linked Content 3","Linked Content 4","Linked Content 5","Linked Content 6","Linked Content 7","Linked Content 8","Linked Content 9","Linked Content 10","Linked Content 11","Linked Content 12","Linked Content 13","Linked Content 14","Linked Content 15","Linked Content 16","Linked Content 17","Linked Content 18","Linked Content 19","Linked Content 20","Linked Content 21","Linked Content 22","Linked Content 23","Linked Content 24","Linked Content 25","Linked Content 26","Linked Content 27","Linked Content 28","Linked Content 29","Linked Content 30"]
                  sequence {
                      create = {"Level 1 Folder":0,"Level 2 Folder":1,"Level 3 Folder":2,"Level 4 Folder":3,"Description":4}
                      update = {"Collection Name":0,"Folder Identifier":1,"Level 1 Folder":2,"Level 2 Folder":3,"Level 3 Folder":4,"Level 4 Folder":5,"Description":6,"Mapped Topics":7,"Keywords":8,"QR Code Required?":9,"QR Code":10,"Linked Content 1":11,"Linked Content 2":12,"Linked Content 3":13,"Linked Content 4":14,"Linked Content 5":15,"Linked Content 6":16,"Linked Content 7":17,"Linked Content 8":18,"Linked Content 9":19,"Linked Content 10":20,"Linked Content 11":21,"Linked Content 12":22,"Linked Content 13":23,"Linked Content 14":24,"Linked Content 15":25,"Linked Content 16":26,"Linked Content 17":27,"Linked Content 18":28,"Linked Content 19":29,"Linked Content 20":30,"Linked Content 21":31,"Linked Content 22":32,"Linked Content 23":33,"Linked Content 24":34,"Linked Content 25":35,"Linked Content 26":36,"Linked Content 27":37,"Linked Content 28":38,"Linked Content 29":39,"Linked Content 30":40}
                      linkedContents = {"Linked Content 1":0,"Linked Content 2":1,"Linked Content 3":2,"Linked Content 4":3,"Linked Content 5":4,"Linked Content 6":5,"Linked Content 7":6,"Linked Content 8":7,"Linked Content 9":8,"Linked Content 10":9,"Linked Content 11":10,"Linked Content 12":11,"Linked Content 13":12,"Linked Content 14":13,"Linked Content 15":14,"Linked Content 16":15,"Linked Content 17":16,"Linked Content 18":17,"Linked Content 19":18,"Linked Content 20":19,"Linked Content 21":20,"Linked Content 22":21,"Linked Content 23":22,"Linked Content 24":23,"Linked Content 25":24,"Linked Content 26":25,"Linked Content 27":26,"Linked Content 28":27,"Linked Content 29":28,"Linked Content 30":29}
                  }
              }
              mandatory {
                  create = ["Level 1 Folder"]
                  update = ["Collection Name","Folder Identifier"]
              }
          }
      }
      
      plugin.media.base.url="https://"
      
      cloudstorage {
        metadata.replace_absolute_path=true
        relative_path_prefix="CONTENT_STORAGE_BASE_PATH"
        metadata.list= ["appIcon", "artifactUrl", "posterImage", "previewUrl", "thumbnail", "assetsMap", "certTemplate", "itemSetPreviewUrl", "grayScaleAppIcon", "sourceURL", "variants", "downloadUrl", "streamingUrl", "toc_url", "data", "question", "solutions", "editorState", "media", "pdfUrl", "transcripts"]
        read_base_path="https://"
        write_base_path= ["https://"]
      } # Take only root level files (configs/*) for configmaps # Take only root level files (configs/*) for configmaps # Skip env.yaml as configmap, as it's env file
    
  logback.xml: |-
      <configuration>
      
          <conversionRule conversionWord="coloredLevel" converterClass="play.api.libs.logback.ColoredLevel" />
      
          <!-- transaction-event-trigger START -->
          <timestamp key="timestamp" datePattern="yyyy-MM-dd"/>
          <!-- common transactions logs -->
          <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
          <encoder>
              <pattern>%d %msg%n</pattern>
          </encoder>
          </appender>
      
          <appender name="ASYNCSTDOUT" class="ch.qos.logback.classic.AsyncAppender">
          <appender-ref ref="STDOUT" />
          </appender>
      
      
          <logger name="play" level="INFO" />
          <logger name="DefaultPlatformLogger" level="INFO" />
          <!-- Telemetry Loggers-->
      
          <root level="INFO">
          <appender-ref ref="ASYNCSTDOUT" />
          </root>
      
          <logger name="TelemetryEventLogger" level="INFO">
          <appender-ref ref="STDOUT" />
          </logger>
      
      </configuration>
---
# Source: knowledgebb/charts/content/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: content-env
  labels:

    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: content
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: content-0.1.0
  annotations:

    reloader.stakater.com/auto: "true"
data:
  JAVA_OPTIONS: -Xms500m -Xmx500m
  _JAVA_OPTIONS: -Dlog4j2.formatMsgNoLookups=true
---
# Source: knowledgebb/charts/content/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: content-opa
  labels:

    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: content
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: content-0.1.0
  annotations:

    reloader.stakater.com/auto: "true"
data:
  
  common.rego: |-
      package common
      
      import input.attributes.request.http as http_request
      import future.keywords.in
      
      ROLES := {
         "BOOK_REVIEWER": ["createLock", "publishContent", "listLock", "retireLock", "refreshLock", "rejectContent", "rejectContentV2"],
      
         "CONTENT_REVIEWER": ["createLock", "publishContent", "listLock", "retireLock", "refreshLock", "rejectContent", "rejectContentV2"],
      
         "FLAG_REVIEWER": ["publishContent", "rejectContent", "rejectContentV2"],
      
         "BOOK_CREATOR": ["copyContent", "createContent", "createLock", "updateCollaborators", "collectionImport", "collectionExport", "submitContentForReviewV1", "submitContentForReviewV3", "createAsset", "uploadAsset", "updateAsset", "uploadUrlAsset", "copyAsset", "listLock", "retireLock", "refreshLock", "updateContent", "uploadContent"],
      
         "CONTENT_CREATOR": ["updateBatch", "copyContent", "createContent", "createLock", "updateCollaborators", "collectionImport", "collectionExport", "submitContentForReviewV1", "submitContentForReviewV3", "submitDataExhaustRequest", "getDataExhaustRequest", "listDataExhaustRequest", "createAsset", "uploadAsset", "updateAsset", "uploadUrlAsset", "copyAsset", "listLock", "retireLock", "refreshLock", "updateContent", "uploadContent", "courseBatchAddCertificateTemplate", "courseBatchRemoveCertificateTemplate", "createBatch"],
      
         "COURSE_CREATOR": ["updateBatch", "copyContent", "createContent", "createLock", "updateCollaborators", "collectionImport", "collectionExport", "submitContentForReviewV1", "submitContentForReviewV3", "createAsset", "uploadAsset", "updateAsset", "uploadUrlAsset", "copyAsset", "listLock", "retireLock", "refreshLock",  "updateContent", "uploadContent", "courseBatchAddCertificateTemplate", "courseBatchRemoveCertificateTemplate", "createBatch"],
      
         "COURSE_MENTOR": ["updateBatch", "submitDataExhaustRequest", "getDataExhaustRequest", "listDataExhaustRequest", "courseBatchAddCertificateTemplate", "courseBatchRemoveCertificateTemplate", "createBatch"],
      
         "PROGRAM_MANAGER": ["submitDataExhaustRequest", "getDataExhaustRequest", "listDataExhaustRequest"],
      
         "PROGRAM_DESIGNER": ["submitDataExhaustRequest", "getDataExhaustRequest", "listDataExhaustRequest"],
      
         "ORG_ADMIN": ["acceptTnc", "assignRole", "submitDataExhaustRequest", "getDataExhaustRequest", "listDataExhaustRequest", "getUserProfileV5", "updateUserV2", "readUserConsent", "createTenantPreferences", "updateTenantPreferences", "createReport", "deleteReport", "updateReport", "publishReport", "retireReport", "getReportSummary", "listReportSummary", "createReportSummary"],
      
         "REPORT_VIEWER": ["acceptTnc", "getReportSummary", "listReportSummary"],
      
         "REPORT_ADMIN": ["submitDataExhaustRequest", "getDataExhaustRequest", "listDataExhaustRequest", "acceptTnc", "createReport", "deleteReport", "updateReport", "publishReport", "retireReport", "getReportSummary", "listReportSummary", "createReportSummary"],
      
         "PUBLIC": ["PUBLIC"]
      }
      
      x_authenticated_user_token := http_request.headers["x-authenticated-user-token"]
      x_authenticated_for := http_request.headers["x-authenticated-for"]
      private_ingressgateway_ip := ""
      
      jwt_public_keys := {
      "KEYCLOAK_KID": "KEYCLOAK_PUBLIC_KEY"
      }
      
      user_token := {"header": header, "payload": payload} {
        encoded := x_authenticated_user_token
        [header, payload, _] := io.jwt.decode(encoded)
      }
      
      for_token := {"payload": payload} {
        encoded := x_authenticated_for
        [_, payload, _] := io.jwt.decode(encoded)
      }
      
      iss := "https:///auth/realms/sunbird"
      token_kid := user_token.header.kid
      token_iss := user_token.payload.iss
      token_exp := user_token.payload.exp
      current_time := time.now_ns()
      
      token_sub := split(user_token.payload.sub, ":")
      # Check for both cases - With and without federation_id in sub field as below
      # sub := f:federation_id:user_id OR sub := user_id
      token_userid = token_sub[2] {
          count(token_sub) == 3
      } else = token_sub[0] {
          count(token_sub) == 1
      }
      for_token_userid := for_token.payload.sub
      for_token_parentid := for_token.payload.parentId
      
      # Desktop app is still using keycloak tokens which will not have roles
      # This is a temporary fix where we will append the roles as PUBLIC in OPA
      
      default_role := [{"role": "PUBLIC", "scope": []}]
      
      token_roles = user_token.payload.roles {
          user_token.payload.roles
      } else = default_role {
          not user_token.payload.roles
      }
      
      for_token_exists {
        x_authenticated_for
        count(x_authenticated_for) > 0
      }
      
      userid = token_userid {
          not x_authenticated_for
      } else = token_userid {
          count(x_authenticated_for) == 0 # This is a temporary fix as the mobile app is sending empty headers as x-authenticated-for: ""
      } else = for_token_userid {
          for_token_exists
      }
      
      validate_token {
        io.jwt.verify_rs256(x_authenticated_user_token, jwt_public_keys[token_kid])
        token_exp * 1000000000 > current_time
        token_iss == iss
      }
      
      is_an_internal_request {
        http_request.host == private_ingressgateway_ip
      }
      
      acls_check(acls) = indicies {
        validate_token
        indicies := [idx | some i; ROLES[token_roles[i].role][_] == acls[_]; idx := i]
        count(indicies) > 0
      }
      
      role_check(roles) = indicies {
        indicies := [idx | some i; token_roles[i].role in roles; idx := i]
        count(indicies) > 0
      }
      
      org_check(roles) = token_organisationids {
        indicies :=  role_check(roles)
        count(indicies) > 0
        token_organisationids := [ids | ids := token_roles[indicies[_]].scope[_].organisationId]
        count(token_organisationids) > 0
      }
      
      parent_id_check {
          x_authenticated_for
          count(x_authenticated_for) > 0
          token_userid == for_token_parentid
      }
      
      parent_id_check {
          count(x_authenticated_for) == 0
      }
      
      parent_id_check {
          not x_authenticated_for
      }
      
      public_role_check {
        acls := ["PUBLIC"]
        roles := ["PUBLIC"]
        acls_check(acls)
        role_check(roles)
        userid
        parent_id_check
      }
  
  main.rego: |-
      package main
      
      import input.attributes.request.http as http_request
      import data.policies as policy
      import future.keywords.in
      
      default allow = {
        "allowed": false,
        "headers": {"x-request-allowed": "no"},
        "body": "You do not have permission to perform this operation",
        "http_status": 403
      }
      
      urls[keys] { policy.urls_to_action_mapping[keys]}
      
      regex_urls := [url | url := regex.find_n(urls[_], http_request.path, 1)[0]]
      matching_urls := [url | some i; startswith(http_request.path, regex_urls[i]); url := regex_urls[i]]
      identified_url := max(matching_urls)
      identified_action := policy.urls_to_action_mapping[identified_url]
      
      # Desktop app is not sending x-authenticated-for header due to which managed user flow is breaking
      # This is a temporary fix till the desktop app issue is fixed
      skipped_consumers := ["portal", "desktop"]
      x_consumer_username := http_request.headers["x-consumer-username"]
      check_if_consumer_is_skipped {
         x_consumer_username in skipped_consumers
      }
      
      allow = status {
         not check_if_consumer_is_skipped
         policy[identified_action]
         status := {
            "allowed": true,
            "headers": {"x-request-allowed": "yes"},
            "body": "OPA Checks Passed",
            "http_status": 200
         }
      }
      
      allow = status {
         not identified_action
         status := {
            "allowed": true,
            "headers": {"x-request-allowed": "yes"},
            "body": "OPA Checks Skipped",
            "http_status": 200
         }
      }
      
      # Desktop app is not sending x-authenticated-for header due to which managed user flow is breaking
      # This is a temporary fix till the desktop app issue is fixed
      allow = status {
         check_if_consumer_is_skipped
         status := {
            "allowed": true,
            "headers": {"x-request-allowed": "yes"},
            "body": "OPA Checks Skipped",
            "http_status": 200
         }
      }
  
  policies.rego: |-
      package policies
      
      import data.common as super
      import future.keywords.in
      import input.attributes.request.http as http_request
      
      x_channel_id := http_request.headers["x-channel-id"]
      
      urls_to_action_mapping := {
      # "/content/v3/create": "createContent",
        "/collection/v4/import": "collectionImport",
        "/collection/v4/export": "collectionExport",
        "/content/v3/review": "submitContentForReviewV3",
        "/asset/v4/create": "createAsset",
        "/asset/v4/update": "updateAsset",
        "/asset/v4/upload/url": "uploadUrlAsset",
        "/asset/v4/upload": "uploadAsset",
        "/asset/v4/copy": "copyAsset",
        "/content/v4/reject": "rejectContentV2"
      }
      
      collectionImport {
        acls := ["collectionImport"]
        roles := ["BOOK_CREATOR", "CONTENT_CREATOR", "COURSE_CREATOR"]
        super.acls_check(acls)
        super.role_check(roles)
      }
      
      collectionExport {
        acls := ["collectionExport"]
        roles := ["BOOK_CREATOR", "CONTENT_CREATOR", "COURSE_CREATOR"]
        super.acls_check(acls)
        super.role_check(roles)
      }
      
      submitContentForReviewV3 {
        acls := ["submitContentForReviewV3"]
        roles := ["BOOK_CREATOR", "CONTENT_CREATOR", "COURSE_CREATOR"]
        super.acls_check(acls)
        super.role_check(roles)
      }
      
      createAsset {
        acls := ["createAsset"]
        roles := ["BOOK_CREATOR", "CONTENT_CREATOR", "COURSE_CREATOR"]
        super.acls_check(acls)
        # Org check will do an implicit role check so there is no need to invoke super.role_check(roles)
        token_organisationids := super.org_check(roles)
        x_channel_id in token_organisationids
        input.parsed_body.request.asset.channel in token_organisationids
        input.parsed_body.request.asset.channel == x_channel_id
        input.parsed_body.request.asset.createdBy == super.userid
      }
      
      # Optional request.asset.createdBy in payload - https://project-sunbird.atlassian.net/browse/SB-29753
      createAsset {
        acls := ["createAsset"]
        roles := ["BOOK_CREATOR", "CONTENT_CREATOR", "COURSE_CREATOR"]
        super.acls_check(acls)
        # Org check will do an implicit role check so there is no need to invoke super.role_check(roles)
        token_organisationids := super.org_check(roles)
        x_channel_id in token_organisationids
        input.parsed_body.request.asset.channel in token_organisationids
        input.parsed_body.request.asset.channel == x_channel_id
        not input.parsed_body.request.asset.createdBy
      }
      
      updateAsset {
        acls := ["updateAsset"]
        roles := ["BOOK_CREATOR", "CONTENT_CREATOR", "COURSE_CREATOR"]
        super.acls_check(acls)
        # Org check will do an implicit role check so there is no need to invoke super.role_check(roles)
        token_organisationids := super.org_check(roles)
        x_channel_id in token_organisationids
      }
      
      uploadAsset {
        acls := ["uploadAsset"]
        roles := ["BOOK_CREATOR", "CONTENT_CREATOR", "COURSE_CREATOR"]
        super.acls_check(acls)
        # Org check will do an implicit role check so there is no need to invoke super.role_check(roles)
        token_organisationids := super.org_check(roles)
        x_channel_id in token_organisationids
      }
      
      uploadUrlAsset {
        acls := ["uploadUrlAsset"]
        roles := ["BOOK_CREATOR", "CONTENT_CREATOR", "COURSE_CREATOR"]
        super.acls_check(acls)
        # Org check will do an implicit role check so there is no need to invoke super.role_check(roles)
        token_organisationids := super.org_check(roles)
        x_channel_id in token_organisationids
      }
      
      copyAsset {
        acls := ["copyAsset"]
        roles := ["BOOK_CREATOR", "CONTENT_CREATOR", "COURSE_CREATOR"]
        super.acls_check(acls)
        # Org check will do an implicit role check so there is no need to invoke super.role_check(roles)
        token_organisationids := super.org_check(roles)
        x_channel_id in token_organisationids
      }
      
      rejectContentV2 {
        acls := ["rejectContentV2"]
        roles := ["BOOK_REVIEWER", "CONTENT_REVIEWER", "FLAG_REVIEWER"]
        super.acls_check(acls)
        super.role_check(roles)
      }
      
      
      # createContent {
      #   # acls := ["createContent"]
      #   #
      #   #  # Due to portal legacy code, we need to add REVIEWER roles also for this API
      #   # roles := ["BOOK_CREATOR", "CONTENT_CREATOR", "COURSE_CREATOR", "BOOK_REVIEWER", "CONTENT_REVIEWER"]
      #   # super.acls_check(acls)
      #   #
      #   #  # Org check will do an implicit role check so there is no need to invoke super.role_check(roles)
      #   # token_organisationids := super.org_check(roles)
      #   #
      #   #  # The below payload is being invoked when creating contents
      #   # input.parsed_body.request.content.createdFor[_] in token_organisationids
      #   # input.parsed_body.request.content.createdBy == super.userid
      
      #   # This rule has been disabled since request from VDN flink job is directly invoking content service
      #   # via private ingress and is not passing any tokens / headers.
      #   # Hence this is blocking the creation workflow in VDN.
      #   # This should be moved to a system token if invoked via private ingess.
      #   true
      # }
---
# Source: knowledgebb/charts/content/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: content-envoy
  labels:

    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: content
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: content-0.1.0
  annotations:

    reloader.stakater.com/auto: "true"
data:
  
  config.yaml: |-
      static_resources:
        listeners:
        - name: listener_0
          address:
            socket_address:
              address: 0.0.0.0
              port_value: 9999
          per_connection_buffer_limit_bytes: 62914560
          filter_chains:
          - filters:
            - name: envoy.http_connection_manager
              typed_config:
                "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
                codec_type: auto
                stat_prefix: ingress_http
                access_log:
                - name: envoy.access_loggers.stdout
                  typed_config:
                    "@type": type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog
                    log_format:
                      text_format: "%DOWNSTREAM_DIRECT_REMOTE_ADDRESS_WITHOUT_PORT% - - [%START_TIME(%d/%b/%Y:%H:%M:%S %z)%] \"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\" %RESPONSE_CODE% %RESPONSE_FLAGS% %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \"%REQ(X-FORWARDED-FOR)%\" \"%REQ(USER-AGENT)%\" \"%REQ(X-REQUEST-ID)%\" \"%REQ(X-DEVICE-ID)%\" \"%REQ(X-CHANNEL-ID)%\" \"%REQ(X-APP-ID)%\" \"%REQ(X-APP-VER)%\" \"%REQ(X-SESSION-ID)%\" \"%REQ(:AUTHORITY)%\" \"%UPSTREAM_HOST%\"\n"
                route_config:
                  name: local_route
                  virtual_hosts:
                  - name: backend
                    domains:
                    - "*"
                    routes:
                    - match:
                        prefix: "/opa/metrics"
                      route:
                        prefix_rewrite: "/metrics"
                        cluster: opa
                      typed_per_filter_config:
                        envoy.filters.http.ext_authz:
                          "@type": type.googleapis.com/envoy.extensions.filters.http.ext_authz.v3.ExtAuthzPerRoute
                          disabled: true
                    - match:
                        prefix: "/health"
                      route:
                        cluster: service
                      typed_per_filter_config:
                        envoy.filters.http.ext_authz:
                          "@type": type.googleapis.com/envoy.extensions.filters.http.ext_authz.v3.ExtAuthzPerRoute
                          disabled: true
                    - match:
                        prefix: "/health"
                      route:
                        cluster: service
                      typed_per_filter_config:
                        envoy.filters.http.ext_authz:
                          "@type": type.googleapis.com/envoy.extensions.filters.http.ext_authz.v3.ExtAuthzPerRoute
                          disabled: true
                    - match:
                        prefix: "/"
                      route:
                        cluster: service
                        timeout: 60s
                http_filters:
                - name: envoy.filters.http.ext_authz
                  typed_config:
                    "@type": type.googleapis.com/envoy.extensions.filters.http.ext_authz.v3.ExtAuthz
                    transport_api_version: V3
                    with_request_body:
                      max_request_bytes: 62914560
                      allow_partial_message: true
                    failure_mode_allow: true
                    grpc_service:
                      google_grpc:
                        target_uri: 127.0.0.1:9191
                        stat_prefix: ext_authz
                      timeout: 5s
                - name: envoy.filters.http.router
        clusters:
        - name: service
          connect_timeout: 5s
          per_connection_buffer_limit_bytes: 62914560
          type: static
          load_assignment:
            cluster_name: service
            endpoints:
            - lb_endpoints:
              - endpoint:
                  address:
                    socket_address:
                      address: 127.0.0.1
                      port_value: 9000
        - name: opa
          connect_timeout: 5s
          type: static
          load_assignment:
            cluster_name: opa
            endpoints:
            - lb_endpoints:
              - endpoint:
                  address:
                    socket_address:
                      address: 127.0.0.1
                      port_value: 8181
      admin:
        access_log_path: "/dev/stdout"
        address:
          socket_address:
            address: 0.0.0.0
            port_value: 10000
---
# Source: knowledgebb/charts/dial/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: dial
  labels:

    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: dial
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: dial-0.1.0
  annotations:

    reloader.stakater.com/auto: "true"
data: # Take only root level files (configs/*) for configmaps # Skip env.yaml as configmap, as it's env file
    
  application.conf: |-
      # This is the main configuration file for the application.
      # https://www.playframework.com/documentation/latest/ConfigFile
      # ~~~~~
      
      ## Akka
      # https://www.playframework.com/documentation/latest/JavaAkka#Configuration
      # ~~~~~
      akka {
          #loggers =["akka.event.Logging$DefaultLogger"]
          #log-config-on-start = true
      }
      
      ## Secret key
      # http://www.playframework.com/documentation/latest/ApplicationSecret
      # ~~~~~
      #play.crypto.secret = "changeme"
      
      
      ## Internationalisation
      # https://www.playframework.com/documentation/latest/JavaI18N
      # ~~~~~
      play.i18n {
          # The application languages
          langs = [ "en" ]
      }
      
      ## Play HTTP settings
      # ~~~~~
      play.http {
          ## Router
          # https://www.playframework.com/documentation/latest/JavaRouting
          # https://www.playframework.com/documentation/latest/ScalaRouting
          # ~~~~~
          # Define the Router object to use for this application.
          # This router will be looked up first when the application is starting up,
          # so make sure this is the entry point.
          # Furthermore, it's assumed your route file is named properly.
          # So for an application router like `my.application.Router`,
          # you may need to define a router file `conf/my.application.routes`.
          # Default to Routes in the root package (aka "apps" folder) (and conf/routes)
          #router = my.application.Router
          #TO allow more data in request body
          parser.maxDiskBuffer=50MB
          parser.maxMemoryBuffer=50MB
      }
      
      ## Netty Provider
      # ~~~~~
      play.server.netty {
          # If you run Play on Linux, you can use Netty's native socket transport
          # for higher performance with less garbage.
          #transport = "native"
          maxChunkSize = 30000000
      }
      
      ## WS (HTTP Client)
      # ~~~~~
      libraryDependencies += javaWs
      
      ## Cache
      # https://www.playframework.com/documentation/latest/JavaCache
      # https://www.playframework.com/documentation/latest/ScalaCache
      # ~~~~~
      # Play comes with an integrated cache API that can reduce the operational
      # overhead of repeated requests. You must enable this by adding to build.sbt:
      #
      libraryDependencies += cache
      #
      play.cache {
          # If you want to bind several caches, you can bind the individually
          #bindCaches = ["db-cache", "user-cache", "session-cache"]
      }
      
      //play.http.filters= filters.HealthCheckFilter
      
      # Logger
      # ~~~~~
      # You can also configure logback (http://logback.qos.ch/),
      # by providing an application-logger.xml file in the conf directory.
      
      # Root logger:
      #logger.root=ERROR
      
      # Logger used by the framework:
      #logger.play=INFO
      
      # Logger provided to your application:
      #logger.application=DEBUG
      
      # APP Specific config
      # ~~~~~
      # Application specific configurations can be provided here
      application.global=Global
      
      play.http.parser.maxMemoryBuffer = 900000K
      play.http.parser.maxDiskBuffer = 900000K
      
      play.server.http.port = 9000
      # Logger
      # ~~~~~
      # You can also configure logback (http://logback.qos.ch/),
      # by providing an application-logger.xml file in the conf directory.
      
      # Root logger:
      logger.root=ERROR
      
      # Logger used by the framework:
      logger.play=INFO
      
      # Logger provided to your application:
      logger.application=DEBUG
      
      # APP Specific config
      # ~~~~~
      # Application specific configurations can be provided here
      
      # Cache-Manager Configuration
      cache.type="redis"
      
      
      search.es_conn_info="elasticsearch:9200"
      search.fields.query=["name^100","title^100","lemma^100","code^100","tags^100","question^100","domain","subject","description^10","keywords^25","ageGroup^10","filter^10","theme^10","genre^10","objects^25","contentType^100","language^200","teachingMode^25","skills^10","learningObjective^10","curriculum^100","gradeLevel^100","developer^100","attributions^10","owner^50","text","words","releaseNotes","body"]
      search.fields.date= ["lastUpdatedOn","createdOn","versionDate","lastSubmittedOn","lastPublishedOn"]
      search.batch.size=500
      search.connection.timeout=30
      
      #Top N Config for Search Telemetry
      telemetry.search.topn=5
      telemetry_env= dev
      installation.id=ekstep
      
      # Configuration for default channel ID
      channel.default="in.ekstep"
      
      # Redis Configuration
      redis {
          host:"redis-master"
          port:"6379"
          maxConnections:"128"
      }
      
      # DIAL Code Configuration
      dialcode.keyspace.name="_dialcode_store"
      dialcode.keyspace.table="dial_code"
      dialcode.max_count="100"
      
      # System Configuration
      system.config.keyspace.name="_dialcode_store"
      system.config.table="system_config"
      
      #Publisher Configuration
      publisher.keyspace.name="_dialcode_store"
      publisher.keyspace.table="publisher"
      
      #QR coddes
      qrcodes.keyspace.name="dialcodes"
      qrcodes.keyspace.table="dialcode_batch"
      
      #DIAL Code Generator Configuration
      dialcode.strip.chars="0"
      dialcode.length=6.0
      dialcode.large.prime_number=1679979167
      
      #DIAL Code ElasticSearch Configuration
      dialcode.index="true"
      dialcode.object_type="DialCode"
      
      # Cassandra Configuration
      cassandra.lp.connection="cassandra:9042"
      cassandra.lpa.connection="cassandra:9042"
      
      
      # Consistency Level for Multi Node Cassandra cluster
      
      cassandra.lp.consistency.level=LOCAL_QUORUM
      
      #Kafka Configuration
      kafka {
          urls = "kafka:9092"
          topic {
          send_enable = true
          graph_event = ".knowlg.learning.graph.events"
          }
      }
      
      jsonld {
          basePath = "{{ .Values.global.object_storage_endpoint }}/{{ .Values.global.public_container_name }}/sunbird-dial-staging/jsonld-schema/local"
          type = "sbed"
          localPath = "/tmp"
          ttl = 43200
          sb_schema = ["http://store.knowlg.sunbird.org/dial/specs/sbed/schema.jsonld"]
      }
      
      cloudstorage {
          metadata.replace_absolute_path="true"
          relative_path_prefix="DIAL_STORAGE_BASE_PATH"
          read_base_path="https://"
      }
      
      dial_id = "https:///dial/{dialcode}"
      dial_type = "https:///ns/"
      cloud_storage_container = "/dial" # Take only root level files (configs/*) for configmaps # Take only root level files (configs/*) for configmaps # Skip env.yaml as configmap, as it's env file
    
  logback.xml: |-
      <configuration>
      
          <conversionRule conversionWord="coloredLevel" converterClass="play.api.libs.logback.ColoredLevel" />
      
          <!-- transaction-event-trigger START -->
          <timestamp key="timestamp" datePattern="yyyy-MM-dd"/>
          <!-- common transactions logs -->
          <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
              <encoder>
                  <pattern>%d %msg%n</pattern>
              </encoder>
          </appender>
      
          <appender name="ASYNCSTDOUT" class="ch.qos.logback.classic.AsyncAppender">
              <appender-ref ref="STDOUT" />
          </appender>
      
      
          <logger name="play" level="INFO" />
          <logger name="DefaultPlatformLogger" level="INFO" />
          <!-- Telemetry Loggers-->
          <logger name="TelemetryEventLogger" level="INFO" />
      
          <root level="INFO">
              <appender-ref ref="ASYNCSTDOUT" />
          </root>
      
      </configuration>
---
# Source: knowledgebb/charts/dial/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: dial-env
  labels:

    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: dial
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: dial-0.1.0
  annotations:

    reloader.stakater.com/auto: "true"
data:
  # You can add key value pair here, to create env values.
  # for example,
  
  # ENV: dev
  JAVA_OPTIONS: -Xms500m -Xmx500m
  _JAVA_OPTIONS: -Dlog4j2.formatMsgNoLookups=true
---
# Source: knowledgebb/charts/flink/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: asset-enrichment-config
  labels:

    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:

    reloader.stakater.com/auto: "true"
data:
  config: |-
    include file("/data/flink/conf/base-config.conf")
    job {
      env = ""
    }
    
    kafka {
      input.topic = ".knowlg.learning.job.request"
      groupId = "-asset-enrichment-group"
      video_stream.topic = ".knowlg.content.postpublish.request"
    }
    
    task {
      consumer.parallelism = 1
      router.parallelism = 1
      videoEnrichment.parallelism = 1
      imageEnrichment.parallelism = 1
    }
    
    content {
      stream {
        enabled = false
        mimeType = ["video/mp4", "video/webm"]
      }
      youtube {
        applicationName = "fetch-youtube-license"
        regexPattern = ["\\?vi?=([^&]*)", "watch\\?.*v=([^&]*)", "(?:embed|vi?)/([^/?]*)", "^([A-Za-z0-9\\-\\_]*)"]
      }
      upload.context.driven = true
      max.iteration.count = 2
    }
    
    thumbnail.max {
      sample = 5
      size.pixel = 150
    }
    
    cloudstorage.metadata.replace_absolute_path=true
    cloudstorage.relative_path_prefix="CONTENT_STORAGE_BASE_PATH"
    cloudstorage.read_base_path= https://
    cloudstorage.write_base_path= ["https://"]
    cloudstorage.metadata.list= ["appIcon","posterImage","artifactUrl","downloadUrl","variants","previewUrl","pdfUrl", "streamingUrl", "toc_url"]
    
    cloud_storage_type=""
    cloud_storage_key=""
    cloud_storage_secret=""
    cloud_storage_container=""
    cloud_storage_endpoint= 
    
    
    
  flink-conf: |-
    jobmanager.memory.flink.size: 1024m
    taskmanager.memory.flink.size: 1024m
    taskmanager.numberOfTaskSlots: 1
    jobManager.numberOfTaskSlots: 1
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    scheduler-mode: reactive
    heartbeat.timeout: 8000
    heartbeat.interval: 5000
    taskmanager.memory.process.size: 1700m
    jobmanager.memory.process.size: 1600m
    # classloader.resolve-order: "parent-first"
    state.backend: filesystem
    
  base-config: |-
    kafka {
      broker-servers = "kafka:9092"
      producer.broker-servers = "kafka:9092"
      consumer.broker-servers = "kafka:9092"
      zookeeper = "zookeeper:2181"
      producer {
        max-request-size = 1572864
        batch.size = 98304
        linger.ms = 10
        compression = "snappy"
      }
      output.system.event.topic = ".knowlg.system.events"
    }
    job {
      env = ""
      enable.distributed.checkpointing = true
      # No valid checkpoint_store_type configured, skipping statebackend.
    }
      
    task {
      parallelism = 1
      consumer.parallelism = 1
      checkpointing.compressed = true
      checkpointing.interval = 60000
      checkpointing.pause.between.seconds = 3000
      restart-strategy.attempts = 3
      restart-strategy.delay = 30000 # in milli-seconds
    }
    redis {
    host = "redis-master"
    port = "6379"
    connection {
      max = 2
      idle.min = 1
      idle.max = 2
      minEvictableIdleTimeSeconds = 120
      timeBetweenEvictionRunsSeconds = 300
        }
    }
    lms-cassandra {
    host = "cassandra"
    port = "9042"
    }
    
    neo4j {
    routePath = "bolt://neo4j:7687"
    graph = "domain"
    }
    
    es {
    basePath = "elasticsearch:9200"
    }
    
  log4j_console_properties: |-
    # This affects logging for both user code and Flink
    rootLogger.level = INFO
    rootLogger.appenderRef.console.ref = ConsoleAppender
    
    # Uncomment this if you want to _only_ change Flink's logging
    #logger.flink.name = org.apache.flink
    #logger.flink.level = INFO
    
    # The following lines keep the log level of common libraries/connectors on
    # log level INFO. The root logger does not override this. You have to manually
    # change the log levels here.
    logger.akka.name = akka
    logger.akka.level = ERROR
    logger.kafka.name= org.apache.kafka
    logger.kafka.level = ERROR
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = ERROR
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = ERROR
    
    # Log all infos to the console
    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    
    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF
---
# Source: knowledgebb/charts/flink/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: audit-event-generator-config
  labels:

    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:

    reloader.stakater.com/auto: "true"
data:
  config: |-
    include file("/data/flink/conf/base-config.conf")
    job {
      env = ""
    }
    
    kafka {
      input.topic = ".knowlg.learning.graph.events"
      output.topic = ".knowlg.telemetry.raw"
      groupId = "-audit-event-generator-group"
    }
    
    task {
    consumer.parallelism = 1
    parallelism = 1
    producer.parallelism = 1
    window.time = 60
    }
    
    schema {
      basePath = "https:////schemas/local/content/"
    }
    
    channel.default = "org.sunbird"
    
    
  flink-conf: |-
    jobmanager.memory.flink.size: 1024m
    taskmanager.memory.flink.size: 1024m
    taskmanager.numberOfTaskSlots: 1
    jobManager.numberOfTaskSlots: 1
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    scheduler-mode: reactive
    heartbeat.timeout: 8000
    heartbeat.interval: 5000
    taskmanager.memory.process.size: 1700m
    jobmanager.memory.process.size: 1600m
    # classloader.resolve-order: "parent-first"
    state.backend: filesystem
    
  base-config: |-
    kafka {
      broker-servers = "kafka:9092"
      producer.broker-servers = "kafka:9092"
      consumer.broker-servers = "kafka:9092"
      zookeeper = "zookeeper:2181"
      producer {
        max-request-size = 1572864
        batch.size = 98304
        linger.ms = 10
        compression = "snappy"
      }
      output.system.event.topic = ".knowlg.system.events"
    }
    job {
      env = ""
      enable.distributed.checkpointing = true
      # No valid checkpoint_store_type configured, skipping statebackend.
    }
      
    task {
      parallelism = 1
      consumer.parallelism = 1
      checkpointing.compressed = true
      checkpointing.interval = 60000
      checkpointing.pause.between.seconds = 3000
      restart-strategy.attempts = 3
      restart-strategy.delay = 30000 # in milli-seconds
    }
    redis {
    host = "redis-master"
    port = "6379"
    connection {
      max = 2
      idle.min = 1
      idle.max = 2
      minEvictableIdleTimeSeconds = 120
      timeBetweenEvictionRunsSeconds = 300
        }
    }
    lms-cassandra {
    host = "cassandra"
    port = "9042"
    }
    
    neo4j {
    routePath = "bolt://neo4j:7687"
    graph = "domain"
    }
    
    es {
    basePath = "elasticsearch:9200"
    }
    
  log4j_console_properties: |-
    # This affects logging for both user code and Flink
    rootLogger.level = INFO
    rootLogger.appenderRef.console.ref = ConsoleAppender
    
    # Uncomment this if you want to _only_ change Flink's logging
    #logger.flink.name = org.apache.flink
    #logger.flink.level = INFO
    
    # The following lines keep the log level of common libraries/connectors on
    # log level INFO. The root logger does not override this. You have to manually
    # change the log levels here.
    logger.akka.name = akka
    logger.akka.level = ERROR
    logger.kafka.name= org.apache.kafka
    logger.kafka.level = ERROR
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = ERROR
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = ERROR
    
    # Log all infos to the console
    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    
    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF
---
# Source: knowledgebb/charts/flink/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: audit-history-indexer-config
  labels:

    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:

    reloader.stakater.com/auto: "true"
data:
  config: |-
    include file("/data/flink/conf/base-config.conf")
    job {
      env = ""
    }
    
    kafka {
      input.topic = ".knowlg.learning.graph.events"
      groupId = "-audit-history-indexer-group"
    }
    
    task {
      consumer.parallelism = 1
      parallelism = 1
      window.time = 60
    }
    
    timezone = "IST"
    
    
  flink-conf: |-
    jobmanager.memory.flink.size: 1024m
    taskmanager.memory.flink.size: 1024m
    taskmanager.numberOfTaskSlots: 1
    jobManager.numberOfTaskSlots: 1
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    scheduler-mode: reactive
    heartbeat.timeout: 8000
    heartbeat.interval: 5000
    taskmanager.memory.process.size: 1700m
    jobmanager.memory.process.size: 1600m
    # classloader.resolve-order: "parent-first"
    state.backend: filesystem
    
  base-config: |-
    kafka {
      broker-servers = "kafka:9092"
      producer.broker-servers = "kafka:9092"
      consumer.broker-servers = "kafka:9092"
      zookeeper = "zookeeper:2181"
      producer {
        max-request-size = 1572864
        batch.size = 98304
        linger.ms = 10
        compression = "snappy"
      }
      output.system.event.topic = ".knowlg.system.events"
    }
    job {
      env = ""
      enable.distributed.checkpointing = true
      # No valid checkpoint_store_type configured, skipping statebackend.
    }
      
    task {
      parallelism = 1
      consumer.parallelism = 1
      checkpointing.compressed = true
      checkpointing.interval = 60000
      checkpointing.pause.between.seconds = 3000
      restart-strategy.attempts = 3
      restart-strategy.delay = 30000 # in milli-seconds
    }
    redis {
    host = "redis-master"
    port = "6379"
    connection {
      max = 2
      idle.min = 1
      idle.max = 2
      minEvictableIdleTimeSeconds = 120
      timeBetweenEvictionRunsSeconds = 300
        }
    }
    lms-cassandra {
    host = "cassandra"
    port = "9042"
    }
    
    neo4j {
    routePath = "bolt://neo4j:7687"
    graph = "domain"
    }
    
    es {
    basePath = "elasticsearch:9200"
    }
    
  log4j_console_properties: |-
    # This affects logging for both user code and Flink
    rootLogger.level = INFO
    rootLogger.appenderRef.console.ref = ConsoleAppender
    
    # Uncomment this if you want to _only_ change Flink's logging
    #logger.flink.name = org.apache.flink
    #logger.flink.level = INFO
    
    # The following lines keep the log level of common libraries/connectors on
    # log level INFO. The root logger does not override this. You have to manually
    # change the log levels here.
    logger.akka.name = akka
    logger.akka.level = ERROR
    logger.kafka.name= org.apache.kafka
    logger.kafka.level = ERROR
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = ERROR
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = ERROR
    
    # Log all infos to the console
    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    
    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF
---
# Source: knowledgebb/charts/flink/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: content-publish-config
  labels:

    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:

    reloader.stakater.com/auto: "true"
data:
  config: |-
    include file("/data/flink/conf/base-config.conf")
    
    job {
      env = ""
    }
    
    kafka {
      input.topic = ".knowlg.publish.job.request"
      post_publish.topic = ".knowlg.content.postpublish.request"
      mvc.topic = ".knowlg.mvc.processor.job.request"
      error.topic = ".knowlg.learning.events.failed"
      groupId = "-content-publish-group"
    }
    
    task {
      consumer.parallelism = 1
      parallelism = 1
      router.parallelism = 1
    }
    
    redis {
      database {
        contentCache.id = 0
      }
    }
    
    content {
      bundleLocation = "/tmp/contentBundle"
      isECARExtractionEnabled = true
      retry_asset_download_count = 1
      keyspace = "_content_store"
      table = "content_data"
      tmp_file_location = "/tmp"
      objectType = ["Content", "ContentImage","Collection","CollectionImage"]
      mimeType = ["application/pdf",
                                  "application/vnd.ekstep.ecml-archive",
                                  "application/vnd.ekstep.html-archive",
                                  "application/vnd.android.package-archive",
                                  "application/vnd.ekstep.content-archive",
                                  "application/epub",
                                  "application/msword",
                                  "application/vnd.ekstep.h5p-archive",
                                  "video/webm",
                                  "video/mp4",
                                  "application/vnd.ekstep.content-collection",
                                  "video/quicktime",
                                  "application/octet-stream",
                                  "application/json",
                                  "application/javascript",
                                  "application/xml",
                                  "text/plain",
                                  "text/html",
                                  "text/javascript",
                                  "text/xml",
                                  "text/css",
                                  "image/jpeg",
                                  "image/jpg",
                                  "image/png",
                                  "image/tiff",
                                  "image/bmp",
                                  "image/gif",
                                  "image/svg+xml",
                                  "image/x-quicktime",
                                  "video/avi",
                                  "video/mpeg",
                                  "video/quicktime",
                                  "video/3gpp",
                                  "video/mp4",
                                  "video/ogg",
                                  "video/webm",
                                  "video/msvideo",
                                  "video/x-msvideo",
                                  "video/x-qtc",
                                  "video/x-mpeg",
                                  "audio/mp3",
                                  "audio/mp4",
                                  "audio/mpeg",
                                  "audio/ogg",
                                  "audio/webm",
                                  "audio/x-wav",
                                  "audio/wav",
                                  "audio/mpeg3",
                                  "audio/x-mpeg-3",
                                  "audio/vorbis",
                                  "application/x-font-ttf",
                                  "application/vnd.ekstep.plugin-archive",
                                  "video/x-youtube",
                                  "video/youtube",
                                  "text/x-url"]
      asset_download_duration = "60 seconds"
    
      stream {
        enabled = false
        mimeType = ["video/mp4", "video/webm"]
      }
      artifact.size.for_online=209715200
    
      downloadFiles {
        spine = ["appIcon"]
        full = ["appIcon", "grayScaleAppIcon", "artifactUrl", "itemSetPreviewUrl", "media"]
      }
    
      nested.fields=["badgeAssertions", "targets", "badgeAssociations", "plugins", "me_totalTimeSpent", "me_totalPlaySessionCount", "me_totalTimeSpentInSec", "batches", "trackable", "credentials", "discussionForum", "provider", "osMetadata", "actions"]
    
    }
    
    hierarchy {
      keyspace = "_hierarchy_store"
      table = "content_hierarchy"
    }
    
    cloud_storage {
        folder {
            content = "content"
            artifact = "artifact"
        }
    }
    
    service {
      print.basePath = "http://print:5000"
    }
    
    contentTypeToPrimaryCategory {
      ClassroomTeachingVideo: "Explanation Content"
      ConceptMap: "Learning Resource"
      Course: "Course"
      CuriosityQuestionSet: "Practice Question Set"
      eTextBook: "eTextbook"
      Event: "Event"
      EventSet: "Event Set"
      ExperientialResource: "Learning Resource"
      ExplanationResource: "Explanation Content"
      ExplanationVideo: "Explanation Content"
      FocusSpot: "Teacher Resource"
      LearningOutcomeDefinition: "Teacher Resource"
      MarkingSchemeRubric: "Teacher Resource"
      PedagogyFlow: "Teacher Resource"
      PracticeQuestionSet: "Practice Question Set"
      PracticeResource: "Practice Question Set"
      SelfAssess: "Course Assessment"
      TeachingMethod: "Teacher Resource"
      TextBook: "Digital Textbook"
      Collection: "Content Playlist"
      ExplanationReadingMaterial: "Learning Resource"
      LearningActivity: "Learning Resource"
      LessonPlan: "Content Playlist"
      LessonPlanResource: "Teacher Resource"
      PreviousBoardExamPapers: "Learning Resource"
      TVLesson: "Explanation Content"
      OnboardingResource: "Learning Resource"
      ReadingMaterial: "Learning Resource"
      Template: "Template"
      Asset: "Asset"
      Plugin: "Plugin"
      LessonPlanUnit: "Lesson Plan Unit"
      CourseUnit: "Course Unit"
      TextBookUnit: "Textbook Unit"
      Asset: "Certificate Template"
    }
    
    max_allowed_content_name = 120
    enableDIALContextUpdate = "Yes"
    
    cloudstorage.metadata.replace_absolute_path=true
    cloudstorage.relative_path_prefix="CONTENT_STORAGE_BASE_PATH"
    cloudstorage.read_base_path="https://"
    cloudstorage.write_base_path=["https://"]
    cloudstorage.metadata.list=["appIcon","posterImage","artifactUrl","downloadUrl","variants","previewUrl","pdfUrl", "streamingUrl", "toc_url"]
    
    cloud_storage_type=""
    cloud_storage_key=""
    cloud_storage_secret=""
    cloud_storage_container=""
    cloud_storage_endpoint=""
    
  flink-conf: |-
    jobmanager.memory.flink.size: 1024m
    taskmanager.memory.flink.size: 1024m
    taskmanager.numberOfTaskSlots: 1
    jobManager.numberOfTaskSlots: 1
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    scheduler-mode: reactive
    heartbeat.timeout: 8000
    heartbeat.interval: 5000
    taskmanager.memory.process.size: 1700m
    jobmanager.memory.process.size: 1600m
    # classloader.resolve-order: "parent-first"
    state.backend: filesystem
    
  base-config: |-
    kafka {
      broker-servers = "kafka:9092"
      producer.broker-servers = "kafka:9092"
      consumer.broker-servers = "kafka:9092"
      zookeeper = "zookeeper:2181"
      producer {
        max-request-size = 1572864
        batch.size = 98304
        linger.ms = 10
        compression = "snappy"
      }
      output.system.event.topic = ".knowlg.system.events"
    }
    job {
      env = ""
      enable.distributed.checkpointing = true
      # No valid checkpoint_store_type configured, skipping statebackend.
    }
      
    task {
      parallelism = 1
      consumer.parallelism = 1
      checkpointing.compressed = true
      checkpointing.interval = 60000
      checkpointing.pause.between.seconds = 3000
      restart-strategy.attempts = 3
      restart-strategy.delay = 30000 # in milli-seconds
    }
    redis {
    host = "redis-master"
    port = "6379"
    connection {
      max = 2
      idle.min = 1
      idle.max = 2
      minEvictableIdleTimeSeconds = 120
      timeBetweenEvictionRunsSeconds = 300
        }
    }
    lms-cassandra {
    host = "cassandra"
    port = "9042"
    }
    
    neo4j {
    routePath = "bolt://neo4j:7687"
    graph = "domain"
    }
    
    es {
    basePath = "elasticsearch:9200"
    }
    
  log4j_console_properties: |-
    # This affects logging for both user code and Flink
    rootLogger.level = INFO
    rootLogger.appenderRef.console.ref = ConsoleAppender
    
    # Uncomment this if you want to _only_ change Flink's logging
    #logger.flink.name = org.apache.flink
    #logger.flink.level = INFO
    
    # The following lines keep the log level of common libraries/connectors on
    # log level INFO. The root logger does not override this. You have to manually
    # change the log levels here.
    logger.akka.name = akka
    logger.akka.level = ERROR
    logger.kafka.name= org.apache.kafka
    logger.kafka.level = ERROR
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = ERROR
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = ERROR
    
    # Log all infos to the console
    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    
    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF
---
# Source: knowledgebb/charts/flink/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: dialcode-context-updater-config
  labels:

    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:

    reloader.stakater.com/auto: "true"
data:
  config: |-
    include file("/data/flink/conf/base-config.conf")
    job {
      env = ""
    }
    
    kafka {
      input.topic = ".knowlg.dialcode.context.job.request"
      failed.topic = ".knowlg.dialcode.context.job.request.failed"
      groupId = "-dialcode-group"
    }
    
    task {
      consumer.parallelism = 1
      parallelism = 1
      dialcode-context-updater.parallelism = 1
    }
    
    dialcode_context_updater {
        actions="dialcode-context-update"
        search_mode="Collection"
        context_map_path = "https://raw.githubusercontent.com/project-sunbird/knowledge-platform-jobs/release-5.0.0/dialcode-context-updater/src/main/resources/contextMapping.json"
        identifier_search_fields = ["identifier", "primaryCategory", "channel"]
        dial_code_context_read_api_path = "/dialcode/v4/read/"
        dial_code_context_update_api_path = "/dialcode/v4/update/"
    }
    
    service {
      search.basePath = "http://search-service:9000"
      dial_service.basePath = "http://dial-service:9000"
    }
    
    es_sync_wait_time = 5000
    
    
    
  flink-conf: |-
    jobmanager.memory.flink.size: 1024m
    taskmanager.memory.flink.size: 1024m
    taskmanager.numberOfTaskSlots: 1
    jobManager.numberOfTaskSlots: 1
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    scheduler-mode: reactive
    heartbeat.timeout: 8000
    heartbeat.interval: 5000
    taskmanager.memory.process.size: 1700m
    jobmanager.memory.process.size: 1600m
    # classloader.resolve-order: "parent-first"
    state.backend: filesystem
    
  base-config: |-
    kafka {
      broker-servers = "kafka:9092"
      producer.broker-servers = "kafka:9092"
      consumer.broker-servers = "kafka:9092"
      zookeeper = "zookeeper:2181"
      producer {
        max-request-size = 1572864
        batch.size = 98304
        linger.ms = 10
        compression = "snappy"
      }
      output.system.event.topic = ".knowlg.system.events"
    }
    job {
      env = ""
      enable.distributed.checkpointing = true
      # No valid checkpoint_store_type configured, skipping statebackend.
    }
      
    task {
      parallelism = 1
      consumer.parallelism = 1
      checkpointing.compressed = true
      checkpointing.interval = 60000
      checkpointing.pause.between.seconds = 3000
      restart-strategy.attempts = 3
      restart-strategy.delay = 30000 # in milli-seconds
    }
    redis {
    host = "redis-master"
    port = "6379"
    connection {
      max = 2
      idle.min = 1
      idle.max = 2
      minEvictableIdleTimeSeconds = 120
      timeBetweenEvictionRunsSeconds = 300
        }
    }
    lms-cassandra {
    host = "cassandra"
    port = "9042"
    }
    
    neo4j {
    routePath = "bolt://neo4j:7687"
    graph = "domain"
    }
    
    es {
    basePath = "elasticsearch:9200"
    }
    
  log4j_console_properties: |-
    # This affects logging for both user code and Flink
    rootLogger.level = INFO
    rootLogger.appenderRef.console.ref = ConsoleAppender
    
    # Uncomment this if you want to _only_ change Flink's logging
    #logger.flink.name = org.apache.flink
    #logger.flink.level = INFO
    
    # The following lines keep the log level of common libraries/connectors on
    # log level INFO. The root logger does not override this. You have to manually
    # change the log levels here.
    logger.akka.name = akka
    logger.akka.level = ERROR
    logger.kafka.name= org.apache.kafka
    logger.kafka.level = ERROR
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = ERROR
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = ERROR
    
    # Log all infos to the console
    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    
    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF
---
# Source: knowledgebb/charts/flink/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: post-publish-processor-config
  labels:

    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:

    reloader.stakater.com/auto: "true"
data:
  config: |-
    include file("/data/flink/conf/base-config.conf")
    job {
      env = ""
    }
    
    kafka {
      input.topic = ".knowlg.content.postpublish.request"
      groupId = "-post-publish-processor-group"
      publish.topic = ".learning.job.request"
      qrimage.topic = ".knowlg.qrimage.request"
      dialcode.context.topic = ".knowlg.dialcode.context.job.request"
    }
    
    task {
      consumer.parallelism = 1
      router.parallelism = 1
      shallow_copy.parallelism = 1
      link_dialcode.parallelism = 1
      batch_create.parallelism = 1
      dialcode_context_updater.parallelism = 1
    }
    
    lms-cassandra {
      keyspace = "sunbird_courses"
      batchTable = "course_batch"
    }
    
    dialcode-cassandra {
      keyspace = "dialcodes"
      imageTable = "dialcode_images"
    }
    
    service {
      search.basePath = "http://search-service:9000"
      lms.basePath = "http://lms-service:9000"
      learning_service.basePath = "http://learning:8080/learning-service"
      dial.basePath = "https://dial-service:9000"
      content.basePath = "http://content-service:9000"
    }
    
    dialcode {
      linkable.primaryCategory = ["Course"]
    }
    
    cloudstorage.metadata.replace_absolute_path=true
    cloudstorage.relative_path_prefix= "CONTENT_STORAGE_BASE_PATH"
    cloudstorage.read_base_path="https://"
    cloudstorage.write_base_path=["https://"]
    cloudstorage.metadata.list=["appIcon","posterImage","artifactUrl","downloadUrl","variants","previewUrl","pdfUrl", "streamingUrl", "toc_url"]
    
    cloud_storage_type=""
    cloud_storage_key=""
    cloud_storage_secret=""
    cloud_storage_container=""
    cloud_storage_endpoint=""
    
    
    
  flink-conf: |-
    jobmanager.memory.flink.size: 1024m
    taskmanager.memory.flink.size: 1024m
    taskmanager.numberOfTaskSlots: 1
    jobManager.numberOfTaskSlots: 1
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    scheduler-mode: reactive
    heartbeat.timeout: 8000
    heartbeat.interval: 5000
    taskmanager.memory.process.size: 1700m
    jobmanager.memory.process.size: 1600m
    # classloader.resolve-order: "parent-first"
    state.backend: filesystem
    
  base-config: |-
    kafka {
      broker-servers = "kafka:9092"
      producer.broker-servers = "kafka:9092"
      consumer.broker-servers = "kafka:9092"
      zookeeper = "zookeeper:2181"
      producer {
        max-request-size = 1572864
        batch.size = 98304
        linger.ms = 10
        compression = "snappy"
      }
      output.system.event.topic = ".knowlg.system.events"
    }
    job {
      env = ""
      enable.distributed.checkpointing = true
      # No valid checkpoint_store_type configured, skipping statebackend.
    }
      
    task {
      parallelism = 1
      consumer.parallelism = 1
      checkpointing.compressed = true
      checkpointing.interval = 60000
      checkpointing.pause.between.seconds = 3000
      restart-strategy.attempts = 3
      restart-strategy.delay = 30000 # in milli-seconds
    }
    redis {
    host = "redis-master"
    port = "6379"
    connection {
      max = 2
      idle.min = 1
      idle.max = 2
      minEvictableIdleTimeSeconds = 120
      timeBetweenEvictionRunsSeconds = 300
        }
    }
    lms-cassandra {
    host = "cassandra"
    port = "9042"
    }
    
    neo4j {
    routePath = "bolt://neo4j:7687"
    graph = "domain"
    }
    
    es {
    basePath = "elasticsearch:9200"
    }
    
  log4j_console_properties: |-
    # This affects logging for both user code and Flink
    rootLogger.level = INFO
    rootLogger.appenderRef.console.ref = ConsoleAppender
    
    # Uncomment this if you want to _only_ change Flink's logging
    #logger.flink.name = org.apache.flink
    #logger.flink.level = INFO
    
    # The following lines keep the log level of common libraries/connectors on
    # log level INFO. The root logger does not override this. You have to manually
    # change the log levels here.
    logger.akka.name = akka
    logger.akka.level = ERROR
    logger.kafka.name= org.apache.kafka
    logger.kafka.level = ERROR
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = ERROR
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = ERROR
    
    # Log all infos to the console
    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    
    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF
---
# Source: knowledgebb/charts/flink/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: qrcode-image-generator-config
  labels:

    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:

    reloader.stakater.com/auto: "true"
data:
  config: |-
    include file("/data/flink/conf/base-config.conf")
    job {
      env = ""
    }
    
    kafka {
      input.topic = ".knowlg.qrimage.request"
      groupId = "-qrcode-image-generator-group"
    }
    
    task {
      consumer.parallelism = 1
      parallelism = 1
      window.time = 60
    }
    
    lp.tmp.file.location="/tmp"
    
    qr.image {
        imageFormat="png"
        bottomMargin=0
        margin=1
    }
    
    lms-cassandra {
      keyspace = "dialcodes"
      table {
        image = "dialcode_images"
        batch = "dialcode_batch"
      }
    }
    
    # Default value is 120
    max_allowed_character_for_file_name = 120
    
    cloudstorage.metadata.replace_absolute_path=true
    cloudstorage.relative_path_prefix="DIAL_STORAGE_BASE_PATH"
    cloudstorage.read_base_path="https://"
    cloudstorage.write_base_path=["https://"]
    cloudstorage.metadata.list=["appIcon","posterImage","artifactUrl","downloadUrl","variants","previewUrl","pdfUrl", "streamingUrl", "toc_url"]
    
    cloud_storage_type=""
    cloud_storage_key=""
    cloud_storage_secret=""
    cloud_storage_container=""
    cloud_storage_endpoint=""
    
    
    
  flink-conf: |-
    jobmanager.memory.flink.size: 1024m
    taskmanager.memory.flink.size: 1024m
    taskmanager.numberOfTaskSlots: 1
    jobManager.numberOfTaskSlots: 1
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    scheduler-mode: reactive
    heartbeat.timeout: 8000
    heartbeat.interval: 5000
    taskmanager.memory.process.size: 1700m
    jobmanager.memory.process.size: 1600m
    # classloader.resolve-order: "parent-first"
    state.backend: filesystem
    
  base-config: |-
    kafka {
      broker-servers = "kafka:9092"
      producer.broker-servers = "kafka:9092"
      consumer.broker-servers = "kafka:9092"
      zookeeper = "zookeeper:2181"
      producer {
        max-request-size = 1572864
        batch.size = 98304
        linger.ms = 10
        compression = "snappy"
      }
      output.system.event.topic = ".knowlg.system.events"
    }
    job {
      env = ""
      enable.distributed.checkpointing = true
      # No valid checkpoint_store_type configured, skipping statebackend.
    }
      
    task {
      parallelism = 1
      consumer.parallelism = 1
      checkpointing.compressed = true
      checkpointing.interval = 60000
      checkpointing.pause.between.seconds = 3000
      restart-strategy.attempts = 3
      restart-strategy.delay = 30000 # in milli-seconds
    }
    redis {
    host = "redis-master"
    port = "6379"
    connection {
      max = 2
      idle.min = 1
      idle.max = 2
      minEvictableIdleTimeSeconds = 120
      timeBetweenEvictionRunsSeconds = 300
        }
    }
    lms-cassandra {
    host = "cassandra"
    port = "9042"
    }
    
    neo4j {
    routePath = "bolt://neo4j:7687"
    graph = "domain"
    }
    
    es {
    basePath = "elasticsearch:9200"
    }
    
  log4j_console_properties: |-
    # This affects logging for both user code and Flink
    rootLogger.level = INFO
    rootLogger.appenderRef.console.ref = ConsoleAppender
    
    # Uncomment this if you want to _only_ change Flink's logging
    #logger.flink.name = org.apache.flink
    #logger.flink.level = INFO
    
    # The following lines keep the log level of common libraries/connectors on
    # log level INFO. The root logger does not override this. You have to manually
    # change the log levels here.
    logger.akka.name = akka
    logger.akka.level = ERROR
    logger.kafka.name= org.apache.kafka
    logger.kafka.level = ERROR
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = ERROR
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = ERROR
    
    # Log all infos to the console
    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    
    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF
---
# Source: knowledgebb/charts/flink/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: search-indexer-config
  labels:

    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:

    reloader.stakater.com/auto: "true"
data:
  config: |-
    include file("/data/flink/conf/base-config.conf")
    job {
      env = ""
    }
    kafka {
      event.max.size = "1048576" # Max is only 1MB
      input.topic = ".knowlg.learning.graph.events"
      error.topic = ".knowlg.learning.events.failed"
      groupId = "-search-indexer-group"
      producer {
        max-request-size = 5242880
      }
    }
    task {
      consumer.parallelism = 1
      router.parallelism = 1
      compositeSearch.parallelism = 1
      dialcodeIndexer.parallelism = 1
      dialcodemetricsIndexer.parallelism = 1
    }
    compositesearch.index.name = "compositesearch"
    nested.fields = ["badgeAssertions", "targets", "badgeAssociations", "plugins", "me_totalTimeSpent", "me_totalPlaySessionCount", "me_totalTimeSpentInSec", "batches", "trackable", "credentials", "discussionForum", "provider", "osMetadata", "actions"]
    schema.definition_cache.expiry = 14400
    restrict {
      metadata.objectTypes = []
      objectTypes = ["EventSet", "Questionnaire", "Misconception", "FrameworkType", "EventSet", "Event"]
    }
    cloudstorage.metadata.replace_absolute_path=true
    cloudstorage.relative_path_prefix= "CONTENT_STORAGE_BASE_PATH"
    cloudstorage.read_base_path="https://"
    cloudstorage.write_base_path=["https://"] 
    cloudstorage.mecloudstorage.metadata.list=["appIcon","posterImage","artifactUrl","downloadUrl","variants","previewUrl","pdfUrl", "streamingUrl", "toc_url"]
    cloud_storage_container=""
    
    
  flink-conf: |-
    jobmanager.memory.flink.size: 1024m
    taskmanager.memory.flink.size: 1024m
    taskmanager.numberOfTaskSlots: 1
    jobManager.numberOfTaskSlots: 1
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    scheduler-mode: reactive
    heartbeat.timeout: 8000
    heartbeat.interval: 5000
    taskmanager.memory.process.size: 1700m
    jobmanager.memory.process.size: 1600m
    # classloader.resolve-order: "parent-first"
    state.backend: filesystem
    
  base-config: |-
    kafka {
      broker-servers = "kafka:9092"
      producer.broker-servers = "kafka:9092"
      consumer.broker-servers = "kafka:9092"
      zookeeper = "zookeeper:2181"
      producer {
        max-request-size = 1572864
        batch.size = 98304
        linger.ms = 10
        compression = "snappy"
      }
      output.system.event.topic = ".knowlg.system.events"
    }
    job {
      env = ""
      enable.distributed.checkpointing = true
      # No valid checkpoint_store_type configured, skipping statebackend.
    }
      
    task {
      parallelism = 1
      consumer.parallelism = 1
      checkpointing.compressed = true
      checkpointing.interval = 60000
      checkpointing.pause.between.seconds = 3000
      restart-strategy.attempts = 3
      restart-strategy.delay = 30000 # in milli-seconds
    }
    redis {
    host = "redis-master"
    port = "6379"
    connection {
      max = 2
      idle.min = 1
      idle.max = 2
      minEvictableIdleTimeSeconds = 120
      timeBetweenEvictionRunsSeconds = 300
        }
    }
    lms-cassandra {
    host = "cassandra"
    port = "9042"
    }
    
    neo4j {
    routePath = "bolt://neo4j:7687"
    graph = "domain"
    }
    
    es {
    basePath = "elasticsearch:9200"
    }
    
  log4j_console_properties: |-
    # This affects logging for both user code and Flink
    rootLogger.level = INFO
    rootLogger.appenderRef.console.ref = ConsoleAppender
    
    # Uncomment this if you want to _only_ change Flink's logging
    #logger.flink.name = org.apache.flink
    #logger.flink.level = INFO
    
    # The following lines keep the log level of common libraries/connectors on
    # log level INFO. The root logger does not override this. You have to manually
    # change the log levels here.
    logger.akka.name = akka
    logger.akka.level = ERROR
    logger.kafka.name= org.apache.kafka
    logger.kafka.level = ERROR
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = ERROR
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = ERROR
    
    # Log all infos to the console
    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    
    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF
---
# Source: knowledgebb/charts/flink/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: video-stream-generator-config
  labels:

    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:

    reloader.stakater.com/auto: "true"
data:
  config: |-
    include file("/data/flink/conf/base-config.conf")
    job {
      env = ""
    }
    
    kafka {
      input.topic = ".knowlg.content.postpublish.request"
      groupId = "-video-stream-generator-group"
    }
    
    task {
      consumer.parallelism = 1
      parallelism = 1
      timer.duration = 60
      max.retries = 10
    }
    
    lms-cassandra {
      keyspace = "_platform_db"
      table = "job_request"
    }
    
    service {
      content {
        basePath = "https://content:9000"
      }
    }
    
    # Azure Media Service Config
    azure {
      location = "centralindia"
      tenant = "tenant"
      subscription_id = "subscription id "
    
      login {
        endpoint="https://login.microsoftonline.com"
      }
    
      api {
        endpoint="https://management.azure.com"
        version = "2018-07-01"
      }
    
      account_name = "account name"
      resource_group_name = "group name"
    
      transform {
        default = "media_transform_default"
        hls = "media_transform_hls"
      }
    
      stream {
        base_url = "https://sunbirdspikemedia-inct.streaming.media.azure.net"
        endpoint_name = "default"
        protocol = "Hls"
        policy_name = "Predefined_ClearStreamingOnly"
      }
    
      token {
        client_key = "client key"
        client_secret = "client secret"
      }
    }
    
    azure_tenant="tenant"
    azure_subscription_id="subscription id"
    azure_account_name=""
    azure_resource_group_name="group name"
    azure_token_client_key="client key"
    azure_token_client_secret="client secret"
    
    # CSP Name. e.g: aws or azure
    media_service_type="aws"
    
    #AWS Elemental Media Convert Config
    aws {
      region="ap-south-1"
      content_bucket_name="awsmedia-spike"
      token {
        access_key="access key"
        access_secret="access secret"
      }
      api {
        endpoint="API Endpoint for media convert"
        version="2017-08-29"
      }
      service {
        name="mediaconvert"
        queue="Media Convert Queue Id"
        role="Media Convert Role Name"
      }
      stream {
        protocol="Hls"
      }
    }
    
    media_service_job_success_status=["FINISHED", "COMPLETE"]
    
    
  flink-conf: |-
    jobmanager.memory.flink.size: 1024m
    taskmanager.memory.flink.size: 1024m
    taskmanager.numberOfTaskSlots: 1
    jobManager.numberOfTaskSlots: 1
    parallelism.default: 1
    jobmanager.execution.failover-strategy: region
    taskmanager.memory.network.fraction: 0.1
    scheduler-mode: reactive
    heartbeat.timeout: 8000
    heartbeat.interval: 5000
    taskmanager.memory.process.size: 1700m
    jobmanager.memory.process.size: 1600m
    # classloader.resolve-order: "parent-first"
    state.backend: filesystem
    
  base-config: |-
    kafka {
      broker-servers = "kafka:9092"
      producer.broker-servers = "kafka:9092"
      consumer.broker-servers = "kafka:9092"
      zookeeper = "zookeeper:2181"
      producer {
        max-request-size = 1572864
        batch.size = 98304
        linger.ms = 10
        compression = "snappy"
      }
      output.system.event.topic = ".knowlg.system.events"
    }
    job {
      env = ""
      enable.distributed.checkpointing = true
      # No valid checkpoint_store_type configured, skipping statebackend.
    }
      
    task {
      parallelism = 1
      consumer.parallelism = 1
      checkpointing.compressed = true
      checkpointing.interval = 60000
      checkpointing.pause.between.seconds = 3000
      restart-strategy.attempts = 3
      restart-strategy.delay = 30000 # in milli-seconds
    }
    redis {
    host = "redis-master"
    port = "6379"
    connection {
      max = 2
      idle.min = 1
      idle.max = 2
      minEvictableIdleTimeSeconds = 120
      timeBetweenEvictionRunsSeconds = 300
        }
    }
    lms-cassandra {
    host = "cassandra"
    port = "9042"
    }
    
    neo4j {
    routePath = "bolt://neo4j:7687"
    graph = "domain"
    }
    
    es {
    basePath = "elasticsearch:9200"
    }
    
  log4j_console_properties: |-
    # This affects logging for both user code and Flink
    rootLogger.level = INFO
    rootLogger.appenderRef.console.ref = ConsoleAppender
    
    # Uncomment this if you want to _only_ change Flink's logging
    #logger.flink.name = org.apache.flink
    #logger.flink.level = INFO
    
    # The following lines keep the log level of common libraries/connectors on
    # log level INFO. The root logger does not override this. You have to manually
    # change the log levels here.
    logger.akka.name = akka
    logger.akka.level = ERROR
    logger.kafka.name= org.apache.kafka
    logger.kafka.level = ERROR
    logger.hadoop.name = org.apache.hadoop
    logger.hadoop.level = ERROR
    logger.zookeeper.name = org.apache.zookeeper
    logger.zookeeper.level = ERROR
    
    # Log all infos to the console
    appender.console.name = ConsoleAppender
    appender.console.type = CONSOLE
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n
    
    # Suppress the irrelevant (wrong) warnings from the Netty channel handler
    logger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline
    logger.netty.level = OFF
---
# Source: knowledgebb/charts/kafka/charts/zookeeper/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: zookeeper-kbb-scripts
  namespace: sunbird
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-11.0.2
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.8.0"
    app.kubernetes.io/component: zookeeper
data:
  init-certs.sh: |-
    #!/bin/bash
  setup.sh: |-
    #!/bin/bash

    # Execute entrypoint as usual after obtaining ZOO_SERVER_ID
    # check ZOO_SERVER_ID in persistent volume via myid
    # if not present, set based on POD hostname
    if [[ -f "/bitnami/zookeeper/data/myid" ]]; then
        export ZOO_SERVER_ID="$(cat /bitnami/zookeeper/data/myid)"
    else
        HOSTNAME="$(hostname -s)"
        if [[ $HOSTNAME =~ (.*)-([0-9]+)$ ]]; then
            ORD=${BASH_REMATCH[2]}
            export ZOO_SERVER_ID="$((ORD + 1 ))"
        else
            echo "Failed to get index from hostname $HOST"
            exit 1
        fi
    fi
    exec /entrypoint.sh /run.sh
---
# Source: knowledgebb/charts/kafka/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-kbb-scripts
  namespace: "sunbird"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-20.0.2
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.3.1"
  annotations:
    helm.sh/hook-weight: "-5"
data:
  setup.sh: |-
    #!/bin/bash

    ID="${MY_POD_NAME#"kafka-kbb-"}"
    if [[ -f "/bitnami/kafka/data/meta.properties" ]]; then
        export KAFKA_CFG_BROKER_ID="$(grep "broker.id" "/bitnami/kafka/data/meta.properties" | awk -F '=' '{print $2}')"
    else
        export KAFKA_CFG_BROKER_ID="$((ID + 0))"
    fi

    # Configure zookeeper client

    exec /entrypoint.sh /run.sh
---
# Source: knowledgebb/charts/learning/templates/learning-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: learning-config
  namespace: sunbird
data:
  learning-service_application.conf: |
    #Platform Configuration
    platform.services=["learning", "language", "config"]

    # Learning-Service Configuration
    learning.graph_ids=["domain"]
    content.metadata.visibility.parent=["textbookunit", "courseunit", "lessonplanunit"]

    # Language-Service Configuration
    language.graph_ids=["as","bn","en","gu","hi","hoc","jun","ka","mai","mr","unx","or","san","sat","ta","te","urd","pj"]

    # Redis Configuration
    redis.host="redis-master"
    redis.port=6379
    redis.maxConnections=128

    # Cassandra Configuration
    content.keyspace.name="_content_store"
    hierarchy.keyspace.name="_hierarchy_store"
    content.hierarchy.table="content_hierarchy"
    framework.hierarchy.table="framework_hierarchy"

    dialcode.keyspace.name="dialcode_store"
    dialcode.keyspace.table="dial_code"
    dialcode.max_count=1000

    system.config.keyspace.name="dialcode_store"
    system.config.table="dialcode_store"
    publisher.keyspace.name="system_config"
    publisher.keyspace.table="publisher"

    #DIAL Code Generator Configuration
    dialcode.strip.chars="0"
    dialcode.length=6.0
    dialcode.large.prime_number=1679979167

    dialcode.es_conn_info="elasticsearch:9200"
    dialcode.search.limit=1000

    #DIAL Code ElasticSearch Configuration
    dialcode_store.dial_code.index=true
    dialcode_store.dial_code.object_type="DialCode"

    audit.es_conn_info="elasticsearch:9200"

    #Assessment Item Configuration
    assessment.keyspace.name="_content_store"
    assessment.keyspace.table="question_data"

    # Actor System Configuration
    LearningActorSystem {
      akka {
        actor {
          default-dispatcher {
              type = "Dispatcher"
            executor = "fork-join-executor"
            fork-join-executor {
                parallelism-min = 1
                parallelism-factor = 2.0
                parallelism-max = 4
            }
              # Throughput for default Dispatcher, set to 1 for as fair as possible
              throughput = 1
          }
          deployment {
            /HealthCheckManager
            {
              router = smallest-mailbox-pool
                      nr-of-instances = 5
            }
          }
        }
      }
    }

    env= ""

    #Current environment
    cloud_storage.env = ""

    #Folder configuration
    cloud_storage.content.folder = "content"
    cloud_storage.asset.folder = "assets"
    cloud_storage.artefact.folder = "artifact"
    cloud_storage.bundle.folder = "bundle"
    cloud_storage.media.folder = "media"
    cloud_storage.ecar.folder = "ecar_files"
    cloud_storage.itemset.folder = "itemset"

    cloud_storage.upload.url.ttl = 600


    # Media download configuration
    content.media.base.url=" "
    plugin.media.base.url=" "


    # Content Extraction Configuration

    #directory location where store unzip file
    dist.directory = "/data/tmp/dist/"
    output.zipfile = "/data/tmp/story.zip"
    source.folder  = "/data/tmp/temp2/"
    save.directory = "/data/tmp/temp/"

    # FOR CONTENT WORKFLOW PIPELINE (CWP)

    #--Content Workflow Pipeline Mode
    OPERATION_MODE = "TEST"

    #--Maximum Content Package File Size Limit in Bytes (50 MB)
    MAX_CONTENT_PACKAGE_FILE_SIZE_LIMIT = 52428800

    #--Maximum Asset File Size Limit in Bytes (20 MB - 20971520)
    MAX_ASSET_FILE_SIZE_LIMIT = 52428800

    #--No of Retry While File Download Fails
    RETRY_ASSET_DOWNLOAD_COUNT = 1

    # H5P Library Path
    content.h5p.library.path="learning_content_h5p_library_path"

    # ElasticSearch Configuration
    search.es_conn_info="elasticsearch:9200"
    search.fields.query=["name^100","title^100","lemma^100","code^100","domain","subject","description^10","keywords^100","ageGroup^10","filter^10","theme^10","genre^10","objects^25","contentType^100","language^200","teachingMode^25","skills^10","learningObjective^10","curriculum^100","gradeLevel^100","developer^100","attributions^10","identifier^100","IL_UNIQUE_ID^100","owner^50","board^100", "creator^100", "dialcodes^100","text","words","releaseNotes"]
    search.fields.date=["lastUpdatedOn","createdOn","versionDate","lastSubmittedOn","lastPublishedOn"]
    search.batch.size=500
    search.connection.timeout=30

    platform-api-url="http://learning:8080/learning-service"

    # Language Index Configuration
    ignoreStartWordsList=["<Sentence","id=","<fs","head=","case_name=","paradigm=","name=","inf="]
    tagNamesList=["NN","NST","PRP","DEM","VM","VAUX","JJ","RB","PSP","RP","CC","WQ","QF","QC","QO","CL","INTF","INJ","NEG","*C","RDP","ECH","UNK","NP","VGF","VGNF","VGINF","VGNN","JJP","RBP","NEGP","CCP","FRAGP","BLK"]
    discardTokensList=["NNP","((","))","SYM"]
    attributesTagIdentifier=af
    specialCharRegEx="^([$&+,:;=?@#|!]*)$"
    numberRegEx="^([+-]?\\d*\\.?\\d*)$"
    defaultTokenCountAfterWord=10

    # Neo4j Graph Configuration
    graph.dir="/data/graphDB"
    akka.request_timeout=30
    environment.id="20000000"
    graph.passport.key.base="graph_passport_key "
    route.domain="bolt://neo4j:7687"
    route.bolt.write.domain="bolt://neo4j:7687"
    route.bolt.read.domain="bolt://neo4j:7687"
    route.all="bolt://neo4j:8687"
    route.bolt.write.all="bolt://neo4j:8687"
    route.bolt.read.all="bolt://neo4j:8687"
    shard.id="1"
    platform.auth.check.enabled=false
    platform.cache.ttl=3600000
    platform.object.bookmark.expiry="15"

    language.map={"Hindi":"hi", "English":"en", "Telugu":"te", "Kannada":"ka", "Tamil":"ta", "Assamese":"as", "Bengali":"bn", "Bodo":"bo", "Gujarati":"gu", "Konkani":"ko", "Malayalam":"ml", "Marathi":"mr", "Nepali":"ne", "Odia":"or", "Punjabi":"pj", "Sanskrit":"san"}

    framework.max_term_creation_limit=200

    # Enable Suggested Framework in Get Channel API.
    channel.fetch.suggested_frameworks=true

    # Kafka configuration details
    kafka.topics.instruction=".knowlg.learning.job.request"
    kafka.urls="kafka:9092"
    kafka.publish.request.topic=".knowlg.publish.job.request"
    kafka.topic.system.command=".system.command"
    job.request.event.mimetype=["application/pdf",
                                  "application/vnd.ekstep.ecml-archive",
                                  "application/vnd.ekstep.html-archive",
                                  "application/vnd.android.package-archive",
                                  "application/vnd.ekstep.content-archive",
                                  "application/epub",
                                  "application/msword",
                                  "application/vnd.ekstep.h5p-archive",
                                  "video/webm",
                                  "video/mp4",
                                  "application/vnd.ekstep.content-collection",
                                  "video/quicktime",
                                  "application/octet-stream",
                                  "application/json",
                                  "application/javascript",
                                  "application/xml",
                                  "text/plain",
                                  "text/html",
                                  "text/javascript",
                                  "text/xml",
                                  "text/css",
                                  "image/jpeg",
                                  "image/jpg",
                                  "image/png",
                                  "image/tiff",
                                  "image/bmp",
                                  "image/gif",
                                  "image/svg+xml",
                                  "image/x-quicktime",
                                  "video/avi",
                                  "video/mpeg",
                                  "video/quicktime",
                                  "video/3gpp",
                                  "video/mp4",
                                  "video/ogg",
                                  "video/webm",
                                  "video/msvideo",
                                  "video/x-msvideo",
                                  "video/x-qtc",
                                  "video/x-mpeg",
                                  "audio/mp3",
                                  "audio/mp4",
                                  "audio/mpeg",
                                  "audio/ogg",
                                  "audio/webm",
                                  "audio/x-wav",
                                  "audio/wav",
                                  "audio/mpeg3",
                                  "audio/x-mpeg-3",
                                  "audio/vorbis",
                                  "application/x-font-ttf",
                                  "application/vnd.ekstep.plugin-archive",
                                  "video/x-youtube",
                                  "video/youtube",
                                  "text/x-url"]

    #Youtube Standard Licence Validation
    learning.content.youtube.validate.license=true
    learning.content.youtube.application.name="fetch-youtube-license"
    learning_content_youtube_apikey=""

    #Telemetry pdata ID
    telemetry_env="sunbird"
    telemetry.pdata_id="sunbird.ekstep.learning.platform"
    telemetry.search.topn=5

    #Copy Content validation configuration
    learning.content.copy.invalid_status_list=["Flagged","FlagDraft","FlagReview","Retired", "Processing"]
    learning.content.copy.props_to_remove=["downloadUrl", "artifactUrl", "variants","createdOn", "collections", "children",
    "lastUpdatedOn", "SYS_INTERNAL_LAST_UPDATED_ON","versionKey", "s3Key", "status", "pkgVersion", "toc_url",
    "mimeTypesCount", "contentTypesCount", "leafNodesCount", "childNodes", "prevState", "lastPublishedOn", "flagReasons",
    "compatibilityLevel", "size", "publishChecklist", "publishComment", "lastPublishedBy", "rejectReasons", "rejectComment",
    "gradeLevel", "subject", "medium", "board", "topic", "purpose", "subtopic", "contentCredits", "owner",
    "collaborators", "creators", "contributors", "badgeAssertions", "dialcodes", "concepts", "keywords", "reservedDialcodes", "dialcodeRequired", "leafNodes"]

    # Metadata to be added to copied content from origin
    learning.content.copy.origin_data=["name", "author", "license", "organisation"]

    learning.content.type.not.copied.list=["Asset"]

    #Youtube License Validation Regex Pattern
    youtube.license.regex.pattern=["\\?vi?=([^&]*)", "watch\\?.*v=([^&]*)", "(?:embed|vi?)/([^/?]*)","^([A-Za-z0-9\\-\\_]*)"]

    #Cloud Storage details
    cloud_storage_type="azure"
    cloud_storage_key=""
    cloud_storage_secret=""
    cloud_storage_container="" 
    cloud_storage_endpoint=""

    installation.id="sunbird"

    # Configuration for DIALCode linking
    learning.content.link_dialcode_validation=true
    dialcode.api.search.url="http://dial-service:9000/dialcode/v3/search"
    dialcode.api.generate.url="http://dial-service:9000/dialcode/v3/generate"

    # Configuration for default channel ID
    channel.default="in.ekstep"

    learning.telemetry_ignored_props=["body","screenshots","stageIcons","editorState"]
    learning.telemetry_req_length=100

    learning.valid_license=["creativeCommon"]
    learning.service_provider=["youtube"]

    # Valid content type for dialcode reserve
    learning.reserve_dialcode.mimeType=["application/vnd.ekstep.content-collection"]

    # Max count to be reserved against a content
    learnig.reserve_dialcode.max_count=2500

    cassandra.lp.connection="cassandra:9042"
    cassandra.lpa.connection="cassandra:9042"

    compositesearch.index.name="compositesearch"

    #restrict.metadata.objectTypes="Content,ContentImage,AssessmentItem,Channel,Framework,Category,CategoryInstance,Term,Concept,Dimension,Domain"

    # Consistency Level for Multi Node Cassandra cluster
    cassandra.lp.consistency.level=LOCAL_QUORUM

    # Redis Cache Configuration
    content.cache.ttl=86400
    content.cache.read=true
    content.cache.hierarchy=true

    framework.categories_cached=["subject", "medium", "gradeLevel", "board"]
    framework.cache.ttl=86400
    framework.cache.read=true

    # Creative common license type
    content.license="CC BY 4.0"

    # Content Tagging Config
    content.tagging.backward_enable=true
    content.tagging.property="subject,medium"

    # Search Service Config
    kp.search_service.base_url="http://search-service:9000"

    # CNAME migration variables

    cloudstorage {
      metadata.replace_absolute_path=true
      relative_path_prefix="contents"
      metadata.list=["appIcon", "artifactUrl", "posterImage", "previewUrl", "thumbnail", "assetsMap", "certTemplate", "itemSetPreviewUrl", "grayScaleAppIcon", "sourceURL", "variants", "downloadUrl", "streamingUrl", "toc_url", "data", "question", "solutions", "editorState", "media", "pdfUrl", "transcripts"]
      read_base_path="https://"
      write_base_path="https://"

    }
---
# Source: knowledgebb/charts/neo4j/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: logstash-config-kbb
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: neo4j-kbb
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: neo4j-0.1.0
  annotations:
    helm.sh/hook-weight: "-5"
    reloader.stakater.com/auto: "true"
data:
  logstash.conf: |-
    input {
      file {
        start_position =>"beginning"
        type => "graph_event"
        path => ["/txn-handler/learning_graph_event_neo4j.log"]
        sincedb_path => "/usr/share/logstash/.sincedb_learning_graph_event_mw"
      }
    }
    filter {
      grok {
        match => [ "message",
                  "%{TIMESTAMP_ISO8601:timestamp} %{GREEDYDATA:msg}"]
      }
      mutate {
          gsub => [ "message","%{timestamp}","" ]
          strip => [ "message" ]
      }
      json {
          source => "message"
      }
    }
    output {
      kafka {
        bootstrap_servers => "kafka:9092"
        codec => plain {
            format => "%{message}"
        }
        message_key => "%{nodeUniqueId}"
        topic_id => ".knowlg.learning.graph.events"
        retries => 20
        retry_backoff_ms => 180000
      }
    }
---
# Source: knowledgebb/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-kbb-configuration
  namespace: "sunbird"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.8.3
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.9"
  annotations:
    helm.sh/hook-weight: "-5"
data:
  redis.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
  master.conf: |-
    dir /data
    # User-supplied master configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of master configuration
  replica.conf: |-
    dir /data
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
---
# Source: knowledgebb/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-kbb-health
  namespace: "sunbird"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.8.3
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.9"
  annotations:
    helm.sh/hook-weight: "-5"
data:
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: knowledgebb/charts/redis/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-kbb-scripts
  namespace: "sunbird"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.8.3
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.9"
  annotations:
    helm.sh/hook-weight: "-5"
data:
  start-master.sh: |
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--protected-mode" "no")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
    exec redis-server "${ARGS[@]}"
  start-replica.sh: |
    #!/bin/bash

    get_port() {
        hostname="$1"
        type="$2"

        port_var=$(echo "${hostname^^}_SERVICE_PORT_$type" | sed "s/-/_/g")
        port=${!port_var}

        if [ -z "$port" ]; then
            case $type in
                "SENTINEL")
                    echo 26379
                    ;;
                "REDIS")
                    echo 6379
                    ;;
            esac
        else
            echo $port
        fi
    }

    get_full_hostname() {
        hostname="$1"
        full_hostname="${hostname}.${HEADLESS_SERVICE}"
        echo "${full_hostname}"
    }

    REDISPORT=$(get_port "$HOSTNAME" "REDIS")

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/replica.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi

    echo "" >> /opt/bitnami/redis/etc/replica.conf
    echo "replica-announce-port $REDISPORT" >> /opt/bitnami/redis/etc/replica.conf
    echo "replica-announce-ip $(get_full_hostname "$HOSTNAME")" >> /opt/bitnami/redis/etc/replica.conf
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--replicaof" "${REDIS_MASTER_HOST}" "${REDIS_MASTER_PORT_NUMBER}")
    ARGS+=("--protected-mode" "no")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/replica.conf")
    exec redis-server "${ARGS[@]}"
---
# Source: knowledgebb/charts/search/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: search
  labels:

    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: search
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: search-0.1.0
  annotations:

    reloader.stakater.com/auto: "true"
data: # Take only root level files (configs/*) for configmaps # Skip env.yaml as configmap, as it's env file
    
  application.conf: |-
      # This is the main configuration file for the application.
      # https://www.playframework.com/documentation/latest/ConfigFile
      # ~~~~~
      # Play uses HOCON as its configuration file format.  HOCON has a number
      # of advantages over other config formats, but there are two things that
      # can be used when modifying settings.
      #
      # You can include other configuration files in this main application.conf file:
      #include "extra-config.conf"
      #
      # You can declare variables and substitute for them:
      #mykey = ${some.value}
      #
      # And if an environment variable exists when there is no other substitution, then
      # HOCON will fall back to substituting environment variable:
      #mykey = ${JAVA_HOME}
      
      ## Akka
      # https://www.playframework.com/documentation/latest/ScalaAkka#Configuration
      # https://www.playframework.com/documentation/latest/JavaAkka#Configuration
      # ~~~~~
      # Play uses Akka internally and exposes Akka Streams and actors in Websockets and
      # other streaming HTTP responses.
      akka {
        # "akka.log-config-on-start" is extraordinarly useful because it log the complete
        # configuration at INFO level, including defaults and overrides, so it s worth
        # putting at the very top.
        #
        # Put the following in your conf/logback.xml file:
        #
        # <logger name="akka.actor" level="INFO" />
        #
        # And then uncomment this line to debug the configuration.
        #
        #log-config-on-start = true
        default-dispatcher {
          # This will be used if you have set "executor = "fork-join-executor""
          fork-join-executor {
            # Min number of threads to cap factor-based parallelism number to
            parallelism-min = 8
            # The parallelism factor is used to determine thread pool size using the
            # following formula: ceil(available processors * factor). Resulting size
            # is then bounded by the parallelism-min and parallelism-max values.
            parallelism-factor = 32.0
            # Max number of threads to cap factor-based parallelism number to
            parallelism-max = 64
            # Setting to "FIFO" to use queue like peeking mode which "poll" or "LIFO" to use stack
            # like peeking mode which "pop".
            task-peeking-mode = "FIFO"
          }
        }
        actors-dispatcher {
          type = "Dispatcher"
          executor = "fork-join-executor"
          fork-join-executor {
            parallelism-min = 8
            parallelism-factor = 32.0
            parallelism-max = 64
          }
          # Throughput for default Dispatcher, set to 1 for as fair as possible
          throughput = 1
        }
        actor {
          deployment {
            /searchActor
              {
                router = smallest-mailbox-pool
                nr-of-instances = 10
                dispatcher = actors-dispatcher
              }
            /healthActor
              {
                router = smallest-mailbox-pool
                nr-of-instances = 10
                dispatcher = actors-dispatcher
              }
          }
        }
      }
      
      ## Secret key
      # http://www.playframework.com/documentation/latest/ApplicationSecret
      # ~~~~~
      # The secret key is used to sign Play's session cookie.
      # This must be changed for production, but we don't recommend you change it in this file.
      play.http.secret.key = "secretKey"
      
      ## Modules
      # https://www.playframework.com/documentation/latest/Modules
      # ~~~~~
      # Control which modules are loaded when Play starts.  Note that modules are
      # the replacement for "GlobalSettings", which are deprecated in 2.5.x.
      # Please see https://www.playframework.com/documentation/latest/GlobalSettings
      # for more information.
      #
      # You can also extend Play functionality by using one of the publically available
      # Play modules: https://playframework.com/documentation/latest/ModuleDirectory
      play.modules {
        # By default, Play will load any class called Module that is defined
        # in the root package (the "app" directory), or you can define them
        # explicitly below.
        # If there are any built-in modules that you want to enable, you can list them here.
        #enabled += my.application.Module
      
        # If there are any built-in modules that you want to disable, you can list them here.
        #disabled += ""
        enabled += modules.SearchModule
      }
      
      ## IDE
      # https://www.playframework.com/documentation/latest/IDE
      # ~~~~~
      # Depending on your IDE, you can add a hyperlink for errors that will jump you
      # directly to the code location in the IDE in dev mode. The following line makes
      # use of the IntelliJ IDEA REST interface:
      #play.editor="http://localhost:63342/api/file/?file=%s&line=%s"
      
      ## Internationalisation
      # https://www.playframework.com/documentation/latest/JavaI18N
      # https://www.playframework.com/documentation/latest/ScalaI18N
      # ~~~~~
      # Play comes with its own i18n settings, which allow the user's preferred language
      # to map through to internal messages, or allow the language to be stored in a cookie.
      play.i18n {
        # The application languages
        langs = [ "en" ]
      
        # Whether the language cookie should be secure or not
        #langCookieSecure = true
      
        # Whether the HTTP only attribute of the cookie should be set to true
        #langCookieHttpOnly = true
      }
      
      ## Play HTTP settings
      # ~~~~~
      play.http {
        ## Router
        # https://www.playframework.com/documentation/latest/JavaRouting
        # https://www.playframework.com/documentation/latest/ScalaRouting
        # ~~~~~
        # Define the Router object to use for this application.
        # This router will be looked up first when the application is starting up,
        # so make sure this is the entry point.
        # Furthermore, it's assumed your route file is named properly.
        # So for an application router like `my.application.Router`,
        # you may need to define a router file `conf/my.application.routes`.
        # Default to Routes in the root package (aka "apps" folder) (and conf/routes)
        #router = my.application.Router
      
        ## Action Creator
        # https://www.playframework.com/documentation/latest/JavaActionCreator
        # ~~~~~
        #actionCreator = null
      
        ## ErrorHandler
        # https://www.playframework.com/documentation/latest/JavaRouting
        # https://www.playframework.com/documentation/latest/ScalaRouting
        # ~~~~~
        # If null, will attempt to load a class called ErrorHandler in the root package,
        #errorHandler = null
      
        ## Session & Flash
        # https://www.playframework.com/documentation/latest/JavaSessionFlash
        # https://www.playframework.com/documentation/latest/ScalaSessionFlash
        # ~~~~~
        session {
          # Sets the cookie to be sent only over HTTPS.
          #secure = true
      
          # Sets the cookie to be accessed only by the server.
          #httpOnly = true
      
          # Sets the max-age field of the cookie to 5 minutes.
          # NOTE: this only sets when the browser will discard the cookie. Play will consider any
          # cookie value with a valid signature to be a valid session forever. To implement a server side session timeout,
          # you need to put a timestamp in the session and check it at regular intervals to possibly expire it.
          #maxAge = 300
      
          # Sets the domain on the session cookie.
          #domain = "example.com"
        }
      
        flash {
          # Sets the cookie to be sent only over HTTPS.
          #secure = true
      
          # Sets the cookie to be accessed only by the server.
          #httpOnly = true
        }
      }
      
      play.http.parser.maxDiskBuffer = 10MB
      parsers.anyContent.maxLength = 10MB
      
      play.server.provider = play.core.server.NettyServerProvider
      
      ## Netty Provider
      # https://www.playframework.com/documentation/latest/SettingsNetty
      # ~~~~~
      play.server.netty {
        # Whether the Netty wire should be logged
        log.wire = true
      
        # If you run Play on Linux, you can use Netty's native socket transport
        # for higher performance with less garbage.
        transport = "jdk"
      }
      
      ## WS (HTTP Client)
      # https://www.playframework.com/documentation/latest/ScalaWS#Configuring-WS
      # ~~~~~
      # The HTTP client primarily used for REST APIs.  The default client can be
      # configured directly, but you can also create different client instances
      # with customized settings. You must enable this by adding to build.sbt:
      #
      # libraryDependencies += ws // or javaWs if using java
      #
      play.ws {
        # Sets HTTP requests not to follow 302 requests
        #followRedirects = false
      
        # Sets the maximum number of open HTTP connections for the client.
        #ahc.maxConnectionsTotal = 50
      
        ## WS SSL
        # https://www.playframework.com/documentation/latest/WsSSL
        # ~~~~~
        ssl {
          # Configuring HTTPS with Play WS does not require programming.  You can
          # set up both trustManager and keyManager for mutual authentication, and
          # turn on JSSE debugging in development with a reload.
          #debug.handshake = true
          #trustManager = {
          #  stores = [
          #    { type = "JKS", path = "exampletrust.jks" }
          #  ]
          #}
        }
      }
      
      ## Cache
      # https://www.playframework.com/documentation/latest/JavaCache
      # https://www.playframework.com/documentation/latest/ScalaCache
      # ~~~~~
      # Play comes with an integrated cache API that can reduce the operational
      # overhead of repeated requests. You must enable this by adding to build.sbt:
      #
      # libraryDependencies += cache
      #
      play.cache {
        # If you want to bind several caches, you can bind the individually
        #bindCaches = ["db-cache", "user-cache", "session-cache"]
      }
      
      ## Filter Configuration
      # https://www.playframework.com/documentation/latest/Filters
      # ~~~~~
      # There are a number of built-in filters that can be enabled and configured
      # to give Play greater security.
      #
      play.filters {
      
        # Enabled filters are run automatically against Play.
        # CSRFFilter, AllowedHostFilters, and SecurityHeadersFilters are enabled by default.
        enabled = [filters.AccessLogFilter]
      
        # Disabled filters remove elements from the enabled list.
        # disabled += filters.CSRFFilter
      
      
        ## CORS filter configuration
        # https://www.playframework.com/documentation/latest/CorsFilter
        # ~~~~~
        # CORS is a protocol that allows web applications to make requests from the browser
        # across different domains.
        # NOTE: You MUST apply the CORS configuration before the CSRF filter, as CSRF has
        # dependencies on CORS settings.
        cors {
          # Filter paths by a whitelist of path prefixes
          #pathPrefixes = ["/some/path", ...]
      
          # The allowed origins. If null, all origins are allowed.
          #allowedOrigins = ["http://www.example.com"]
      
          # The allowed HTTP methods. If null, all methods are allowed
          #allowedHttpMethods = ["GET", "POST"]
        }
      
        ## Security headers filter configuration
        # https://www.playframework.com/documentation/latest/SecurityHeaders
        # ~~~~~
        # Defines security headers that prevent XSS attacks.
        # If enabled, then all options are set to the below configuration by default:
        headers {
          # The X-Frame-Options header. If null, the header is not set.
          #frameOptions = "DENY"
      
          # The X-XSS-Protection header. If null, the header is not set.
          #xssProtection = "1; mode=block"
      
          # The X-Content-Type-Options header. If null, the header is not set.
          #contentTypeOptions = "nosniff"
      
          # The X-Permitted-Cross-Domain-Policies header. If null, the header is not set.
          #permittedCrossDomainPolicies = "master-only"
      
          # The Content-Security-Policy header. If null, the header is not set.
          #contentSecurityPolicy = "default-src 'self'"
        }
      
        ## Allowed hosts filter configuration
        # https://www.playframework.com/documentation/latest/AllowedHostsFilter
        # ~~~~~
        # Play provides a filter that lets you configure which hosts can access your application.
        # This is useful to prevent cache poisoning attacks.
        hosts {
          # Allow requests to example.com, its subdomains, and localhost:9000.
          #allowed = [".example.com", "localhost:9000"]
        }
      }
      
      play.http.parser.maxMemoryBuffer = 50MB
      akka.http.parsing.max-content-length = 50MB
       
      schema.base_path= "https:////schemas/local"
      
      
      telemetry_env=sunbird
      installation.id=ekstep
      
      # ElasticSearch Configuration
      ekstepPlatformApiUserId="search-service"
      search.es_conn_info="elasticsearch:9200"
      search.fields.query=["name^100","title^100","lemma^100","code^100","domain","subject","description^10","keywords^100","ageGroup^10","filter^10","theme^10","genre^10","objects^25","contentType^100","language^200","teachingMode^25","skills^10","learningObjective^10","curriculum^100","gradeLevel^100","developer^100","attributions^10","identifier^100","IL_UNIQUE_ID^100","owner^50","board^100","relatedBoards^100","creator^100", "dialcodes^100","text","words","releaseNotes"]
      search.fields.date= ["lastUpdatedOn","createdOn","versionDate","lastSubmittedOn","lastPublishedOn"]
      search.batch.size=500
      search.connection.timeout=30
      search.fields.mode_collection=["identifier","name","objectType","contentType","mimeType","size","childNodes","board","subject","medium","gradeLevel","appIcon","resourceType","origin","originData"]
      
      language.map={
        "Assamese": "as",
        "Bengali": "bn",
        "Bodo": "bo",
        "English": "en",
        "Gujarati": "gu",
        "Hindi": "hi",
        "Kannada": "ka",
        "Konkani": "ko",
        "Malayalam": "ml",
        "Marathi": "mr",
        "Nepali": "ne",
        "Odia": "or",
        "Punjabi": "pj",
        "Sanskrit": "san",
        "Tamil": "ta",
        "Telugu": "te",
      }
      # Configuration for default channel ID
      channel.default="in.ekstep"
      compositesearch.index.name="compositesearch"
      
      content.tagging.backward_enable=false
      content.tagging.property=["subject","medium"]
      search.payload.log_enable=true
      
      
       # Take only root level files (configs/*) for configmaps # Take only root level files (configs/*) for configmaps # Skip env.yaml as configmap, as it's env file
    
  logback.xml: |-
      <configuration>
      
          <conversionRule conversionWord="coloredLevel" converterClass="play.api.libs.logback.ColoredLevel" />
      
          <!-- transaction-event-trigger START -->
          <timestamp key="timestamp" datePattern="yyyy-MM-dd"/>
          <!-- common transactions logs -->
          <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
          <encoder>
              <pattern>%d %msg%n</pattern>
          </encoder>
          </appender>
      
          <appender name="ASYNCSTDOUT" class="ch.qos.logback.classic.AsyncAppender">
          <appender-ref ref="STDOUT" />
          </appender>
      
      
          <logger name="play" level="INFO" />
          <logger name="DefaultPlatformLogger" level="INFO" />
          <!-- Telemetry Loggers-->
          <logger name="TelemetryEventLogger" level="INFO" />
      
          <root level="INFO">
          <appender-ref ref="ASYNCSTDOUT" />
          </root>
      
      </configuration>
---
# Source: knowledgebb/charts/search/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: search-env
  labels:

    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: search
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: search-0.1.0
  annotations:

    reloader.stakater.com/auto: "true"
data:
  # You can add key value pair here, to create env values.
  # for example,
  
  # ENV: dev
  JAVA_OPTIONS: -Xms500m -Xmx500m
  _JAVA_OPTIONS: -Dlog4j2.formatMsgNoLookups=true
---
# Source: knowledgebb/charts/taxonomy/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: taxonomy
  labels:

    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: taxonomy
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: taxonomy-0.1.0
  annotations:

    reloader.stakater.com/auto: "true"
data: # Take only root level files (configs/*) for configmaps # Skip env.yaml as configmap, as it's env file
    
  application.conf: |-
          # This is the main configuration file for the application.
          # https://www.playframework.com/documentation/latest/ConfigFile
          # ~~~~~
          # Play uses HOCON as its configuration file format.  HOCON has a number
          # of advantages over other config formats, but there are two things that
          # can be used when modifying settings.
          #
          # You can include other configuration files in this main application.conf file:
          #include "extra-config.conf"
          #
          # You can declare variables and substitute for them:
          #mykey = ${some.value}
          #
          # And if an environment variable exists when there is no other substitution, then
          # HOCON will fall back to substituting environment variable:
          #mykey = ${JAVA_HOME}
      
          ## Akka
          # https://www.playframework.com/documentation/latest/ScalaAkka#Configuration
          # https://www.playframework.com/documentation/latest/JavaAkka#Configuration
          # ~~~~~
          # Play uses Akka internally and exposes Akka Streams and actors in Websockets and
          # other streaming HTTP responses.
          akka {
            # "akka.log-config-on-start" is extraordinarly useful because it log the complete
            # configuration at INFO level, including defaults and overrides, so it s worth
            # putting at the very top.
            #
            # Put the following in your conf/logback.xml file:
            #
            # <logger name="akka.actor" level="INFO" />
            #
            # And then uncomment this line to debug the configuration.
            #
            #log-config-on-start = true
            default-dispatcher {
              # This will be used if you have set "executor = "fork-join-executor""
              fork-join-executor {
                # Min number of threads to cap factor-based parallelism number to
                parallelism-min = 8
      
                # The parallelism factor is used to determine thread pool size using the
                # following formula: ceil(available processors * factor). Resulting size
                # is then bounded by the parallelism-min and parallelism-max values.
                parallelism-factor = 32.0
      
                # Max number of threads to cap factor-based parallelism number to
                parallelism-max = 64
      
                # Setting to "FIFO" to use queue like peeking mode which "poll" or "LIFO" to use stack
                # like peeking mode which "pop".
                task-peeking-mode = "FIFO"
              }
            }
            actors-dispatcher {
              type = "Dispatcher"
              executor = "fork-join-executor"
              fork-join-executor {
                parallelism-min = 8
                parallelism-factor = 32.0
                parallelism-max = 64
              }
              # Throughput for default Dispatcher, set to 1 for as fair as possible
              throughput = 1
            }
            actor {
              deployment {
                /contentActor
                  {
                    router = smallest-mailbox-pool
                    nr-of-instances = 10
                    dispatcher = actors-dispatcher
                  }
              }
            }
          }
      
          ## Secret key
          # http://www.playframework.com/documentation/latest/ApplicationSecret
          # ~~~~~
          # The secret key is used to sign Play's session cookie.
          # This must be changed for production, but we don't recommend you change it in this file.
          play.http.secret.key="secretKey"
      
          ## Modules
          # https://www.playframework.com/documentation/latest/Modules
          # ~~~~~
          # Control which modules are loaded when Play starts.  Note that modules are
          # the replacement for "GlobalSettings", which are deprecated in 2.5.x.
          # Please see https://www.playframework.com/documentation/latest/GlobalSettings
          # for more information.
          #
          # You can also extend Play functionality by using one of the publically available
          # Play modules: https://playframework.com/documentation/latest/ModuleDirectory
          play.modules {
            # By default, Play will load any class called Module that is defined
            # in the root package (the "app" directory), or you can define them
            # explicitly below.
            # If there are any built-in modules that you want to enable, you can list them here.
            enabled += modules.TaxonomyModule
      
            # If there are any built-in modules that you want to disable, you can list them here.
            #disabled += ""
          }
      
          ## IDE
          # https://www.playframework.com/documentation/latest/IDE
          # ~~~~~
          # Depending on your IDE, you can add a hyperlink for errors that will jump you
          # directly to the code location in the IDE in dev mode. The following line makes
          # use of the IntelliJ IDEA REST interface:
          #play.editor="http://localhost:63342/api/file/?file=%s&line=%s"
      
          ## Internationalisation
          # https://www.playframework.com/documentation/latest/JavaI18N
          # https://www.playframework.com/documentation/latest/ScalaI18N
          # ~~~~~
          # Play comes with its own i18n settings, which allow the user's preferred language
          # to map through to internal messages, or allow the language to be stored in a cookie.
          play.i18n {
            # The application languages
            langs = [ "en" ]
      
            # Whether the language cookie should be secure or not
            #langCookieSecure = true
      
            # Whether the HTTP only attribute of the cookie should be set to true
            #langCookieHttpOnly = true
          }
      
          ## Play HTTP settings
          # ~~~~~
          play.http {
            ## Router
            # https://www.playframework.com/documentation/latest/JavaRouting
            # https://www.playframework.com/documentation/latest/ScalaRouting
            # ~~~~~
            # Define the Router object to use for this application.
            # This router will be looked up first when the application is starting up,
            # so make sure this is the entry point.
            # Furthermore, it's assumed your route file is named properly.
            # So for an application router like `my.application.Router`,
            # you may need to define a router file `conf/my.application.routes`.
            # Default to Routes in the root package (aka "apps" folder) (and conf/routes)
            #router = my.application.Router
      
            ## Action Creator
            # https://www.playframework.com/documentation/latest/JavaActionCreator
            # ~~~~~
            #actionCreator = null
      
            ## ErrorHandler
            # https://www.playframework.com/documentation/latest/JavaRouting
            # https://www.playframework.com/documentation/latest/ScalaRouting
            # ~~~~~
            # If null, will attempt to load a class called ErrorHandler in the root package,
            #errorHandler = null
      
            ## Session & Flash
            # https://www.playframework.com/documentation/latest/JavaSessionFlash
            # https://www.playframework.com/documentation/latest/ScalaSessionFlash
            # ~~~~~
            session {
              # Sets the cookie to be sent only over HTTPS.
              #secure = true
      
              # Sets the cookie to be accessed only by the server.
              #httpOnly = true
      
              # Sets the max-age field of the cookie to 5 minutes.
              # NOTE: this only sets when the browser will discard the cookie. Play will consider any
              # cookie value with a valid signature to be a valid session forever. To implement a server side session timeout,
              # you need to put a timestamp in the session and check it at regular intervals to possibly expire it.
              #maxAge = 300
      
              # Sets the domain on the session cookie.
              #domain = "example.com"
            }
      
            flash {
              # Sets the cookie to be sent only over HTTPS.
              #secure = true
      
              # Sets the cookie to be accessed only by the server.
              #httpOnly = true
            }
          }
      
          play.server.http.idleTimeout = 60s
          play.http.parser.maxDiskBuffer = 10MB
          parsers.anyContent.maxLength = 10MB
      
          ## Netty Provider
          # https://www.playframework.com/documentation/latest/SettingsNetty
          # ~~~~~
          play.server.netty {
            # Whether the Netty wire should be logged
            log.wire = true
      
            # If you run Play on Linux, you can use Netty's native socket transport
            # for higher performance with less garbage.
            transport = "jdk"
          }
      
          ## WS (HTTP Client)
          # https://www.playframework.com/documentation/latest/ScalaWS#Configuring-WS
          # ~~~~~
          # The HTTP client primarily used for REST APIs.  The default client can be
          # configured directly, but you can also create different client instances
          # with customized settings. You must enable this by adding to build.sbt:
          #
          # libraryDependencies += ws // or javaWs if using java
          #
          play.ws {
            # Sets HTTP requests not to follow 302 requests
            #followRedirects = false
      
            # Sets the maximum number of open HTTP connections for the client.
            #ahc.maxConnectionsTotal = 50
      
            ## WS SSL
            # https://www.playframework.com/documentation/latest/WsSSL
            # ~~~~~
            ssl {
              # Configuring HTTPS with Play WS does not require programming.  You can
              # set up both trustManager and keyManager for mutual authentication, and
              # turn on JSSE debugging in development with a reload.
              #debug.handshake = true
              #trustManager = {
              #  stores = [
              #    { type = "JKS", path = "exampletrust.jks" }
              #  ]
              #}
            }
          }
      
          ## Cache
          # https://www.playframework.com/documentation/latest/JavaCache
          # https://www.playframework.com/documentation/latest/ScalaCache
          # ~~~~~
          # Play comes with an integrated cache API that can reduce the operational
          # overhead of repeated requests. You must enable this by adding to build.sbt:
          #
          # libraryDependencies += cache
          #
          play.cache {
            # If you want to bind several caches, you can bind the individually
            #bindCaches = ["db-cache", "user-cache", "session-cache"]
          }
      
          ## Filter Configuration
          # https://www.playframework.com/documentation/latest/Filters
          # ~~~~~
          # There are a number of built-in filters that can be enabled and configured
          # to give Play greater security.
          #
          play.filters {
      
            # Enabled filters are run automatically against Play.
            # CSRFFilter, AllowedHostFilters, and SecurityHeadersFilters are enabled by default.
            enabled = [filters.AccessLogFilter]
      
            # Disabled filters remove elements from the enabled list.
            # disabled += filters.CSRFFilter
      
      
            ## CORS filter configuration
            # https://www.playframework.com/documentation/latest/CorsFilter
            # ~~~~~
            # CORS is a protocol that allows web applications to make requests from the browser
            # across different domains.
            # NOTE: You MUST apply the CORS configuration before the CSRF filter, as CSRF has
            # dependencies on CORS settings.
            cors {
              # Filter paths by a whitelist of path prefixes
              #pathPrefixes = ["/some/path", ...]
      
              # The allowed origins. If null, all origins are allowed.
              #allowedOrigins = ["http://www.example.com"]
      
              # The allowed HTTP methods. If null, all methods are allowed
              #allowedHttpMethods = ["GET", "POST"]
            }
      
            ## Security headers filter configuration
            # https://www.playframework.com/documentation/latest/SecurityHeaders
            # ~~~~~
            # Defines security headers that prevent XSS attacks.
            # If enabled, then all options are set to the below configuration by default:
            headers {
              # The X-Frame-Options header. If null, the header is not set.
              #frameOptions = "DENY"
      
              # The X-XSS-Protection header. If null, the header is not set.
              #xssProtection = "1; mode=block"
      
              # The X-Content-Type-Options header. If null, the header is not set.
              #contentTypeOptions = "nosniff"
      
              # The X-Permitted-Cross-Domain-Policies header. If null, the header is not set.
              #permittedCrossDomainPolicies = "master-only"
      
              # The Content-Security-Policy header. If null, the header is not set.
              #contentSecurityPolicy = "default-src 'self'"
            }
      
            ## Allowed hosts filter configuration
            # https://www.playframework.com/documentation/latest/AllowedHostsFilter
            # ~~~~~
            # Play provides a filter that lets you configure which hosts can access your application.
            # This is useful to prevent cache poisoning attacks.
            hosts {
              # Allow requests to example.com, its subdomains, and localhost:9000.
              #allowed = [".example.com", "localhost:9000"]
            }
          }
      
          play.http.parser.maxMemoryBuffer = 50MB
          akka.http.parsing.max-content-length = 50MB
          
          schema.base_path= "https:////schemas/local"
          
      
          # Cassandra Configuration
          cassandra {
            lp {
              connection: "cassandra:9042"
            }
            lpa {
              connection: "cassandra:9042"
            }
          }
      
          # Redis Configuration
          redis {
            host:"redis-master"
              port:"6379"
              maxConnections:"128"
          }
      
          # Configuration
          akka.request_timeout: 30
          environment.id: 10000000
          graph {
            dir: "/data/graphDB"
            ids: ["domain"]
            passport.key.base: "secretKey"
          }
          route.domain="bolt://neo4j:7687"
          route.bolt.write.domain="bolt://neo4j:7687"
          route.bolt.read.domain="bolt://neo4j:7687"
          route.all="bolt://neo4j:8687"
          route.bolt.write.all="bolt://neo4j:8687"
          route.bolt.read.all="bolt://neo4j:8687"
      
          shard.id: 1
          platform {
            auth.check.enabled: false
            cache.ttl: 3600000
          }
      
          # Cloud Storage Config
          cloud_storage_type="azure"
          cloud_storage_key=""
          cloud_storage_secret=""
          cloud_storage_container="" 
          cloud_storage_endpoint=""
      
          installation.id: ekstep
      
          kafka {
            urls : "kafka:9092"
          }
      
          channel {
            default: "in.ekstep"
          }
      
          languageCode {
            assamese : "as"
            bengali : "bn"
            english : "en"
            gujarati : "gu"
            hindi : "hi"
            kannada : "ka"
            marathi : "mr"
            odia : "or"
            tamil : "ta"
            telugu : "te"
          }
          objectcategorydefinition.keyspace="_category_store"
      
          # Framework master category validation Supported values are Yes/No
          master.category.validation.enabled= "Yes"
      
          framework.keyspace="_hierarchy_store"
          framework.hierarchy.table="framework_hierarchy"
          framework.categories_cached="["subject", "medium", "gradeLevel", "board"]"
          framework.cache.ttl=86400
          framework.cache.read="true"
       # Take only root level files (configs/*) for configmaps # Take only root level files (configs/*) for configmaps # Skip env.yaml as configmap, as it's env file
    
  logback.xml: |-
      <configuration>
      
          <conversionRule conversionWord="coloredLevel" converterClass="play.api.libs.logback.ColoredLevel" />
      
          <!-- transaction-event-trigger START -->
          <timestamp key="timestamp" datePattern="yyyy-MM-dd"/>
          <!-- common transactions logs -->
          <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
          <encoder>
              <pattern>%d %msg%n</pattern>
          </encoder>
          </appender>
      
          <appender name="ASYNCSTDOUT" class="ch.qos.logback.classic.AsyncAppender">
          <appender-ref ref="STDOUT" />
          </appender>
      
      
          <logger name="play" level="INFO" />
          <logger name="DefaultPlatformLogger" level="INFO" />
          <!-- Telemetry Loggers-->
      
          <root level="INFO">
          <appender-ref ref="ASYNCSTDOUT" />
          </root>
      
          <logger name="TelemetryEventLogger" level="INFO">
      
          </logger>
          
      </configuration>
---
# Source: knowledgebb/charts/taxonomy/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: taxonomy-env
  labels:

    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: taxonomy
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: taxonomy-0.1.0
  annotations:

    reloader.stakater.com/auto: "true"
data:
  # You can add key value pair here, to create env values.
  # for example,
  
  # ENV: dev
  JAVA_OPTIONS: -Xms500m -Xmx500m
  _JAVA_OPTIONS: -Dlog4j2.formatMsgNoLookups=true
---
# Source: knowledgebb/charts/neo4j/templates/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: neo4j-kbb-claim
  labels:
    app: neo4j-kbb
  annotations: 
    helm.sh/resource-policy: "keep"
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 25Gi
---
# Source: knowledgebb/charts/cassandra/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: cassandra-kbb-headless
  namespace: "sunbird"
  labels:
    app.kubernetes.io/name: cassandra
    helm.sh/chart: cassandra-10.1.0
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "4.1.0"
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: intra
      port: 7000
      targetPort: intra
    - name: tls
      port: 7001
      targetPort: tls
    - name: jmx
      port: 7199
      targetPort: jmx
    - name: cql
      port: 9042
      targetPort: cql
  selector:
    app.kubernetes.io/name: cassandra
    app.kubernetes.io/instance: neo4j-kbb
---
# Source: knowledgebb/charts/cassandra/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: cassandra-kbb
  namespace: "sunbird"
  labels:
    app.kubernetes.io/name: cassandra
    helm.sh/chart: cassandra-10.1.0
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "4.1.0"
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: cql
      port: 9042
      targetPort: cql
      nodePort: null
    - name: metrics
      port: 8080
      nodePort: null
  selector:
    app.kubernetes.io/name: cassandra
    app.kubernetes.io/instance: neo4j-kbb
---
# Source: knowledgebb/charts/content/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: content-service
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: content
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: content-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
  ports:
  - name: http-content
    protocol: 
    port: 9000
    targetPort: 9000
  - name: opa-metrics
    port: 8181
    protocol: TCP
    targetPort: 8181
  - name: envoy-metrics
    port: 10000
    protocol: TCP
    targetPort: 10000
  selector:
    app.kubernetes.io/name: content
---
# Source: knowledgebb/charts/dial/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: dial-service
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: dial
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: dial-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
  ports:
  - name: http-dial
    port: 9000
    targetPort: 9000
  selector:
    app.kubernetes.io/name: dial
---
# Source: knowledgebb/charts/elasticsearch/templates/master/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch-kbb-master-hl
  namespace: "sunbird"
  labels: 
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-19.5.14
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "8.6.2"
    app.kubernetes.io/component: master
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  type: ClusterIP
  publishNotReadyAddresses: true
  ports:
    - name: tcp-rest-api
      port: 9200
      targetPort: rest-api
    - name: tcp-transport
      port: 9300
      targetPort: transport
  selector:
    app.kubernetes.io/name: elasticsearch
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/component: master
---
# Source: knowledgebb/charts/elasticsearch/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch-kbb
  namespace: "sunbird"
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-19.5.14
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "8.6.2"
    app.kubernetes.io/component: master
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-rest-api
      port: 9200
      targetPort: rest-api
      nodePort: null
    - name: tcp-transport
      port: 9300
      nodePort: null
  selector:
    app.kubernetes.io/name: elasticsearch
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/component: master
---
# Source: knowledgebb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: asset-enrichment-jobmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
    #clusterIP: None
  ports:
    - name: http
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: asset-enrichment-jobmanager
---
# Source: knowledgebb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: asset-enrichment-taskmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
    #clusterIP: None
  ports:
    - name: http
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: asset-enrichment-taskmanager
---
# Source: knowledgebb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: audit-event-generator-jobmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
    #clusterIP: None
  ports:
    - name: http
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: audit-event-generator-jobmanager
---
# Source: knowledgebb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: audit-event-generator-taskmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
    #clusterIP: None
  ports:
    - name: http
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: audit-event-generator-taskmanager
---
# Source: knowledgebb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: audit-history-indexer-jobmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
    #clusterIP: None
  ports:
    - name: http
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: audit-history-indexer-jobmanager
---
# Source: knowledgebb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: audit-history-indexer-taskmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
    #clusterIP: None
  ports:
    - name: http
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: audit-history-indexer-taskmanager
---
# Source: knowledgebb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: content-publish-jobmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
    #clusterIP: None
  ports:
    - name: http
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: content-publish-jobmanager
---
# Source: knowledgebb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: content-publish-taskmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
    #clusterIP: None
  ports:
    - name: http
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: content-publish-taskmanager
---
# Source: knowledgebb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: dialcode-context-updater-jobmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
    #clusterIP: None
  ports:
    - name: http
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: dialcode-context-updater-jobmanager
---
# Source: knowledgebb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: dialcode-context-updater-taskmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
    #clusterIP: None
  ports:
    - name: http
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: dialcode-context-updater-taskmanager
---
# Source: knowledgebb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: post-publish-processor-jobmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
    #clusterIP: None
  ports:
    - name: http
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: post-publish-processor-jobmanager
---
# Source: knowledgebb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: post-publish-processor-taskmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
    #clusterIP: None
  ports:
    - name: http
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: post-publish-processor-taskmanager
---
# Source: knowledgebb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: qrcode-image-generator-jobmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
    #clusterIP: None
  ports:
    - name: http
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: qrcode-image-generator-jobmanager
---
# Source: knowledgebb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: qrcode-image-generator-taskmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
    #clusterIP: None
  ports:
    - name: http
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: qrcode-image-generator-taskmanager
---
# Source: knowledgebb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: search-indexer-jobmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
    #clusterIP: None
  ports:
    - name: http
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: search-indexer-jobmanager
---
# Source: knowledgebb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: search-indexer-taskmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
    #clusterIP: None
  ports:
    - name: http
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: search-indexer-taskmanager
---
# Source: knowledgebb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: video-stream-generator-jobmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
    #clusterIP: None
  ports:
    - name: http
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: video-stream-generator-jobmanager
---
# Source: knowledgebb/charts/flink/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: video-stream-generator-taskmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
    #clusterIP: None
  ports:
    - name: http
      port: 80
      targetPort: 8081
  selector:
    app.kubernetes.io/component: video-stream-generator-taskmanager
---
# Source: knowledgebb/charts/kafka/charts/zookeeper/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: zookeeper-kbb-headless
  namespace: sunbird
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-11.0.2
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.8.0"
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/component: zookeeper
---
# Source: knowledgebb/charts/kafka/charts/zookeeper/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: zookeeper-kbb
  namespace: sunbird
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-11.0.2
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.8.0"
    app.kubernetes.io/component: zookeeper
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 2181
      targetPort: client
      nodePort: null
    - name: tcp-follower
      port: 2888
      targetPort: follower
    - name: tcp-election
      port: 3888
      targetPort: election
  selector:
    app.kubernetes.io/name: zookeeper
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/component: zookeeper
---
# Source: knowledgebb/charts/kafka/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: kafka-kbb-headless
  namespace: "sunbird"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-20.0.2
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.3.1"
    app.kubernetes.io/component: kafka
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: false
  ports:
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: kafka-client
    - name: tcp-internal
      port: 9093
      protocol: TCP
      targetPort: kafka-internal
  selector:
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/component: kafka
---
# Source: knowledgebb/charts/kafka/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: kafka-kbb
  namespace: "sunbird"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-20.0.2
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.3.1"
    app.kubernetes.io/component: kafka
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-client
      port: 9092
      protocol: TCP
      targetPort: kafka-client
      nodePort: null
  selector:
    app.kubernetes.io/name: kafka
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/component: kafka
---
# Source: knowledgebb/charts/learning/templates/deployment.yaml
apiVersion: v1
kind: Service
metadata:
  name: learning
  namespace: sunbird
  labels:
    app: learning
spec:
  ports:
    - name: http-learning
      protocol: TCP
      port: 8080
  selector:
    app: learning
---
# Source: knowledgebb/charts/neo4j/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: neo4j-kbb
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: neo4j-kbb
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: neo4j-0.1.0
  annotations:
    helm.sh/hook-weight: "-5"
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
  ports:
  - name: cypher-port
    port: 7474
    targetPort: 7474
  - name: bolt-port-1
    port: 7687
    targetPort: 7687
  - name: bolt-port-2
    port: 8687
    targetPort: 8687
  selector:
    app.kubernetes.io/name: neo4j-kbb
---
# Source: knowledgebb/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-kbb-headless
  namespace: "sunbird"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.8.3
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.9"
  annotations:
    helm.sh/hook-weight: "-5"
    
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: neo4j-kbb
---
# Source: knowledgebb/charts/redis/templates/master/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-kbb-master
  namespace: "sunbird"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.8.3
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.9"
    app.kubernetes.io/component: master
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  type: ClusterIP
  internalTrafficPolicy: Cluster
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/component: master
---
# Source: knowledgebb/charts/redis/templates/replicas/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-kbb-replicas
  namespace: "sunbird"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.8.3
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.9"
    app.kubernetes.io/component: replica
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  type: ClusterIP
  internalTrafficPolicy: Cluster
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/name: redis
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/component: replica
---
# Source: knowledgebb/charts/search/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: search-service
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: search
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: search-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
  ports:
  - name: http-search
    port: 9000
    targetPort: 9000
  selector:
    app.kubernetes.io/name: search
---
# Source: knowledgebb/charts/taxonomy/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: taxonomy-service
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: taxonomy
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: taxonomy-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  type: ClusterIP
  ports:
  - name: http-taxonomy
    port: 9000
    targetPort: 9000
  selector:
    app.kubernetes.io/name: taxonomy
---
# Source: knowledgebb/charts/content/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: content
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: content
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: content-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: content
  template:
    metadata:
      labels:
        app.kubernetes.io/name: content
      annotations:
        checksum/config: cd8f09dc5c99756f63eb746528a93c37ea3f173fdbfab3568bd8272a9ffc78b8
    spec:
      serviceAccountName: content
      securityContext:
        {}
      containers:
        - name: content
          image: "sunbirded.azurecr.io/content-service:release-6-1.0-CSS-1.4.7.1_v1"
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /health
              port: 9000
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /health
              port: 9000
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          ports:
            - name: http-content
              containerPort: 9000
          resources:
            limits:
              cpu: 1
              memory: 1024Mi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            {}
          envFrom:
          - configMapRef:
              name: content-env
          volumeMounts:
          - name: config
            mountPath: /home/sunbird/content-service-1.0-SNAPSHOT/config
        - args:
          - envoy
          - --config-path
          - /config/config.yaml
          env:
          - name: ENVOY_UID
            value: "1111"
          image: envoyproxy/envoy:v1.20.0
          imagePullPolicy: IfNotPresent
          name: envoy
          livenessProbe:
            
            failureThreshold: 2
            httpGet:
              path: /ready
              port: 10000
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 15
            timeoutSeconds: 5
          readinessProbe:
            
            failureThreshold: 2
            httpGet:
              path: /ready
              port: 10000
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 15
            timeoutSeconds: 5
          resources:
            
            limits:
              cpu: "1"
              memory: 1024Mi
            requests:
              cpu: 100m
              memory: 100Mi
          volumeMounts:
            - mountPath: /config
              name: envoy-config
              readOnly: true
        - command: ["/bin/sh", "-c"]
          args:
          - |
            KID=$(cat /keys/KEYCLOAK_PUBLIC_KEY_KID) && PUBLIC_KEY=$(cat /keys/KEYCLOAK_PUBLIC_KEY) && \
            PUBLIC_KEY_PEM=$(echo "$PUBLIC_KEY" | sed -e 's/.\{64\}/&\\\\n/g' -e '1s/^/-----BEGIN PUBLIC KEY-----\\\\n/' -e '$s/$/\\\\n-----END PUBLIC KEY-----/') && \
            mkdir /policies && cp -Lr /opa-policies/*.rego /policies && \
            sed -i "s|KEYCLOAK_KID|$KID|g" /policies/common.rego && \
            sed -i "s|KEYCLOAK_PUBLIC_KEY|$PUBLIC_KEY_PEM|g" /policies/common.rego && \
            /app/opa run --server /policies \
              --addr=localhost:8181 \
              --diagnostic-addr=0.0.0.0:8282 \
              --set=plugins.envoy_ext_authz_grpc.addr=:9191 \
              --set=plugins.envoy_ext_authz_grpc.path=main/allow \
              --set=decision_logs.plugin=print_decision_logs_on_failure \
              --set=plugins.print_decision_logs_on_failure.stdout=true \
              --log-level=error \
              --ignore=.*
          image: sunbirded.azurecr.io/opa:0.34.2-envoy
          imagePullPolicy: IfNotPresent
          name: opa
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health?plugins
              port: 8282
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 15
            timeoutSeconds: 5
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health?plugins
              port: 8282
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 15
            timeoutSeconds: 5
          resources:
            limits:
              cpu: "1"
              memory: 1024Mi
            requests:
              cpu: 100m
              memory: 100Mi
          volumeMounts:
            - mountPath: /opa-policies
              name: opa-policy
              readOnly: true
            - name: keycloak-key
              mountPath: /keys/
      initContainers:
        - args:
          - -p
          - "9999"
          - -u
          - "1111"
          - -w
          - "8282,10000"
          image: openpolicyagent/proxy_init:v5
          imagePullPolicy: IfNotPresent
          name: proxy-init
          resources:
            
            limits:
              cpu: "1"
              memory: 1024Mi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
            runAsNonRoot: false
            runAsUser: 0
      volumes:
      - name: env
        configMap:
          name: content-env
      - name: config
        configMap:
          name: content
      - name: envoy-config
        configMap:
          name: content-envoy
      - name: opa-policy
        configMap:
          name: content-opa
      - name: keycloak-key
        configMap:
          name: keycloak-kids-keys
---
# Source: knowledgebb/charts/dial/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dial
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: dial
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: dial-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: dial
  template:
    metadata:
      labels:
        app.kubernetes.io/name: dial
      annotations:
        checksum/config: f2e615bd0d282f718162c506f76141070d64943a99c646edf2830bc1e0cb7722
    spec:
      serviceAccountName: dial
      securityContext:
        {}
      containers:
        - name: dial
          image: "sunbirded.azurecr.io/sunbird-dial-service:release-6.1.0_RC2_213843a_30"
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /health
              port: 9000
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /health
              port: 9000
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          ports:
            - name: http-dial
              containerPort: 9000
          resources:
            limits:
              cpu: 1
              memory: 1024Mi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            {}
          envFrom:
          - configMapRef:
              name: dial-env
          volumeMounts:
          - name: config
            mountPath: /home/sunbird/sunbird-dial-service-1.0-SNAPSHOT/config
      volumes:
      - name: env
        configMap:
          name: dial-env
      - name: config
        configMap:
          name: dial
---
# Source: knowledgebb/charts/flink/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: asset-enrichment-taskmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    checksum/job-config: 4e59bee45927bec15aa36abe967f02ae27c91650879cfbc28858064727ebac73
    
    reloader.stakater.com/auto: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: flink
      app.kubernetes.io/component: asset-enrichment-taskmanager
  template:
    metadata:
      labels:
        app.kubernetes.io/name: flink
        app.kubernetes.io/component: asset-enrichment-taskmanager
      annotations:
        checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/job-config: 4e59bee45927bec15aa36abe967f02ae27c91650879cfbc28858064727ebac73
    spec:
      serviceAccountName: 
      securityContext:
        fsGroup: 0
        runAsUser: 0
      containers:
        - name: flink
          image: "sunbirded.azurecr.io/knowledge-platform-jobs:release-5.7.0_RC3_18c0d23_90"
          imagePullPolicy: IfNotPresent
          workingDir: 
          command: ["/opt/flink/bin/taskmanager.sh"]
          args:
            ["start-foreground",
            "-Dweb.submit.enable=false",
            "-Dmetrics.reporter.prom.class=org.apache.flink.metrics.prometheus.PrometheusReporter",
            "-Dmetrics.reporter.prom.host=asset-enrichment-taskmanager",
            "-Dmetrics.reporter.prom.port=9251-9260",
            "-Djobmanager.rpc.address=asset-enrichment-jobmanager",
            "-Dtaskmanager.rpc.port=6122"]
          ports:
            - name: http
              containerPort: 8081
          resources:
            limits:
              cpu: 1
              memory: 2048Mi
            requests:
              cpu: 100m
              memory: 500Mi
          securityContext:
            {}
          volumeMounts:
          - name: flink-config-volume
            mountPath: /opt/flink/conf/flink-conf.yaml
            subPath: flink-conf.yaml
          - name: flink-config-volume
            mountPath: /opt/flink/conf/log4j-console.properties
            subPath: log4j-console.properties
      volumes:
      - name: flink-config-volume
        configMap:
          name: asset-enrichment-config
          items:
          - key: flink-conf
            path: flink-conf.yaml
          - key: log4j_console_properties
            path: log4j-console.properties
---
# Source: knowledgebb/charts/flink/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: audit-event-generator-taskmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    checksum/job-config: 5b2ce7580f1c46c847a28be02ead42a53447b9a5650a566ec5d67aab58aa9087
    
    reloader.stakater.com/auto: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: flink
      app.kubernetes.io/component: audit-event-generator-taskmanager
  template:
    metadata:
      labels:
        app.kubernetes.io/name: flink
        app.kubernetes.io/component: audit-event-generator-taskmanager
      annotations:
        checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/job-config: 5b2ce7580f1c46c847a28be02ead42a53447b9a5650a566ec5d67aab58aa9087
    spec:
      serviceAccountName: 
      securityContext:
        fsGroup: 0
        runAsUser: 0
      containers:
        - name: flink
          image: "sunbirded.azurecr.io/knowledge-platform-jobs:release-5.7.0_RC3_18c0d23_90"
          imagePullPolicy: IfNotPresent
          workingDir: 
          command: ["/opt/flink/bin/taskmanager.sh"]
          args:
            ["start-foreground",
            "-Dweb.submit.enable=false",
            "-Dmetrics.reporter.prom.class=org.apache.flink.metrics.prometheus.PrometheusReporter",
            "-Dmetrics.reporter.prom.host=audit-event-generator-taskmanager",
            "-Dmetrics.reporter.prom.port=9251-9260",
            "-Djobmanager.rpc.address=audit-event-generator-jobmanager",
            "-Dtaskmanager.rpc.port=6122"]
          ports:
            - name: http
              containerPort: 8081
          resources:
            limits:
              cpu: 1
              memory: 2048Mi
            requests:
              cpu: 100m
              memory: 500Mi
          securityContext:
            {}
          volumeMounts:
          - name: flink-config-volume
            mountPath: /opt/flink/conf/flink-conf.yaml
            subPath: flink-conf.yaml
          - name: flink-config-volume
            mountPath: /opt/flink/conf/log4j-console.properties
            subPath: log4j-console.properties
      volumes:
      - name: flink-config-volume
        configMap:
          name: audit-event-generator-config
          items:
          - key: flink-conf
            path: flink-conf.yaml
          - key: log4j_console_properties
            path: log4j-console.properties
---
# Source: knowledgebb/charts/flink/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: audit-history-indexer-taskmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    checksum/job-config: f2facb5c30295e72477f6a729d52c156bff6266179700296cafdf5c084a62dc2
    
    reloader.stakater.com/auto: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: flink
      app.kubernetes.io/component: audit-history-indexer-taskmanager
  template:
    metadata:
      labels:
        app.kubernetes.io/name: flink
        app.kubernetes.io/component: audit-history-indexer-taskmanager
      annotations:
        checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/job-config: f2facb5c30295e72477f6a729d52c156bff6266179700296cafdf5c084a62dc2
    spec:
      serviceAccountName: 
      securityContext:
        fsGroup: 0
        runAsUser: 0
      containers:
        - name: flink
          image: "sunbirded.azurecr.io/knowledge-platform-jobs:release-5.7.0_RC3_18c0d23_90"
          imagePullPolicy: IfNotPresent
          workingDir: 
          command: ["/opt/flink/bin/taskmanager.sh"]
          args:
            ["start-foreground",
            "-Dweb.submit.enable=false",
            "-Dmetrics.reporter.prom.class=org.apache.flink.metrics.prometheus.PrometheusReporter",
            "-Dmetrics.reporter.prom.host=audit-history-indexer-taskmanager",
            "-Dmetrics.reporter.prom.port=9251-9260",
            "-Djobmanager.rpc.address=audit-history-indexer-jobmanager",
            "-Dtaskmanager.rpc.port=6122"]
          ports:
            - name: http
              containerPort: 8081
          resources:
            limits:
              cpu: 1
              memory: 2048Mi
            requests:
              cpu: 100m
              memory: 500Mi
          securityContext:
            {}
          volumeMounts:
          - name: flink-config-volume
            mountPath: /opt/flink/conf/flink-conf.yaml
            subPath: flink-conf.yaml
          - name: flink-config-volume
            mountPath: /opt/flink/conf/log4j-console.properties
            subPath: log4j-console.properties
      volumes:
      - name: flink-config-volume
        configMap:
          name: audit-history-indexer-config
          items:
          - key: flink-conf
            path: flink-conf.yaml
          - key: log4j_console_properties
            path: log4j-console.properties
---
# Source: knowledgebb/charts/flink/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: content-publish-taskmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    checksum/job-config: 47d6db764bc7879aee7e0b4f21ab996b322baf7b1425b7b0b488ab6bd54d0409
    
    reloader.stakater.com/auto: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: flink
      app.kubernetes.io/component: content-publish-taskmanager
  template:
    metadata:
      labels:
        app.kubernetes.io/name: flink
        app.kubernetes.io/component: content-publish-taskmanager
      annotations:
        checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/job-config: 47d6db764bc7879aee7e0b4f21ab996b322baf7b1425b7b0b488ab6bd54d0409
    spec:
      serviceAccountName: 
      securityContext:
        fsGroup: 0
        runAsUser: 0
      containers:
        - name: flink
          image: "sunbirded.azurecr.io/knowledge-platform-jobs:release-5.7.0_RC3_18c0d23_90"
          imagePullPolicy: IfNotPresent
          workingDir: 
          command: ["/opt/flink/bin/taskmanager.sh"]
          args:
            ["start-foreground",
            "-Dweb.submit.enable=false",
            "-Dmetrics.reporter.prom.class=org.apache.flink.metrics.prometheus.PrometheusReporter",
            "-Dmetrics.reporter.prom.host=content-publish-taskmanager",
            "-Dmetrics.reporter.prom.port=9251-9260",
            "-Djobmanager.rpc.address=content-publish-jobmanager",
            "-Dtaskmanager.rpc.port=6122"]
          ports:
            - name: http
              containerPort: 8081
          resources:
            limits:
              cpu: 1
              memory: 2048Mi
            requests:
              cpu: 100m
              memory: 500Mi
          securityContext:
            {}
          volumeMounts:
          - name: flink-config-volume
            mountPath: /opt/flink/conf/flink-conf.yaml
            subPath: flink-conf.yaml
          - name: flink-config-volume
            mountPath: /opt/flink/conf/log4j-console.properties
            subPath: log4j-console.properties
      volumes:
      - name: flink-config-volume
        configMap:
          name: content-publish-config
          items:
          - key: flink-conf
            path: flink-conf.yaml
          - key: log4j_console_properties
            path: log4j-console.properties
---
# Source: knowledgebb/charts/flink/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dialcode-context-updater-taskmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    checksum/job-config: f85df668acc795353c985327db92aeb32d3983617ca273d1cd0449a1fece3440
    
    reloader.stakater.com/auto: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: flink
      app.kubernetes.io/component: dialcode-context-updater-taskmanager
  template:
    metadata:
      labels:
        app.kubernetes.io/name: flink
        app.kubernetes.io/component: dialcode-context-updater-taskmanager
      annotations:
        checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/job-config: f85df668acc795353c985327db92aeb32d3983617ca273d1cd0449a1fece3440
    spec:
      serviceAccountName: 
      securityContext:
        fsGroup: 0
        runAsUser: 0
      containers:
        - name: flink
          image: "sunbirded.azurecr.io/knowledge-platform-jobs:release-5.7.0_RC3_18c0d23_90"
          imagePullPolicy: IfNotPresent
          workingDir: 
          command: ["/opt/flink/bin/taskmanager.sh"]
          args:
            ["start-foreground",
            "-Dweb.submit.enable=false",
            "-Dmetrics.reporter.prom.class=org.apache.flink.metrics.prometheus.PrometheusReporter",
            "-Dmetrics.reporter.prom.host=dialcode-context-updater-taskmanager",
            "-Dmetrics.reporter.prom.port=9251-9260",
            "-Djobmanager.rpc.address=dialcode-context-updater-jobmanager",
            "-Dtaskmanager.rpc.port=6122"]
          ports:
            - name: http
              containerPort: 8081
          resources:
            limits:
              cpu: 1
              memory: 2048Mi
            requests:
              cpu: 100m
              memory: 500Mi
          securityContext:
            {}
          volumeMounts:
          - name: flink-config-volume
            mountPath: /opt/flink/conf/flink-conf.yaml
            subPath: flink-conf.yaml
          - name: flink-config-volume
            mountPath: /opt/flink/conf/log4j-console.properties
            subPath: log4j-console.properties
      volumes:
      - name: flink-config-volume
        configMap:
          name: dialcode-context-updater-config
          items:
          - key: flink-conf
            path: flink-conf.yaml
          - key: log4j_console_properties
            path: log4j-console.properties
---
# Source: knowledgebb/charts/flink/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: post-publish-processor-taskmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    checksum/job-config: 5d321b3b7f7ac8fbfc5ac8d57728b0d082fb3d0af1e7e21e60ddd97eead3a7a7
    
    reloader.stakater.com/auto: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: flink
      app.kubernetes.io/component: post-publish-processor-taskmanager
  template:
    metadata:
      labels:
        app.kubernetes.io/name: flink
        app.kubernetes.io/component: post-publish-processor-taskmanager
      annotations:
        checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/job-config: 5d321b3b7f7ac8fbfc5ac8d57728b0d082fb3d0af1e7e21e60ddd97eead3a7a7
    spec:
      serviceAccountName: 
      securityContext:
        fsGroup: 0
        runAsUser: 0
      containers:
        - name: flink
          image: "sunbirded.azurecr.io/knowledge-platform-jobs:release-5.7.0_RC3_18c0d23_90"
          imagePullPolicy: IfNotPresent
          workingDir: 
          command: ["/opt/flink/bin/taskmanager.sh"]
          args:
            ["start-foreground",
            "-Dweb.submit.enable=false",
            "-Dmetrics.reporter.prom.class=org.apache.flink.metrics.prometheus.PrometheusReporter",
            "-Dmetrics.reporter.prom.host=post-publish-processor-taskmanager",
            "-Dmetrics.reporter.prom.port=9251-9260",
            "-Djobmanager.rpc.address=post-publish-processor-jobmanager",
            "-Dtaskmanager.rpc.port=6122"]
          ports:
            - name: http
              containerPort: 8081
          resources:
            limits:
              cpu: 1
              memory: 2048Mi
            requests:
              cpu: 100m
              memory: 500Mi
          securityContext:
            {}
          volumeMounts:
          - name: flink-config-volume
            mountPath: /opt/flink/conf/flink-conf.yaml
            subPath: flink-conf.yaml
          - name: flink-config-volume
            mountPath: /opt/flink/conf/log4j-console.properties
            subPath: log4j-console.properties
      volumes:
      - name: flink-config-volume
        configMap:
          name: post-publish-processor-config
          items:
          - key: flink-conf
            path: flink-conf.yaml
          - key: log4j_console_properties
            path: log4j-console.properties
---
# Source: knowledgebb/charts/flink/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: qrcode-image-generator-taskmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    checksum/job-config: 102879e21329992a206210ec1d01d7bf5a66e5fb06b09671cc663d985620c5f8
    
    reloader.stakater.com/auto: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: flink
      app.kubernetes.io/component: qrcode-image-generator-taskmanager
  template:
    metadata:
      labels:
        app.kubernetes.io/name: flink
        app.kubernetes.io/component: qrcode-image-generator-taskmanager
      annotations:
        checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/job-config: 102879e21329992a206210ec1d01d7bf5a66e5fb06b09671cc663d985620c5f8
    spec:
      serviceAccountName: 
      securityContext:
        fsGroup: 0
        runAsUser: 0
      containers:
        - name: flink
          image: "sunbirded.azurecr.io/knowledge-platform-jobs:release-5.7.0_RC3_18c0d23_90"
          imagePullPolicy: IfNotPresent
          workingDir: 
          command: ["/opt/flink/bin/taskmanager.sh"]
          args:
            ["start-foreground",
            "-Dweb.submit.enable=false",
            "-Dmetrics.reporter.prom.class=org.apache.flink.metrics.prometheus.PrometheusReporter",
            "-Dmetrics.reporter.prom.host=qrcode-image-generator-taskmanager",
            "-Dmetrics.reporter.prom.port=9251-9260",
            "-Djobmanager.rpc.address=qrcode-image-generator-jobmanager",
            "-Dtaskmanager.rpc.port=6122"]
          ports:
            - name: http
              containerPort: 8081
          resources:
            limits:
              cpu: 1
              memory: 2048Mi
            requests:
              cpu: 100m
              memory: 500Mi
          securityContext:
            {}
          volumeMounts:
          - name: flink-config-volume
            mountPath: /opt/flink/conf/flink-conf.yaml
            subPath: flink-conf.yaml
          - name: flink-config-volume
            mountPath: /opt/flink/conf/log4j-console.properties
            subPath: log4j-console.properties
      volumes:
      - name: flink-config-volume
        configMap:
          name: qrcode-image-generator-config
          items:
          - key: flink-conf
            path: flink-conf.yaml
          - key: log4j_console_properties
            path: log4j-console.properties
---
# Source: knowledgebb/charts/flink/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: search-indexer-taskmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    checksum/job-config: deef7e393a5fd10317e1529423296f6018490eb0e39f5cd15aa28997023b615a
    
    reloader.stakater.com/auto: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: flink
      app.kubernetes.io/component: search-indexer-taskmanager
  template:
    metadata:
      labels:
        app.kubernetes.io/name: flink
        app.kubernetes.io/component: search-indexer-taskmanager
      annotations:
        checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/job-config: deef7e393a5fd10317e1529423296f6018490eb0e39f5cd15aa28997023b615a
    spec:
      serviceAccountName: 
      securityContext:
        fsGroup: 0
        runAsUser: 0
      containers:
        - name: flink
          image: "sunbirded.azurecr.io/knowledge-platform-jobs:release-5.7.0_RC3_18c0d23_90"
          imagePullPolicy: IfNotPresent
          workingDir: 
          command: ["/opt/flink/bin/taskmanager.sh"]
          args:
            ["start-foreground",
            "-Dweb.submit.enable=false",
            "-Dmetrics.reporter.prom.class=org.apache.flink.metrics.prometheus.PrometheusReporter",
            "-Dmetrics.reporter.prom.host=search-indexer-taskmanager",
            "-Dmetrics.reporter.prom.port=9251-9260",
            "-Djobmanager.rpc.address=search-indexer-jobmanager",
            "-Dtaskmanager.rpc.port=6122"]
          ports:
            - name: http
              containerPort: 8081
          resources:
            limits:
              cpu: 1
              memory: 2048Mi
            requests:
              cpu: 100m
              memory: 500Mi
          securityContext:
            {}
          volumeMounts:
          - name: flink-config-volume
            mountPath: /opt/flink/conf/flink-conf.yaml
            subPath: flink-conf.yaml
          - name: flink-config-volume
            mountPath: /opt/flink/conf/log4j-console.properties
            subPath: log4j-console.properties
      volumes:
      - name: flink-config-volume
        configMap:
          name: search-indexer-config
          items:
          - key: flink-conf
            path: flink-conf.yaml
          - key: log4j_console_properties
            path: log4j-console.properties
---
# Source: knowledgebb/charts/flink/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: video-stream-generator-taskmanager
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: flink
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: flink-0.1.0
  annotations:
    checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    checksum/job-config: d08f1a187150b1ada3158219a97aa16d096f23f35e1d475a8ff30656975e065e
    
    reloader.stakater.com/auto: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: flink
      app.kubernetes.io/component: video-stream-generator-taskmanager
  template:
    metadata:
      labels:
        app.kubernetes.io/name: flink
        app.kubernetes.io/component: video-stream-generator-taskmanager
      annotations:
        checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
        checksum/job-config: d08f1a187150b1ada3158219a97aa16d096f23f35e1d475a8ff30656975e065e
    spec:
      serviceAccountName: 
      securityContext:
        fsGroup: 0
        runAsUser: 0
      containers:
        - name: flink
          image: "sunbirded.azurecr.io/knowledge-platform-jobs:release-5.7.0_RC3_18c0d23_90"
          imagePullPolicy: IfNotPresent
          workingDir: 
          command: ["/opt/flink/bin/taskmanager.sh"]
          args:
            ["start-foreground",
            "-Dweb.submit.enable=false",
            "-Dmetrics.reporter.prom.class=org.apache.flink.metrics.prometheus.PrometheusReporter",
            "-Dmetrics.reporter.prom.host=video-stream-generator-taskmanager",
            "-Dmetrics.reporter.prom.port=9251-9260",
            "-Djobmanager.rpc.address=video-stream-generator-jobmanager",
            "-Dtaskmanager.rpc.port=6122"]
          ports:
            - name: http
              containerPort: 8081
          resources:
            limits:
              cpu: 1
              memory: 2048Mi
            requests:
              cpu: 100m
              memory: 500Mi
          securityContext:
            {}
          volumeMounts:
          - name: flink-config-volume
            mountPath: /opt/flink/conf/flink-conf.yaml
            subPath: flink-conf.yaml
          - name: flink-config-volume
            mountPath: /opt/flink/conf/log4j-console.properties
            subPath: log4j-console.properties
      volumes:
      - name: flink-config-volume
        configMap:
          name: video-stream-generator-config
          items:
          - key: flink-conf
            path: flink-conf.yaml
          - key: log4j_console_properties
            path: log4j-console.properties
---
# Source: knowledgebb/charts/learning/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: learning
  namespace: sunbird
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  replicas: 1
  strategy:
     rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
  selector:
    matchLabels:
      app: learning
  template:
    metadata:
      labels:
        app: learning
    spec:
      containers:
      - name: learning
        image: "sunbirded.azurecr.io/learning-service:latest"
        imagePullPolicy: Always
        env:
        - name: JAVA_OPTIONS
          value: -Dconfig.file=/usr/local/tomcat/config/application.conf
        envFrom:
        - configMapRef:
            name: learning-config
        resources:
          limits:
            cpu: 1
            memory: 4096Mi
          requests:
            cpu: 100m
            memory: 1024Mi
        ports:
        - containerPort: 8080
        volumeMounts:
          - name: learning-config
            mountPath: /usr/local/tomcat/config/application.conf
            subPath: learning-service_application.conf
      volumes:
      - name: learning-config
        configMap:
          name: learning-config
---
# Source: knowledgebb/charts/neo4j/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: neo4j-kbb
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: neo4j-kbb
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: neo4j-0.1.0
  annotations:
    helm.sh/hook-weight: "-5"
    reloader.stakater.com/auto: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: neo4j-kbb
  template:
    metadata:
      labels:
        app.kubernetes.io/name: neo4j-kbb
      annotations:
        checksum/config: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a
    spec:
      serviceAccountName: neo4j-kbb
      securityContext:
        null

      ###  Init container to fully clean Neo4j database dir
      initContainers:
        - name: cleanup-neo4j-pvc
          image: busybox
          command:
            - sh
            - -c
            - |
              echo "Cleaning Neo4j database directory completely...";
              rm -rf /data/databases/graph.db/*;
              echo "Neo4j database directory cleaned.";
          volumeMounts:
            - name: neo4j-data
              mountPath: /data

      containers:
        - name: neo4j
          image: "sunbirded.azurecr.io/neo4j:3.3.0"
          imagePullPolicy: IfNotPresent
          env:
            - name: NEO4J_dbms_security_auth__enabled
              value: "false"

          ###  Graceful shutdown to release file locks
          lifecycle:
            preStop:
              exec:
                command: ["/bin/bash", "-c", "/var/lib/neo4j/bin/neo4j stop"]
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - '! test -f /var/lib/neo4j/data/databases/graph.db/store_lock'
            initialDelaySeconds: 30
            periodSeconds: 60

          ports:
            - name: cypher-port
              containerPort: 7474
            - name: bolt-port-1
              containerPort: 7687
            - name: bolt-port-2
              containerPort: 8687

          resources:
            limits:
              cpu: "1"
              memory: 2Gi
            requests:
              cpu: 500m
              memory: 1Gi

          securityContext:
            null

          volumeMounts:
            - name: shared-data
              mountPath: /var/lib/neo4j/logs/plugins/txn-handler
            - name: neo4j-data
              mountPath: /data

        - name: logstash
          image: logstash:6.8.21
          imagePullPolicy: Always
          env:
            - name: PIPELINE_WORKERS
              value: "1"
            - name: PIPELINE_BATCH_SIZE
              value: "1"
          volumeMounts:
            - name: config
              mountPath: /usr/share/logstash/pipeline/logstash.conf
              subPath: logstash.conf
            - name: shared-data
              mountPath: /txn-handler

      volumes:
        - name: config
          configMap:
            name: logstash-config-kbb
            items:
              - key: logstash.conf
                path: logstash.conf
        - name: shared-data
          emptyDir: {}
        - name: neo4j-data
          persistentVolumeClaim:
            claimName: neo4j-claim
---
# Source: knowledgebb/charts/search/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: search
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: search
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: search-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: search
  template:
    metadata:
      labels:
        app.kubernetes.io/name: search
      annotations:
        checksum/config: 47588c1fc8d05cacba4793e112c995f7953ed1d4a2a957ba22c6c8db2012fa37
    spec:
      serviceAccountName: search
      securityContext:
        {}
      containers:
        - name: search
          image: "sunbirded.azurecr.io/search-service:release-6.1.0_RC4_2272e93_40"
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /health
              port: 9000
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /health
              port: 9000
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          ports:
            - name: http-search
              containerPort: 9000
          resources:
            limits:
              cpu: 1
              memory: 1024Mi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            {}
          envFrom:
          - configMapRef:
              name: search-env
          volumeMounts:
          - name: config
            mountPath: /home/sunbird/search-service-1.0-SNAPSHOT/config
      volumes:
      - name: env
        configMap:
          name: search-env
      - name: config
        configMap:
          name: search
---
# Source: knowledgebb/charts/taxonomy/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: taxonomy
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: taxonomy
    app.kubernetes.io/version: 1.0.0
    helm.sh/chart: taxonomy-0.1.0
  annotations:
    reloader.stakater.com/auto: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: taxonomy
  template:
    metadata:
      labels:
        app.kubernetes.io/name: taxonomy
      annotations:
        checksum/config: afdce2cf6653b198c60482e8a415684126642e535ceee8ecd106c9cacafb03f7
    spec:
      serviceAccountName: taxonomy
      securityContext:
        {}
      containers:
        - name: taxonomy
          image: "sunbirded.azurecr.io/taxonomy-service:release-6.1.0_RC4_2272e93_47"
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 6
            httpGet:
              path: /health
              port: 9000
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          readinessProbe:
            failureThreshold: 6
            httpGet:
              path: /health
              port: 9000
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          ports:
            - name: http-taxonomy
              containerPort: 9000
          resources:
            limits:
              cpu: 1
              memory: 1024Mi
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            {}
          envFrom:
          - configMapRef:
              name: taxonomy-env
          volumeMounts:
          - name: config
            mountPath: /home/sunbird/taxonomy-service-1.0-SNAPSHOT/config
      volumes:
      - name: env
        configMap:
          name: taxonomy-env
      - name: config
        configMap:
          name: taxonomy
---
# Source: knowledgebb/charts/cassandra/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cassandra-kbb
  namespace: "sunbird"
  labels:
    app.kubernetes.io/name: cassandra
    helm.sh/chart: cassandra-10.1.0
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "4.1.0"
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: cassandra
      app.kubernetes.io/instance: neo4j-kbb
  serviceName: cassandra-kbb-headless
  podManagementPolicy: OrderedReady
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: cassandra
        helm.sh/chart: cassandra-10.1.0
        app.kubernetes.io/instance: neo4j-kbb
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: "4.1.0"
    spec:
      
      serviceAccountName: cassandra-kbb
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: neo4j-kbb
                    app.kubernetes.io/name: cassandra
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      containers:
        - name: cassandra
          command:
            - bash
            - -ec
            - |
              # Node 0 is the password seeder
              if [[ $POD_NAME =~ (.*)-0$ ]]; then
                  echo "Setting node as password seeder"
                  export CASSANDRA_PASSWORD_SEEDER=yes
              else
                  # Only node 0 will execute the startup initdb scripts
                  export CASSANDRA_IGNORE_INITDB_SCRIPTS=1
              fi
              /opt/bitnami/scripts/cassandra/entrypoint.sh /opt/bitnami/scripts/cassandra/run.sh
          image: docker.io/bitnami/cassandra:3.11.13-debian-11-r3
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: CASSANDRA_CLUSTER_NAME
              value: cassandra
            - name: CASSANDRA_SEEDS
              value: "cassandra-kbb-0.cassandra-kbb-headless.sunbird.svc.cluster.local"
            - name: CASSANDRA_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: cassandra-kbb
                  key: cassandra-password
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: CASSANDRA_USER
              value: "cassandra"
            - name: CASSANDRA_NUM_TOKENS
              value: "256"
            - name: CASSANDRA_DATACENTER
              value: dc1
            - name: CASSANDRA_ENDPOINT_SNITCH
              value: SimpleSnitch
            - name: CASSANDRA_KEYSTORE_LOCATION
              value: "/opt/bitnami/cassandra/certs/keystore"
            - name: CASSANDRA_TRUSTSTORE_LOCATION
              value: "/opt/bitnami/cassandra/certs/truststore"
            - name: CASSANDRA_RACK
              value: rack1
            - name: CASSANDRA_TRANSPORT_PORT_NUMBER
              value: "7000"
            - name: CASSANDRA_JMX_PORT_NUMBER
              value: "7199"
            - name: CASSANDRA_CQL_PORT_NUMBER
              value: "9042"
            - name: CASSANDRA_AUTHENTICATOR
              value: AllowAllAuthenticator
            - name: CASSANDRA_AUTHORIZER
              value: AllowAllAuthorizer
          envFrom:
          livenessProbe:
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  nodetool info | grep "Native Transport active: true"
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 30
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  nodetool status | grep -E "^UN\\s+${POD_IP}"
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 30
            successThreshold: 1
            failureThreshold: 5
          lifecycle:
            preStop:
              exec:
                command:
                  - bash
                  - -ec
                  - nodetool drain
          ports:
            - name: intra
              containerPort: 7000
            - name: tls
              containerPort: 7001
            - name: jmx
              containerPort: 7199
            - name: cql
              containerPort: 9042
          resources: 
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /bitnami/cassandra
            
      volumes:
        - name: metrics-conf
          configMap:
            name: cassandra-kbb-metrics-conf
  volumeClaimTemplates:
    - metadata:
        name: data
        labels:
          app.kubernetes.io/name: cassandra
          app.kubernetes.io/instance: neo4j-kbb
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "25Gi"
---
# Source: knowledgebb/charts/elasticsearch/templates/master/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch-kbb-master
  namespace: "sunbird"
  labels:
    app.kubernetes.io/name: elasticsearch
    helm.sh/chart: elasticsearch-19.5.14
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "8.6.2"
    app.kubernetes.io/component: master
    ## Istio Labels: https://istio.io/docs/ops/deployment/requirements/
    app: master
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  replicas: 1
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app.kubernetes.io/name: elasticsearch
      app.kubernetes.io/instance: neo4j-kbb
      app.kubernetes.io/component: master
  serviceName: elasticsearch-kbb-master-hl
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: elasticsearch
        helm.sh/chart: elasticsearch-19.5.14
        app.kubernetes.io/instance: neo4j-kbb
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: "8.6.2"
        app.kubernetes.io/component: master
        ## Istio Labels: https://istio.io/docs/ops/deployment/requirements/
        app: master
      annotations:
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
        ## Image that performs the sysctl operation to modify Kernel settings (needed sometimes to avoid boot errors)
        - name: sysctl
          image: docker.io/bitnami/bitnami-shell-archived:11-debian-11-r54
          imagePullPolicy: "IfNotPresent"
          command:
            - /bin/bash
            - -ec
            - |
              CURRENT=`sysctl -n vm.max_map_count`;
              DESIRED="262144";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w vm.max_map_count=262144;
              fi;
              CURRENT=`sysctl -n fs.file-max`;
              DESIRED="65536";
              if [ "$DESIRED" -gt "$CURRENT" ]; then
                  sysctl -w fs.file-max=65536;
              fi;
          securityContext:
            privileged: true
            runAsUser: 0
          resources:
            limits: {}
            requests: {}
      containers:
        - name: elasticsearch
          image: docker.io/bitnami/elasticsearch:6.8.23
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: ELASTICSEARCH_IS_DEDICATED_NODE
              value: "no"
            - name: ELASTICSEARCH_NODE_ROLES
              value: "master"
            - name: ELASTICSEARCH_TRANSPORT_PORT_NUMBER
              value: "9300"
            - name: ELASTICSEARCH_HTTP_PORT_NUMBER
              value: "9200"
            - name: ELASTICSEARCH_CLUSTER_NAME
              value: "elastic"
            - name: ELASTICSEARCH_CLUSTER_HOSTS
              value: "elasticsearch-kbb-master-hl.sunbird.svc.cluster.local,"
            - name: ELASTICSEARCH_TOTAL_NODES
              value: "1"
            - name: ELASTICSEARCH_CLUSTER_MASTER_HOSTS
              value: elasticsearch-kbb-master-0 
            - name: ELASTICSEARCH_MINIMUM_MASTER_NODES
              value: "1"
            - name: ELASTICSEARCH_ADVERTISED_HOSTNAME
              value: "$(MY_POD_NAME).elasticsearch-kbb-master-hl.sunbird.svc.cluster.local"
            - name: ELASTICSEARCH_HEAP_SIZE
              value: "2G"
          ports:
            - name: rest-api
              containerPort: 9200
            - name: transport
              containerPort: 9300
          livenessProbe:
            failureThreshold: 5
            initialDelaySeconds: 90
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          readinessProbe:
            failureThreshold: 5
            initialDelaySeconds: 90
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /opt/bitnami/scripts/elasticsearch/healthcheck.sh
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2Gi
          volumeMounts:
            - name: data
              mountPath: /bitnami/elasticsearch/data
      volumes:
  volumeClaimTemplates:
    - metadata:
        name: "data"
        annotations:
          helm.sh/hook-weight: "-5"
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "25Gi"
---
# Source: knowledgebb/charts/kafka/charts/zookeeper/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: zookeeper-kbb
  namespace: sunbird
  labels:
    app.kubernetes.io/name: zookeeper
    helm.sh/chart: zookeeper-11.0.2
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.8.0"
    app.kubernetes.io/component: zookeeper
    role: zookeeper
spec:
  replicas: 1
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app.kubernetes.io/name: zookeeper
      app.kubernetes.io/instance: neo4j-kbb
      app.kubernetes.io/component: zookeeper
  serviceName: zookeeper-kbb-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      annotations:
      labels:
        app.kubernetes.io/name: zookeeper
        helm.sh/chart: zookeeper-11.0.2
        app.kubernetes.io/instance: neo4j-kbb
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: "3.8.0"
        app.kubernetes.io/component: zookeeper
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: neo4j-kbb
                    app.kubernetes.io/name: zookeeper
                    app.kubernetes.io/component: zookeeper
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      initContainers:
      containers:
        - name: zookeeper
          image: docker.io/bitnami/zookeeper:3.8.0-debian-11-r65
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /scripts/setup.sh
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: ZOO_DATA_LOG_DIR
              value: ""
            - name: ZOO_PORT_NUMBER
              value: "2181"
            - name: ZOO_TICK_TIME
              value: "2000"
            - name: ZOO_INIT_LIMIT
              value: "10"
            - name: ZOO_SYNC_LIMIT
              value: "5"
            - name: ZOO_PRE_ALLOC_SIZE
              value: "65536"
            - name: ZOO_SNAPCOUNT
              value: "100000"
            - name: ZOO_MAX_CLIENT_CNXNS
              value: "60"
            - name: ZOO_4LW_COMMANDS_WHITELIST
              value: "srvr, mntr, ruok"
            - name: ZOO_LISTEN_ALLIPS_ENABLED
              value: "no"
            - name: ZOO_AUTOPURGE_INTERVAL
              value: "0"
            - name: ZOO_AUTOPURGE_RETAIN_COUNT
              value: "3"
            - name: ZOO_MAX_SESSION_TIMEOUT
              value: "40000"
            - name: ZOO_SERVERS
              value: zookeeper-kbb-0.zookeeper-kbb-headless.sunbird.svc.cluster.local:2888:3888::1 
            - name: ZOO_ENABLE_AUTH
              value: "no"
            - name: ZOO_ENABLE_QUORUM_AUTH
              value: "no"
            - name: ZOO_HEAP_SIZE
              value: "1024"
            - name: ZOO_LOG_LEVEL
              value: "ERROR"
            - name: ALLOW_ANONYMOUS_LOGIN
              value: "yes"
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
          ports:
            - name: client
              containerPort: 2181
            - name: follower
              containerPort: 2888
            - name: election
              containerPort: 3888
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command: ['/bin/bash', '-c', 'echo "ruok" | timeout 2 nc -w 2 localhost 2181 | grep imok']
          volumeMounts:
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
            - name: data
              mountPath: /bitnami/zookeeper
      volumes:
        - name: scripts
          configMap:
            name: zookeeper-kbb-scripts
            defaultMode: 0755
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "2Gi"
---
# Source: knowledgebb/charts/kafka/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka-kbb
  namespace: "sunbird"
  labels:
    app.kubernetes.io/name: kafka
    helm.sh/chart: kafka-20.0.2
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "3.3.1"
    app.kubernetes.io/component: kafka
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: kafka
      app.kubernetes.io/instance: neo4j-kbb
      app.kubernetes.io/component: kafka
  serviceName: kafka-kbb-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: kafka
        helm.sh/chart: kafka-20.0.2
        app.kubernetes.io/instance: neo4j-kbb
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: "3.3.1"
        app.kubernetes.io/component: kafka
      annotations:
    spec:
      
      hostNetwork: false
      hostIPC: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: neo4j-kbb
                    app.kubernetes.io/name: kafka
                    app.kubernetes.io/component: kafka
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      serviceAccountName: kafka-kbb
      containers:
        - name: kafka
          image: docker.io/bitnami/kafka:3.3.1-debian-11-r25
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /scripts/setup.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: KAFKA_CFG_ZOOKEEPER_CONNECT
              value: "zookeeper-kbb"
            - name: KAFKA_INTER_BROKER_LISTENER_NAME
              value: "INTERNAL"
            - name: KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP
              value: "INTERNAL:PLAINTEXT,CLIENT:PLAINTEXT"
            - name: KAFKA_CFG_LISTENERS
              value: "INTERNAL://:9093,CLIENT://:9092"
            - name: KAFKA_CFG_ADVERTISED_LISTENERS
              value: "INTERNAL://$(MY_POD_NAME).kafka-kbb-headless.sunbird.svc.cluster.local:9093,CLIENT://$(MY_POD_NAME).kafka-kbb-headless.sunbird.svc.cluster.local:9092"
            - name: ALLOW_PLAINTEXT_LISTENER
              value: "yes"
            - name: KAFKA_ZOOKEEPER_PROTOCOL
              value: PLAINTEXT
            - name: KAFKA_VOLUME_DIR
              value: "/bitnami/kafka"
            - name: KAFKA_LOG_DIR
              value: "/opt/bitnami/kafka/logs"
            - name: KAFKA_CFG_DELETE_TOPIC_ENABLE
              value: "false"
            - name: KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE
              value: "true"
            - name: KAFKA_HEAP_OPTS
              value: "-Xmx1024m -Xms1024m"
            - name: KAFKA_CFG_LOG_FLUSH_INTERVAL_MESSAGES
              value: "10000"
            - name: KAFKA_CFG_LOG_FLUSH_INTERVAL_MS
              value: "1000"
            - name: KAFKA_CFG_LOG_RETENTION_BYTES
              value: "1073741824"
            - name: KAFKA_CFG_LOG_RETENTION_CHECK_INTERVAL_MS
              value: "300000"
            - name: KAFKA_CFG_LOG_RETENTION_HOURS
              value: "168"
            - name: KAFKA_CFG_MESSAGE_MAX_BYTES
              value: "1000012"
            - name: KAFKA_CFG_LOG_SEGMENT_BYTES
              value: "1073741824"
            - name: KAFKA_CFG_LOG_DIRS
              value: "/bitnami/kafka/data"
            - name: KAFKA_CFG_DEFAULT_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR
              value: "1"
            - name: KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR
              value: "1"
            - name: KAFKA_CFG_NUM_IO_THREADS
              value: "8"
            - name: KAFKA_CFG_NUM_NETWORK_THREADS
              value: "3"
            - name: KAFKA_CFG_NUM_PARTITIONS
              value: "1"
            - name: KAFKA_CFG_NUM_RECOVERY_THREADS_PER_DATA_DIR
              value: "1"
            - name: KAFKA_CFG_SOCKET_RECEIVE_BUFFER_BYTES
              value: "102400"
            - name: KAFKA_CFG_SOCKET_REQUEST_MAX_BYTES
              value: "104857600"
            - name: KAFKA_CFG_SOCKET_SEND_BUFFER_BYTES
              value: "102400"
            - name: KAFKA_CFG_ZOOKEEPER_CONNECTION_TIMEOUT_MS
              value: "6000"
            - name: KAFKA_CFG_AUTHORIZER_CLASS_NAME
              value: ""
            - name: KAFKA_CFG_ALLOW_EVERYONE_IF_NO_ACL_FOUND
              value: "true"
            - name: KAFKA_CFG_SUPER_USERS
              value: "User:admin"
          ports:
            - name: kafka-client
              containerPort: 9092
            - name: kafka-internal
              containerPort: 9093
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: kafka-client
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: kafka-client
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /bitnami/kafka
            - name: logs
              mountPath: /opt/bitnami/kafka/logs
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
      volumes:
        - name: scripts
          configMap:
            name: kafka-kbb-scripts
            defaultMode: 0755
        - name: logs
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "25Gi"
---
# Source: knowledgebb/charts/redis/templates/master/application.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-kbb-master
  namespace: "sunbird"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.8.3
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.9"
    app.kubernetes.io/component: master
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: redis
      app.kubernetes.io/instance: neo4j-kbb
      app.kubernetes.io/component: master
  serviceName: redis-kbb-headless
  updateStrategy:
    type: RollingUpdate
  minReadySeconds: 0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: redis
        helm.sh/chart: redis-17.8.3
        app.kubernetes.io/instance: neo4j-kbb
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: "7.0.9"
        app.kubernetes.io/component: master
      annotations:
        checksum/configmap: 830d3671a49bbc07a59d2f545165b4c8c29bcc44ffd8c641a1685deda1dadaa1
        checksum/health: 9c48c1b0e0ad383b37893e310fecbea1f8e148cb0cc97e7b9bf56b5f04b595d2
        checksum/scripts: 1ed84652357b644e65f28ffb8bcc10bff9dbb2c625bb990fdf2b24c6d406e5b8
        checksum/secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
    spec:
      
      securityContext:
        fsGroup: 1001
      serviceAccountName: redis-kbb
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: neo4j-kbb
                    app.kubernetes.io/name: redis
                    app.kubernetes.io/component: master
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:7.0.9-debian-11-r1
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-master.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: master
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            # One second longer than command timeout should prevent generation of zombie processes.
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: redis-tmp-conf
              mountPath: /opt/bitnami/redis/etc/
            - name: tmp
              mountPath: /tmp
      volumes:
        - name: start-scripts
          configMap:
            name: redis-kbb-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: redis-kbb-health
            defaultMode: 0755
        - name: config
          configMap:
            name: redis-kbb-configuration
        - name: redis-tmp-conf
          emptyDir: {}
        - name: tmp
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: redis-data
        labels:
          app.kubernetes.io/name: redis
          app.kubernetes.io/instance: neo4j-kbb
          app.kubernetes.io/component: master
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "25Gi"
---
# Source: knowledgebb/charts/redis/templates/replicas/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-kbb-replicas
  namespace: "sunbird"
  labels:
    app.kubernetes.io/name: redis
    helm.sh/chart: redis-17.8.3
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: "7.0.9"
    app.kubernetes.io/component: replica
  annotations:
    helm.sh/hook-weight: "-5"
spec:
  replicas: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: redis
      app.kubernetes.io/instance: neo4j-kbb
      app.kubernetes.io/component: replica
  serviceName: redis-kbb-headless
  updateStrategy:
    type: RollingUpdate
  minReadySeconds: 0
  template:
    metadata:
      labels:
        app.kubernetes.io/name: redis
        helm.sh/chart: redis-17.8.3
        app.kubernetes.io/instance: neo4j-kbb
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/version: "7.0.9"
        app.kubernetes.io/component: replica
      annotations:
        checksum/configmap: 830d3671a49bbc07a59d2f545165b4c8c29bcc44ffd8c641a1685deda1dadaa1
        checksum/health: 9c48c1b0e0ad383b37893e310fecbea1f8e148cb0cc97e7b9bf56b5f04b595d2
        checksum/scripts: 1ed84652357b644e65f28ffb8bcc10bff9dbb2c625bb990fdf2b24c6d406e5b8
        checksum/secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
    spec:
      
      securityContext:
        fsGroup: 1001
      serviceAccountName: redis-kbb
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: neo4j-kbb
                    app.kubernetes.io/name: redis
                    app.kubernetes.io/component: replica
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/bitnami/redis:7.0.9-debian-11-r1
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/start-scripts/start-replica.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: replica
            - name: REDIS_MASTER_HOST
              value: redis-kbb-master-0.redis-kbb-headless.sunbird.svc.cluster.local
            - name: REDIS_MASTER_PORT_NUMBER
              value: "6379"
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          startupProbe:
            failureThreshold: 22
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            tcpSocket:
              port: redis
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local_and_master.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local_and_master.sh 1
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: redis-tmp-conf
              mountPath: /opt/bitnami/redis/etc
      volumes:
        - name: start-scripts
          configMap:
            name: redis-kbb-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: redis-kbb-health
            defaultMode: 0755
        - name: config
          configMap:
            name: redis-kbb-configuration
        - name: redis-tmp-conf
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: redis-data
        labels:
          app.kubernetes.io/name: redis
          app.kubernetes.io/instance: neo4j-kbb
          app.kubernetes.io/component: replica
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: knowledgebb/templates/provision/cassandra.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: knowledgebb-cassandra-migration-job
  labels:
    app: knowledgebb-cassandra-migration
    scope: provisioning
  annotations:
    helm.sh/hook-weight: "-4"
spec:
  template:
    metadata:
      labels:
        app: knowledgebb-cassandra-migration
    spec:
      restartPolicy: Never
      volumes:
      - name: shared-volume
        emptyDir: {}
      initContainers:
      - name: wait-for-cassandra
        image: alpine/git
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c']
        args:
          - |
            timeout 120 sh -c 'until nc -z cassandra 9042; do echo waiting for cassandra; sleep 10; done'
            set -e
            cd /opt/shared-volume
            export knowlg_automation_version=release-5.5.0

            # Download the migration files
            git clone --filter=blob:none --no-checkout https://github.com/Sunbird-Knowlg/knowlg-automation --branch=$knowlg_automation_version --depth 1
            cd knowlg-automation
            git sparse-checkout init --cone
            git sparse-checkout set terraform/modules/helm/cassandra/cassandra-helm-chart/files
            git checkout
            chmod -R 777 /opt/shared-volume
        volumeMounts:
          - name: shared-volume
            mountPath: /opt/shared-volume
      containers:
      - name: migration
        image: bitnami/cassandra:3.11.13-debian-11-r3
        imagePullPolicy: IfNotPresent
        command: ['bash', '-c']
        args:
        - |
          cd /opt/shared-volume/knowlg-automation/terraform/modules/helm/cassandra/cassandra-helm-chart/files
          export ENV=
          
          sed -i "s/{{ env }}/$ENV/g" generalized-cassandra.cql
          echo cqlsh cassandra -f "generalized-cassandra.cql"
          cqlsh cassandra -f "generalized-cassandra.cql"
        volumeMounts:
          - name: shared-volume
            mountPath: /opt/shared-volume
  backoffLimit: 0
---
# Source: knowledgebb/templates/provision/kafka.yaml
kind: Job
apiVersion: batch/v1
metadata:
  name: neo4j-kbb-knowledgebb-provisioning
  namespace: "sunbird"
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: knowledgebb
    app.kubernetes.io/version: 1.16.0
    helm.sh/chart: knowledgebb-0.1.0
    app.kubernetes.io/component: kafka-provisioning
    scope: provisioning
  annotations:
    helm.sh/hook-weight: "-4"
spec:
  template:
    metadata:
      labels: 
        app.kubernetes.io/component: kafka-provisioning
    spec:
      restartPolicy: OnFailure
      terminationGracePeriodSeconds: 0
      containers:
        - name: kafka-provisioning
          image: "bitnami/kafka:3.3.1-debian-11-r25"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
          args:
            - -ec
            - |
              echo "Configuring environment"
              . /opt/bitnami/scripts/libkafka.sh
              export CLIENT_CONF="/tmp/client.properties"
              touch $CLIENT_CONF
              kafka_common_conf_set "$CLIENT_CONF" security.protocol "PLAINTEXT"
              KAFKA_SERVICE="kafka:9092"
              kafka_provisioning_commands=(
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server "$KAFKA_SERVICE" \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config retention.ms=172800000 \
                    --command-config ${CLIENT_CONF} \
                    --topic .knowlg.qrimage.request"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server "$KAFKA_SERVICE" \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config retention.ms=172800000 \
                    --command-config ${CLIENT_CONF} \
                    --topic .knowlg.publish.job.request"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server "$KAFKA_SERVICE" \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config retention.ms=172800000 \
                    --command-config ${CLIENT_CONF} \
                    --topic .knowlg.content.postpublish.request"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server "$KAFKA_SERVICE" \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config retention.ms=172800000 \
                    --command-config ${CLIENT_CONF} \
                    --topic .knowlg.learning.job.request"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server "$KAFKA_SERVICE" \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config retention.ms=172800000 \
                    --command-config ${CLIENT_CONF} \
                    --topic .knowlg.learning.graph.events"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server "$KAFKA_SERVICE" \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config retention.ms=172800000 \
                    --command-config ${CLIENT_CONF} \
                    --topic .knowlg.learning.events.failed"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server "$KAFKA_SERVICE" \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config retention.ms=172800000 \
                    --command-config ${CLIENT_CONF} \
                    --topic .knowlg.qrimage.request"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server "$KAFKA_SERVICE" \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config retention.ms=172800000 \
                    --command-config ${CLIENT_CONF} \
                    --topic .knowlg.telemetry.raw"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server "$KAFKA_SERVICE" \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config retention.ms=172800000 \
                    --command-config ${CLIENT_CONF} \
                    --topic .knowlg.dialcode.context.job.request"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server "$KAFKA_SERVICE" \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config retention.ms=172800000 \
                    --command-config ${CLIENT_CONF} \
                    --topic .knowlg.dialcode.context.job.request.failed"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server "$KAFKA_SERVICE" \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config retention.ms=172800000 \
                    --command-config ${CLIENT_CONF} \
                    --topic .knowlg.auto.creation.job.request"
                "/opt/bitnami/kafka/bin/kafka-topics.sh \
                    --create \
                    --if-not-exists \
                    --bootstrap-server "$KAFKA_SERVICE" \
                    --replication-factor 1 \
                    --partitions 1 \
                    --config retention.ms=172800000 \
                    --command-config ${CLIENT_CONF} \
                    --topic .knowlg.transaction.meta"
              )

              echo "Starting provisioning"
              for ((index=0; index < ${#kafka_provisioning_commands[@]}; index+=1))
              do
                for j in $(seq ${index} $((${index}+1-1)))
                do
                    ${kafka_provisioning_commands[j]} & # Async command
                done
                wait  # Wait the end of the jobs
              done

              echo "Provisioning succeeded"
---
# Source: knowledgebb/templates/provision/neo4j.yaml
kind: Job
apiVersion: batch/v1
metadata:
  name: neo4j-kbb-knowledgebb-neo4j-provisioning
  namespace: "sunbird"
  labels:
    app.kubernetes.io/instance: neo4j-kbb
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: knowledgebb
    app.kubernetes.io/version: 1.16.0
    helm.sh/chart: knowledgebb-0.1.0
    app.kubernetes.io/component: neo4j-provisioning
    scope: provisioning
  annotations:
    helm.sh/hook-weight: "-4"
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/component: neo4j-provisioning
    spec:
      restartPolicy: OnFailure
      terminationGracePeriodSeconds: 0
      initContainers:
        - name: wait-for-available-neo4j
          image: busybox
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
          args:
            - -ec
            - |
              PORT=7687
              HOST=neo4j
              TIMEOUT=600
              START_TIME=$(date +%s)

              while ! nc -z $HOST $PORT; do
                CURRENT_TIME=$(date +%s)
                ELAPSED_TIME=$(( CURRENT_TIME - START_TIME ))

                if [ $ELAPSED_TIME -ge $TIMEOUT ]; then
                  echo "Timeout reached. Port $PORT is not available."
                  exit 1
                fi

                sleep 1
              done

              echo "Port Neo4j is now available."
      containers:
        - name: neo4j-provisioning
          image: "sunbirded.azurecr.io/neo4j:3.3.0"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
          args:
            - -ec
            - |
              /var/lib/neo4j/bin/cypher-shell -a bolt://neo4j:7687 'CREATE CONSTRAINT ON (domain:domain) ASSERT domain.IL_UNIQUE_ID IS UNIQUE;'
              sleep 3
              /var/lib/neo4j/bin/cypher-shell -a bolt://neo4j:7687 'CREATE INDEX ON :domain(IL_FUNC_OBJECT_TYPE);'
              sleep 3
              /var/lib/neo4j/bin/cypher-shell -a bolt://neo4j:7687 'CREATE INDEX ON :domain(IL_SYS_NODE_TYPE);'
---
# Source: knowledgebb/templates/provision/job-cleaner.yaml
# This chart is required because helm does not support deleting jobs if not having hooks.
# We can add pre-install, hook as some services may require schemas in db, and it'll cause catch22.
apiVersion: v1
kind: ServiceAccount
metadata:
  name: knowledgebb-job-deleter
  annotations:
    helm.sh/hook: "pre-install,pre-upgrade"
    helm.sh/hook-weight: "-5"
---
# Source: knowledgebb/templates/provision/job-cleaner.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: knowledgebb-job-deleter-role
  annotations:
    helm.sh/hook: "pre-install,pre-upgrade"
    helm.sh/hook-weight: "-5"
rules:
- apiGroups: ["batch"]
  resources: ["jobs"]
  verbs: ["delete", "get", "list"]
---
# Source: knowledgebb/templates/provision/job-cleaner.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: knowledgebb-job-deleter-rolebinding
  annotations:
    helm.sh/hook: "pre-install,pre-upgrade"
    helm.sh/hook-weight: "-5"
subjects:
- kind: ServiceAccount
  name: knowledgebb-job-deleter
roleRef:
  kind: Role
  name: knowledgebb-job-deleter-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: knowledgebb/templates/provision/job-cleaner.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: knowledgebb-job-deleter
  annotations:
    helm.sh/hook: "pre-install,pre-upgrade"
    helm.sh/hook-weight: "-3"
spec:
  template:
    spec:
      serviceAccountName: knowledgebb-job-deleter
      containers:
      - name: kubectl-container
        image: bitnami/kubectl
        imagePullPolicy: IfNotPresent
        command: ["bash", "-c"]
        args:
          - |
            chart_name="knowledgebb"
            for job in $(kubectl get jobs -l scope=provisioning -o name | grep $chart_name); do
              if ! kubectl delete $job; then
                echo "Couldn't delete job $job"
                continue
              fi
              echo "deleted $job"
            done
      restartPolicy: Never
